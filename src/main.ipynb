{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use Case:\n",
    "    1. Use It in Jupyter:\n",
    "                \n",
    "        from path_manager import addpath\n",
    "        paths = addpath()\n",
    "        # Use the returned dictionary if needed\n",
    "        print(\"Base dir:\", paths['BASE_DIR'])\n",
    "\n",
    "    2. Use in Python Script:\n",
    "        from path_manager import addpath\n",
    "        addpath()\n",
    "        # Then import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below code , To save the plot and see the data distribution values. x_data = x-axis is created from the data values by uniformly distributting the data from min(data) to max(data)\n",
    "### we can have quick look of data arrangement , but there is drawback in plot that y-values if they are same should be look piled up on each other will look apart by a certain x-axis difference. so we will plot it differently : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - this just below code is written in module ( MODULE NAME : plot_dataModule.py) -->\n",
    "### - here there are two class methods are defined for both plot_Type = 0/1  --> 0 -->  def plot_simple(self, file, save_name=None)  and  1--> plot_complex(self, file, save_name=None).\n",
    "### -  from plot_dataModule import DataPlotter\n",
    "    # Example data usage\n",
    "    plotter = DataPlotter(\n",
    "        # __init__(self, data_dir, base_dir=None, save_results=True, save_dir=None)\n",
    "        data_dir=\"data/raw_npyData\",  # relative to BASE_DIR\n",
    "        base_dir=\"E:/Projects/substructure_3d_data/Substructure_Different_DataTypes\"\n",
    "    )\n",
    "    plotter.run_all(complex_plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd().parent\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "\n",
    "RES_DIR = BASE_DIR / \"results\"\n",
    "print(f\"Results directory: {RES_DIR}\")\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "%matplotlib inline\n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "def decorator(name):\n",
    "    def decoratorLog(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            print(f\"\\n ---------------------> /// Implementing method: {func.__name__} for {name} \\\\\\ <------------------------------------------------------- \\n\")\n",
    "            results = func(*args, **kwargs)\n",
    "            print(f\"\\n ---------------------> /// Finished executing method: {func.__name__} for {name}\\\\\\ <--------------------------------------------------\\n\")\n",
    "            return results\n",
    "        return wrapper\n",
    "    return decoratorLog\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage of the decorator# This decorator can be used to log the execution of functions with a specific name.\n",
    "# This decorator can be used to log the execution of functions with a specific name.\n",
    "\n",
    "import os\n",
    "filename_print = lambda f: print(f\"File name: {f}\")\n",
    "\n",
    "\n",
    "def readfile():\n",
    "    \"\"\"\n",
    "    Reads .npy files in the data directory and prints their names.\n",
    "    \"\"\"\n",
    "    filepath = BASE_DIR / \"data\" / \"raw_npyData\"\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"File {filepath} does not exist.\")\n",
    "    for file in os.listdir(filepath):\n",
    "        if file.endswith(\".npy\"):\n",
    "            print(f\"Processing file: {file}\")\n",
    "\n",
    "            @decorator(file)\n",
    "            def decorated_filename_print(f):\n",
    "                return filename_print(f)\n",
    "\n",
    "            decorated_filename_print(file)\n",
    "\n",
    "readfile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This below script take file one by one and proceed:\n",
    "    - it will mask the value 1.334 (background value) and return the Masked Data using the  DataMasker methods from the DataProcessor class.DeprecationWarning\n",
    "    - DataMasker return: Masked_data, maskedValues_coordsOnly, filtered_Data_WithoutZero, UnMasked_coords, mask\n",
    "    - mask: it is the boolean array of TRUE and False. when data mask = maskValue --> using logic: mask = data == maskValue\n",
    "    - filtered data after masking with zero(making them 0) for given mask value - {maskValue} and removing those zeros.\n",
    "- Then finally plotted with: \n",
    "    - one with ~mask (After droping the background (i.e. 1.334) only meaningfull information): \n",
    "        - DataPlotter.visualize_and_export_3d_mesh(~mask,data,smoothing=None,title=filename[:-4],save_obj_path=None,save_png_path=save_png_path, save_gif_path=save_gif_path,rotate_and_capture=True)\n",
    "\n",
    "    - one with mask (when background is plotted after extracting from main data):\n",
    "        - DataPlotter.visualize_and_export_3d_mesh(mask,data,smoothing=None,title=filename[:-4],save_obj_path=None,save_png_path=save_png_path, save_gif_path=save_gif_path,rotate_and_capture=True)\n",
    "\n",
    "    - Results is saved here in result path : E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\backgroud_removed\\ \n",
    "\n",
    "    ! when i see the data, i always used to wonder that why the  images from the isasi is So clear without background. \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from listspecificfiles import readlistFiles\n",
    "from preprocessAll import DataPreprocessor\n",
    "from plot3dint import plot3dinteractive\n",
    "from plot_dataModule import DataPlotter\n",
    "\n",
    "filepath1 = r\"data\\raw_npyData\"\n",
    "\n",
    "fpath = readlistFiles(filepath1,'.npy').file_with_Path()\n",
    "# arr1 = np.array([1, 3.3, 4, 5, 6, 3, 8, 9, 3.3, 3.3, 3.3])\n",
    "# print(f\"arr1 shape:{arr1.shape} \\n and \\n {arr1}\")\n",
    "# res = DataPreprocessor.DataMasker(arr1, maskValue=3.3, masked_WithZero=True, masked_ANDRemoved=False)\n",
    "\n",
    "# print(f\"Masked arr1 for 3.3:{res[0]} \\n \")\n",
    "# print(f\"_Masked values coordinates only where masked value = 3.3 :\\n  {res[1]} \\n \")\n",
    "# print(f\"filtered data after masking with zero(making them 0) for given mask value = 3.3 and removing those zeros. :\\n {res[2]} \\n \")\n",
    "# print(f\"coordinates of the unmasked values only:\\n {res[3]} \\n \") \n",
    "\n",
    "Background_removed_path  = RES_DIR / \"backgroud_removed\"\n",
    "os.makedirs(Background_removed_path,exist_ok=True)\n",
    "\n",
    "for file in fpath:\n",
    "    filename = os.path.basename(file)\n",
    "    if filename.endswith('.npy') and filename !='tomo_grafene_48h.npy':\n",
    "        data = np.load(file)\n",
    "      \n",
    "\n",
    "        filenamepng = f\"{filename[:-4]}mainData.png\"\n",
    "        filenamegif = f\"{filename[:-4]}mainData.gif\"\n",
    "\n",
    "        save_png_path = os.path.join(Background_removed_path,filenamepng)\n",
    "        save_gif_path = os.path.join(Background_removed_path, filenamegif)\n",
    "\n",
    "        \n",
    "        Masked_data, maskedValues_coordsOnly, filtered_Data_WithoutZero, UnMasked_coords, mask, bgDataonly = DataPreprocessor.DataMasker(data, maskValue=1.334, masked_WithZero=True, masked_ANDRemoved=False)\n",
    "        \n",
    "        # mesh = DataPlotter.create_mesh_visualize_from_volumeData(Masked_data, grid_factor = 8192, simplify=True, threshold ='percentile', percentileValue= 0.001,  color_mode='gradient')\n",
    "        # DataPlotter.save_mesh_views_as_gif_and_png(mesh, save_dir = Background_removed_path, savefilenamepng= filenamepng, savefilenamegif=filenamegif)\n",
    "\n",
    "        # plot3dinteractive(Masked_data,keyvalue=filename,sample_fraction=0.05)\n",
    "\n",
    "        plot3dinteractive(bgDataonly,keyvalue=filename,sample_fraction=0.01)\n",
    "\n",
    "        # DataPlotter.visualize_and_export_3d_mesh(~mask,data,smoothing=None,title=filename[:-4],save_obj_path=None,save_png_path=save_png_path, save_gif_path=save_gif_path,rotate_and_capture=True)\n",
    "        # DataPlotter.visualize_and_export_3d_mesh(mask,data,smoothing=None,title=filename[:-4],save_obj_path=None,save_png_path=save_png_path, save_gif_path=save_gif_path,rotate_and_capture=True)\n",
    "\n",
    "\n",
    "    # print(f\" Masked data (returns the array of data size) where masked values is - {maskValue} and made them all zeros and left other as it is, or just removed them if masked_ANDRemoved = True.:{Masked_data} \\n \")\n",
    "    # print(f\" Masked values coordinates only where masked value - {maskValue} :\\n  {maskedValues_coordsOnly} \\n \")\n",
    "    # print(f\" filtered data (return a list of nonzero values from the data) after masking with zero(making them 0) for given mask value - {maskValue} and removing those zeros. :\\n {filtered_Data_WithoutZero} \\n \")\n",
    "    # print(f\" coordinates of the unmasked values only:\\n, it returns list of list (nx3 dimension), n is unmasked values.  {UnMasked_coords} \\n \") \n",
    "    # print(f\" mask will be the boolean array of size of data exactly with TRUE and False \\n {mask} \\n \") \n",
    "    # print(f\"Masked values extracted background only, it is list of msked values, size of list is equal to the number  of maskValue in the data. :\\n {maskedValuesExtractedbg} \\n \")\n",
    "print(f\" !-------------------------  process is completed successfuly -------------------------! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "# from path_manager import addpath\n",
    "# paths = addpath()\n",
    "from preprocessAll import DataPreprocessor\n",
    "from data_saver_module import DataSaver\n",
    "\n",
    "save_dir = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\results\\test\"\n",
    "# arr1 = np.array([1, 3.3, 4, 5, 6, 3, 8, 9, 3.3, 3.3, 3.3])\n",
    "arr1 = np.random.randint(0, 10, size=(10, 10, 10)).astype(float)\n",
    "\n",
    "arr1[0, 0, 0] = 3.3\n",
    "arr1[1, 1, 1] = 3.3\n",
    "arr1[2, 2, 2] = 3.3\n",
    "arr1[0, 1, 2] = 3.3\n",
    "arr1[3, 3, 3] = 3.3\n",
    "arr1[4, 4, 4] = 3.3 \n",
    "arr1[0, 0, 1] = 3.3\n",
    "arr1[0, 0, 2] = 3.3\n",
    "arr1[0, 0, 3] = 3.3\n",
    "arr1[0, 0, 4] = 3.3\n",
    "arr1[0, 0, 5] = 3.3\n",
    "arr1[0, 0, 6] = 3.3\n",
    "arr1[0, 0, 7] = 3.3\n",
    "arr1[0, 0, 8] = 3.3 \n",
    "arr1[0, 0, 9] = 3.3\n",
    "arr1[0, 1, 0] = 3.3\n",
    "arr1[0, 1, 1] = 3.3 \n",
    "arr1[0, 1, 2] = 3.3\n",
    "print(\n",
    "    f\"arr1 shape:{arr1.shape} \\n and \\n {arr1} ------------------------------------->\\n \")\n",
    "\n",
    "maskValue = 3.3\n",
    "Masked_data, maskedValues_coordsOnly, filtered_Data_WithoutZero, UnMasked_coords, mask, maskedValuesExtractedbg = DataPreprocessor.DataMasker(\n",
    "    arr1, maskValue=maskValue, masked_WithZero=True, masked_ANDRemoved=False)\n",
    "\n",
    "print(\n",
    "    f\"Masked data whether masked values is - {maskValue} and made them zeros or just removed.:\\n {Masked_data} \\n \")\n",
    "print(\n",
    "    f\"_Masked values coordinates only where masked value - {maskValue} :\\n  {maskedValues_coordsOnly} \\n \")\n",
    "print(\n",
    "    f\"filtered data after masking with zero(making them 0) for given mask value - {maskValue} and removing those zeros. :\\n {filtered_Data_WithoutZero} \\n \")\n",
    "print(f\"coordinates of the unmasked values only:\\n {UnMasked_coords} \\n \")\n",
    "print(f\"mask will be the boolean array of TRUE and False \\n {mask} \\n \")\n",
    "print(\n",
    "    f\"Masked values extracted background only, it is list of msked values, size of list is equal to the number  of maskValue in the data. :\\n {maskedValuesExtractedbg} \\n \")\n",
    "\n",
    "\n",
    "DataSaver.save_masked_Unmasked_into_npy_mat(save_dir=save_dir, base_name=f\"test\", Masked_data=Masked_data,\n",
    "                                                    masked_coords=maskedValues_coordsOnly, filtered_data=filtered_Data_WithoutZero, unmasked_coords=UnMasked_coords, mask=mask)\n",
    "\n",
    "# print(f\"Masked arr1 for 3.3:{res[0]} \\n \")\n",
    "# print(f\"NON_Masked data  arr1 for 3.3:\\n {res[1]} \\n \")\n",
    "# print(f\"NON_Masked coord  arr1 for 3.3:\\n {res[2]} \\n \")\n",
    "# # print(f\"Masked coord for arr1 for 3.3:\\n {res[2]} \\n \")\n",
    "# print(f\"last background only  arr1 for 3.3:\\n {res[-1]} \\n \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \n",
    "arr11 = np.array([1, 3.3, 4, 5, 6, 3, 8, 9, 3.3, 3.3, 3.3,3.30])\n",
    "print(f\"arr11 shape:{arr11.shape} \\n and \\n {arr11} ------------------------------------->\\n \")\n",
    "arr11 = arr11.reshape((3, 4))\n",
    "print(f\"arr11 shape:{arr11.shape} \\n and \\n {arr11} ------------------------------------->\\n \")\n",
    "# arr11 = np.random.randint(0, 10, size=(10, 10, 10)).astype(float)\n",
    "mask = arr11 == 3.3\n",
    "print(f\"maks:{mask}\")\n",
    "# data1 = arr11*mask\n",
    "print(f\"data1:{arr11[:2,:2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important finding regarding the background.\n",
    "    - because for cluster zero which are the masked values with zeros, and in my data as well as measuring instruments the sample is totally surrounded by the water (1.334 masked with zero here in my data), so it's shows cube as well as the surface surrounding the sample also. which seems to be as main data plot, but it's surface of water refractive index surrounding the sample (main data).,\n",
    "#### - for quantitative justification: calculate the number of background data points and Masked_data(foreground) data points, this should be equal to my total data points. all calculation should be for > 0 values - so first make data[data > 0 ] . \n",
    "### - in this way we can say also that how much percentage of my total data is background and main data.\n",
    "#### - will add in future ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this  script load file one by one from the given directory, mask the data with the given mask value, and save the masked and unmasked data into .mat/.npy format in given new directory.\n",
    "# which will be plotted using the matlab .\n",
    "import os\n",
    "from listspecificfiles import readlistFiles\n",
    "from preprocessAll import DataPreprocessor\n",
    "# from plot3dint import plot3dinteractive\n",
    "from plot_dataModule import DataPlotter\n",
    "from data_saver_module import DataSaver\n",
    "\n",
    "filepath1 = r\"data\\raw_npyData\"\n",
    "\n",
    "fpath = readlistFiles(filepath1, '.npy').file_with_Path()\n",
    "\n",
    "sepeareted_background_mainData = RES_DIR / \"sepeareted_background_mainData\"\n",
    "os.makedirs(sepeareted_background_mainData, exist_ok=True)\n",
    "\n",
    "for file in fpath:\n",
    "    filename = os.path.basename(file)\n",
    "    filenameWithoutExtension = filename.split('.')[0]\n",
    "    print(\n",
    "        f\"Processing file: {filename} and its name without extension: {filenameWithoutExtension}\")\n",
    "\n",
    "    if filename.endswith('.npy') and filename != 'tomo_grafene_48h.npy':\n",
    "        data = np.load(file)\n",
    "\n",
    "        # Masked_data, maskedValues_coordsOnly, filtered_Data_WithoutZero, UnMasked_coords, mask, bgDataonly = DataPreprocessor.DataMasker(data, maskValue=1.334, masked_WithZero=True, masked_ANDRemoved=False)\n",
    "        Masked_data, maskedValues_coordsOnly, filtered_Data_WithoutZero, UnMasked_coords, mask, bgDataonly = DataPreprocessor.DataMasker(\n",
    "            data, maskValue=1.334, masked_WithZero=True, masked_ANDRemoved=False)\n",
    "        \n",
    "        # this below code is to save the masked and unmasked data into .npy and .mat format.\n",
    "        # DataSaver.save_masked_Unmasked_into_npy_mat(save_dir=sepeareted_background_mainData, base_name=filenameWithoutExtension, Masked_data=Masked_data,\n",
    "        #                                             masked_coords=maskedValues_coordsOnly, filtered_data=filtered_Data_WithoutZero, unmasked_coords=UnMasked_coords, mask=mask, bgDataonly=bgDataonly)\n",
    "\n",
    "\n",
    "        \n",
    "        # this is to save and calculate the background and foreground stats for the masked data.\n",
    "        DataSaver.save_background_foreground_stats(data,Masked_data, maskedValues_coordsOnly, filtered_Data_WithoutZero, UnMasked_coords, mask, bgDataonly, filenameWithoutExtension, save_dir=sepeareted_background_mainData)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# this script is to plot the raw data from the given directory, and mark the peaks and find left and right edge of the peak. annotate them, \n",
    "# it will plot the data in 2D and 3D format.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "# print(f\"garbage path : {os.getcwd()} and \\n {Path.home()}\")\n",
    "# cws = Path(__file__).resolve().parent   # use it everywhere for current working scripts path (cws)\n",
    "cws = Path.cwd()\n",
    "BASE_DIR = cws.parent             # \\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\\n",
    "DATA_PATH = BASE_DIR /\"data\"      # \\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\\n",
    "rawnypyData_Path = DATA_PATH/\"raw_npyData\"\n",
    "data_dir = str(rawnypyData_Path)\n",
    "print(f\"data_dir : {data_dir}\")\n",
    "\n",
    "\n",
    "save_dir = BASE_DIR/\"results\"/\"rawData_XYPlot\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create if missing\n",
    "\n",
    "plot_Type = int(input(\"enter value 0/1: for simple:0 Complex :1\"))\n",
    "t_start = time.time()\n",
    "\n",
    "# plot_data_list = [] # to collcet all data(y and x axis both) each for loop. \n",
    "\n",
    "plotON_OFF = input(\"want plot: on else it will not plot saev Time enter ---> : \")\n",
    "\n",
    "if plot_Type == 0:\n",
    "\n",
    "    for file in os.listdir(data_dir):\n",
    "\n",
    "        if file.endswith('.npy'):\n",
    "            filename = os.path.join(data_dir, file)\n",
    "            print(f\"filename:{file}\")\n",
    "            data = np.load(filename)\n",
    "            data = data.flatten()\n",
    "            # data = data.reshape(-1, 1)\n",
    "            # created the x_axis with it's min and max values and uniformly distributed between them.\n",
    "            x_data = np.linspace(data.min(), data.max(), len(data))\n",
    "            \n",
    "            # plot_data_list.append((x_data,data,file[:-4])) # this is for collect all x,y and filename to plot further for subplot in next cell.\n",
    "\n",
    "            if plotON_OFF =='on':\n",
    "                # plot the data\n",
    "                plt.figure(figsize=(8, 5))  # Wider aspect ratio\n",
    "                plt.plot(x_data, data, '.', color='royalblue', markersize=0.4, alpha=0.8)\n",
    "                plt.title(f\"{file[:-4]}\", fontsize=14, fontweight='bold')\n",
    "                plt.xlabel('Linearly Spaced values from Data Itself', fontsize=12, fontweight='bold')\n",
    "                plt.ylabel('RI Data', fontsize=12, fontweight='bold')\n",
    "\n",
    "                plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "                plt.xticks(fontsize=10)\n",
    "                plt.yticks(fontsize=10)\n",
    "                plt.tight_layout()\n",
    "            \n",
    "\n",
    "            # save_dir = BASE_DIR/\"results\"/\"rawData_XYPlot\"\n",
    "            save_path = os.path.join(str(save_dir), f\"{file[:-4]}.png\")\n",
    "\n",
    "            if f\"{file[:-4]}.png\" not in os.listdir(save_dir):\n",
    "                print(f\"saving the file in save_path: {save_path}\")\n",
    "                plt.savefig(save_path, dpi=600)\n",
    "            else:\n",
    "                print(f\"file is already available in save_path: {save_path}\")\n",
    "            \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "if plot_Type == 1:\n",
    "    countcomplex = 0\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.npy'):\n",
    "            filename = os.path.join(data_dir, file)\n",
    "            print(f\"processing  file, filename: {file}\")\n",
    "            countcomplex += 1\n",
    "\n",
    "            # if countcomplex > 5:\n",
    "            if file[0:10] =='Tomogramma' or file[0:3] == 'AML':\n",
    "                # print(\" i am not skipping when continue is off : only one file is allowed to be plotted\")\n",
    "                print(f\"skipping the file: {file} and countcomplex: {countcomplex}\")\n",
    "                continue\n",
    "\n",
    "            data = np.load(filename).flatten()\n",
    "\n",
    "            x_data = np.linspace(data.min(), data.max(), len(data))\n",
    "\n",
    "            # === 1. Find horizontal \"flat line\" band ===\n",
    "            # Detect most frequent value range using histogram\n",
    "            hist_vals, bin_edges = np.histogram(data, bins=10000)\n",
    "            dominant_bin_index = np.argmax(hist_vals)\n",
    "            bin_start = bin_edges[dominant_bin_index]\n",
    "            bin_end = bin_edges[dominant_bin_index + 1]\n",
    "\n",
    "            flat_band = data[(data >= bin_start) & (data < bin_end)]\n",
    "\n",
    "            if flat_band.size == 0:\n",
    "                print(\"No flat band detected.\")\n",
    "\n",
    "                continue\n",
    "\n",
    "            flat_min = np.min(flat_band)\n",
    "            flat_max = np.max(flat_band)\n",
    "            flat_mean = np.mean(flat_band)\n",
    "\n",
    "            # === 2. Plot with color-coded data ===\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            scatter = plt.scatter(x_data, data, c=data, cmap='viridis', s=0.4)\n",
    "\n",
    "            # === 3. Draw horizontal lines for flat values ===\n",
    "            flat_y_vals = [(\"Min\", flat_min, 'red'),\n",
    "                           (\"Mean\", flat_mean, 'blue'),\n",
    "                           (\"Max\", flat_max, 'black')]\n",
    "            # used_y = []\n",
    "\n",
    "            # for label, y_val, color in flat_y_vals:\n",
    "            #     plt.axhline(y=y_val, color=color, linestyle='--', linewidth=1)\n",
    "\n",
    "            #     # === 4. Auto offset to avoid text overlap ===\n",
    "            #     offset = 0\n",
    "            #     while any(abs((y_val + offset) - y) < 0.002 * (flat_max - flat_min) for y in used_y):\n",
    "            #         offset += 0.002 * (flat_max - flat_min)\n",
    "            #     y_text = y_val + offset\n",
    "            #     used_y.append(y_text)\n",
    "\n",
    "            #     # Text with position\n",
    "            #     plt.text(x_data[0], y_text, f'{label}: {y_val:.8f}',\n",
    "            #              color=color, fontsize=6, verticalalignment='bottom')\n",
    "            \n",
    "            used_y = []\n",
    "            x_text_position = x_data[0] - 0.0002*(x_data[-1] - x_data[0])  # shift right\n",
    "            \n",
    "\n",
    "            if file[-5:-4] =='h':\n",
    "                text_gap = 0.12* max(abs(flat_max - flat_min), 1e-1)  # vertical spacing\n",
    "            else:\n",
    "                text_gap = 0.05* max(abs(flat_max - flat_min), 1e-1)  # vertical spacing\n",
    "                \n",
    "\n",
    "            for i, (label, y_val, color) in enumerate(flat_y_vals):\n",
    "                y_text = y_val + i * text_gap  # stagger vertically (fixed gap)\n",
    "                plt.axhline(y=y_val, color=color, linestyle='--', linewidth=0.6)\n",
    "\n",
    "                plt.text(x_text_position, y_text, f'{label}: {y_val:.16f}',\n",
    "                        color=color, fontsize=8,\n",
    "                        verticalalignment='bottom', horizontalalignment='left',\n",
    "                        bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))\n",
    "\n",
    "\n",
    "            # === 5. Plot settings ===\n",
    "            plt.title(f\"{file[:-4]}\")\n",
    "            plt.xlabel('Linearly spaced vector from data')\n",
    "            plt.ylabel('RI Values')\n",
    "            plt.grid(True)\n",
    "            cbar = plt.colorbar(scatter)\n",
    "\n",
    "            cbar.set_label('Intensity')\n",
    "\n",
    "            # Save the figure\n",
    "            plt.tight_layout()\n",
    "            # plt.show() # if we put it here then it will show the plot  and the clear it before saving.\n",
    "\n",
    "            save_path = os.path.join(save_dir, f\"{file[:-4]}_marked.png\")\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "            print(f\"saving the file in save_path: {save_path}\")\n",
    "\n",
    "            # if f\"{file[:-4]}_marked.png\" not in os.listdir(save_dir):\n",
    "            #     print(f\"saving the file in save_path: {save_path}\")\n",
    "            #     plt.savefig(save_path, dpi=300)\n",
    "            # else:\n",
    "            #     print(f\"file is already available in save_path: {save_path}\")\n",
    "            # Show the plot\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "t_end = time.time()\n",
    "print(f\"Total Time --------------->: {t_end - t_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from path_manager import addpath\n",
    "path = addpath()\n",
    "from listspecificfiles import readlistFiles\n",
    "from preprocessAll import DataPreprocessor\n",
    "\n",
    "summary = []\n",
    "datpath  = r\"data/raw_npyData\"\n",
    "fpath = readlistFiles(datpath,'.npy').file_with_Path()\n",
    "count = 0\n",
    "for file in fpath:\n",
    "    count +=1\n",
    "    data = np.load(file)\n",
    "    # data_original = data.copy()\n",
    "    # data_zero = data[data > 0]\n",
    "    for d in range(2,6):\n",
    "        binwidth  = 10**-d\n",
    "        DataPreprocessor.histogram_plotWithBinwidthVariation(data, binwidth = binwidth)\n",
    "        # if data.ndim >= 2:\n",
    "        #     data = data.flatten()\n",
    "        # else:\n",
    "        #   # data.ndim == 1:\n",
    "        #     data = data\n",
    "        \n",
    "        # binwidth = (max(data) - min(data)) / nbins \n",
    "        # nbins = (np.max(data) - np.min(data)) / binwidth\n",
    "        # print(f\"nbins before absolute:{nbins}\")\n",
    "        # nbins = int(np.abs(nbins))\n",
    "\n",
    "        # print(f\"dim of data :{data.ndim} \\n nbins:{nbins} \")\n",
    "    if count == 2:\n",
    "        break\n",
    "#     non_zero_count = data_zero.shape[0]\n",
    "#     zero_count = data_original[data_original == 0].shape[0]\n",
    "#     Negative_count = data_original[data_original < 0].shape[0]\n",
    "\n",
    "#     # print(f\"Data file   : {os.path.basename(file)}\\n Total size    : {data.shape}\\n non zeros values in Data  :{data_zero.shape}\\n Zero Values  :{data_original[data_original<=0].shape}\")\n",
    "#     summary.append({\n",
    "#         \"Data File\": os.path.basename(file),\n",
    "#         \"Total Size\":   data.shape,\n",
    "#         \"Non-Zero Values\": non_zero_count,\n",
    "#         \"Zero Values\": zero_count,\n",
    "#         \"Negative Counts\": Negative_count\n",
    "#     })\n",
    "# # Create DataFrame\n",
    "# df = pd.DataFrame(summary)\n",
    "\n",
    "# # Optional: Wrap long file names\n",
    "# df[\"Data File\"] = df[\"Data File\"].apply(lambda x: '\\n'.join(textwrap.wrap(x, width=10)))\n",
    "\n",
    "# # Print as table\n",
    "# print(tabulate(df, headers='keys', tablefmt='None', showindex=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# for d in range(5):\n",
    "#     binwidth  = 10**-d\n",
    "#     print(f\"binwidth:{binwidth}\")\n",
    "# d = np.random.randint(1,10,size=(10))\n",
    "# # print(f\"d:{d}\")\n",
    "# counts,bin_edges = np.histogram(d,bins='auto')\n",
    "\n",
    "# # plt.plot()\n",
    "# print(f\" counts:    {counts}\")\n",
    "# print(f\" bin_edges:    {bin_edges}\")\n",
    "# print(f\" data:    {d}\")\n",
    "# peak_index = np.argmax(counts)\n",
    "# print(f\" peak_index:    {peak_index}\")\n",
    "# peak_range = bin_edges[peak_index+1] - bin_edges[peak_index] # bin edges where peak founds\n",
    "# peakValue = (bin_edges[peak_index] + bin_edges[peak_index+1])/2 # bin centre where peak found\n",
    "# peak_center = np.mean(peak_range)\n",
    "\n",
    "# # print(f\" peakValue:    {peakValue}\")\n",
    "# print(\"Peak bin range:\", peak_range)\n",
    "# print(\"Peak bin center (most common value approx.):\", peak_center)\n",
    "\n",
    "#------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "d = np.random.randint(1, 10, size=(10))\n",
    "\n",
    "# Compute histogram\n",
    "counts, bin_edges = np.histogram(d, bins='auto')\n",
    "\n",
    "# Find peak bin index\n",
    "peak_index = np.argmax(counts)\n",
    "# Calculate bin range and peak value\n",
    "peak_bin_start = bin_edges[peak_index]\n",
    "peak_bin_end = bin_edges[peak_index + 1]\n",
    "peak_range = (peak_bin_start, peak_bin_end)\n",
    "peak_center = (peak_bin_start + peak_bin_end) / 2\n",
    "\n",
    "# Plot histogram\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(d, bins='auto', edgecolor='black',alpha = 0.4)\n",
    "ax.axvline(peak_center, color='yellow', linestyle='--', label='Peak Center')\n",
    "\n",
    "# Annotate peak info\n",
    "ax.text(peak_center, max(counts), f'Peak\\n{peak_center:.2f}', \n",
    "        ha='center', va='bottom', fontsize=10, color='red')\n",
    "\n",
    "ax.set_title(\"Histogram with Peak Highlighted\")\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.legend()\n",
    "\n",
    "# Prepare debug info to print\n",
    "debug_info = {\n",
    "    # \"Data\": d.tolist(),\n",
    "    \"Counts\": counts.tolist(),\n",
    "    \"Bin Edges\": bin_edges.tolist(),\n",
    "    \"Peak Index\": int(peak_index),\n",
    "    \"Peak Bin Range\": peak_range,\n",
    "    \"Peak Center\": float(peak_center)\n",
    "}\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig_path = \"data/histogram_peak_plot.png\"\n",
    "# fig.savefig(fig_path)\n",
    "\n",
    "# debug_info, fig_path\n",
    "print(f\"debug_info:\\n {debug_info}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the command palette (Ctrl+Shift+P) and search for \"Notebook: Collapse Cell Input\" to collapse selected cells.\n",
    "\n",
    "\n",
    "\n",
    "# arr1 = np.array([1,3.3,4,5,6,3,8,9,3.3,3.3,3.3])\n",
    "# print(f\"arr1:{arr1}\")\n",
    "# mask = arr1 == 3.3\n",
    "# print(f\"mask: {mask}\")\n",
    "# arr1[mask] = 0\n",
    "# print(arr1)\n",
    "\n",
    "# arr1 = np.array([1,3.3,4,5,6,3,8,9,3.3,3.3,3.3])\n",
    "# print(f\"\\n \\n ----------\\n arr1:{arr1}\")\n",
    "# mask = arr1 == 3.3\n",
    "# arr1 = arr1[~mask]\n",
    "# print(arr1)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------  Another Example: --------------------------\n",
    "\n",
    "## What FFT Actually Does:\n",
    "## It breaks down a complex signal (e.g., a 1D array of numbers) into a sum of sine and cosine waves of different frequencies.\n",
    "\n",
    "## Each output component tells how much of that frequency is present in your data.\n",
    "\n",
    "### - Practical Uses of FFT:\n",
    "### - Use Case\tExplanation\n",
    "### - Signal Processing\tDetect repeating patterns in time-series (audio, EEG, etc.)\n",
    "### - Tomography / Imaging\tSpot structural periodicities or texture repetition\n",
    "### - Vibration Analysis\tFind dominant oscillations or faults in machines\n",
    "### - Compression (JPEG, MP3)\tRemove high-frequency noise or unimportant parts\n",
    "### - Noise Filtering\tIdentify and suppress frequencies corresponding to noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## - What FFT Actually Does:\n",
    "## - It breaks down a complex signal (e.g., a 1D array of numbers) into a sum of sine and cosine waves of different frequencies.\n",
    "## - Each output component tells how much of that frequency is present in your data.\n",
    "\n",
    "### - Practical Uses of FFT:\n",
    "### - Use Case\tExplanation\n",
    "### - Signal Processing\tDetect repeating patterns in time-series (audio, EEG, etc.)\n",
    "### - Tomography / Imaging\tSpot structural periodicities or texture repetition\n",
    "### - Vibration Analysis\tFind dominant oscillations or faults in machines\n",
    "### - Compression (JPEG, MP3)\tRemove high-frequency noise or unimportant parts\n",
    "### - Noise Filtering\tIdentify and suppress frequencies corresponding to noise -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_dataModule import DataPlotter\n",
    "from pathlib import Path \n",
    "from listspecificfiles import readlistFiles\n",
    "\n",
    "\n",
    "base_dir = BASE_DIR\n",
    "data_dir = BASE_DIR / \"data\" / \"processed\" / \"main_fgdata\"  # Path to the directory containing .npy files\n",
    "data_dir = Path(data_dir)  # Convert to Path object for consistency\n",
    "\n",
    "print(f\"data_dir: {data_dir}\")  # Print the relative path for clarity\n",
    "\n",
    "\n",
    "save_dir = RES_DIR / \"procesed_fgData_XYPlot\"\n",
    "save_dir = Path(save_dir)  # Convert to Path object for consistency\n",
    "print(f\"save_dir: {save_dir}\")  # Print the relative path for clarity\n",
    "\n",
    "\n",
    "save_dir.mkdir(parents = True, exist_ok=True)  # Create if missing\n",
    "\n",
    "plotter = DataPlotter(data_dir=data_dir, base_dir=base_dir, save_results=True, save_dir=save_dir) \n",
    "# because self.files is already defined in the DataPlotter class, constructor.\n",
    "# it will read all files from the data_dir and save them in self.files. can call  it using:  plotter.files\n",
    "\n",
    "# Ensure plotter.files is iterable (list of files)\n",
    "# files = plotter.files if isinstance(plotter.files, (list, tuple)) else [plotter.files]\n",
    "\n",
    "\n",
    "for file in plotter.files:\n",
    "    \n",
    "    filename = file.split('.')[0]  # Get the filename without extension\n",
    "    print(f\"file name with extension: {file} \\n filename without extension : {filename}\\n\")\n",
    "\n",
    "    # DataPlotter(data_dir = data_dir, base_dir = base_dir, save_results=True, save_dir = save_dir).plot_complex(file = fileWithpath, save_name = filename)\n",
    "\n",
    "    plotter.plot_simple(file = file)\n",
    "    # plotter.plot_complex(file = file, save_name = filename)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file\n",
    "file.split('.npy')[0] # Get the filename without extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def research_plot(data_List, title = None, xlabel = None, ylabel = None,figuresize = None):\n",
    "    \"\"\" \n",
    "    # This is done to plot all the raw plot in single window for comarison i.e. -> Create 4x2 subplots for 8 plots\n",
    "    # Therefore plot_data_list is a list created, And data is appended in each loop as  following :\n",
    "    # plot_data_list.append((x_data,data,file[:-4])) # this is for collect all x,y and filename to plot further for subplot in next cell. \n",
    "\n",
    "    \"\"\"\n",
    "    plot_data_list = data_List\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(11.7, 8.3))  # A4 size in inches (landscape)\n",
    "    axes = axes.flatten()  # Flatten to use in a loop\n",
    "\n",
    "    for i, (x_data, data, title) in enumerate(plot_data_list):\n",
    "        ax = axes[i]\n",
    "        ax.plot(x_data, data, '.', color='royalblue', markersize=0.4, alpha=0.8)\n",
    "        ax.set_title(title, fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Linearly Spaced Vector', fontsize=8)\n",
    "        ax.set_ylabel('Original Values', fontsize=8)\n",
    "        ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.6)\n",
    "        ax.tick_params(axis='both', labelsize=10)\n",
    "\n",
    "    # Hide unused subplots if any\n",
    "    for j in range(len(plot_data_list), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"combined_8_plots.png\", dpi=600, bbox_inches='tight')  # Save for publication\n",
    "    plt.show()\n",
    "\n",
    "# for not to plot comment it ! ------------------------------------------------\n",
    "# data_List = plot_data_list\n",
    "# research_plot(data_List, title = None, xlabel = None, ylabel = None, figuresize = None)\n",
    "\n",
    "###################################### for FFT #################################################\n",
    "\"\"\" \n",
    "FFT is not for counting values, but for finding hidden repetition rates (periodic structures).\n",
    "\n",
    "In 3D tomogram analysis, applying FFT can show regular repeating layers or textures in volumetric data.\n",
    "\n",
    "For data with high decimal precision, FFT won’t show “decimal digit frequency”, but repetition in intensity patterns.\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# A small signal: sine + cosine\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "signal = np.sin(2 * np.pi * 3 * x) + 0.5 * np.cos(2 * np.pi * 7 * x)\n",
    "\n",
    "# FFT\n",
    "fft_vals = np.fft.fft(signal)\n",
    "fft_freqs = np.fft.fftfreq(len(signal), d=x[1] - x[0])\n",
    "magnitude = np.abs(fft_vals)\n",
    "\n",
    "# Only show positive frequencies\n",
    "mask = fft_freqs > 0\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(fft_freqs[mask], magnitude[mask])\n",
    "plt.title(\"FFT Magnitude Spectrum\")\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# metadata1 = np.random.rand(1,100)\n",
    "# # metadata1\n",
    "# plt.plot(metadata1,'.')\n",
    "# plt.show()\n",
    "# yinsort = []\n",
    "\n",
    "# ---------- for ------------------------------------\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "\n",
    "# print(f\"garbage path : {os.getcwd()} and \\n {Path.home()}\")\n",
    "# cws = Path(__file__).resolve().parent   # use it everywhere for current working scripts path (cws)\n",
    "cws = Path.cwd()\n",
    "BASE_DIR = cws.parent             # \\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\\n",
    "DATA_PATH = BASE_DIR /\"data\"      # \\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\\n",
    "rawnypyData_Path = DATA_PATH/\"raw_npyData\"\n",
    "data_dir = str(rawnypyData_Path)\n",
    "print(f\"data_dir : {data_dir}\")\n",
    "\n",
    "save_dir = BASE_DIR/\"results\"/ \"ThresholdBackground\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create if missing\n",
    "\n",
    "# Usage Example:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    for file in data_dir:\n",
    "        if file.endswith('.npy'):\n",
    "\n",
    "            path = Path(data_dir,file)\n",
    "            # filename = os.path.basename(path)\n",
    "            data = np.load(path)\n",
    "\n",
    "            analyzer = HistogramPeakAnalyzer(data)\n",
    "            analyzer.compute_histogram()\n",
    "            peaks, counts = analyzer.detect_peaks(height_ratio=0.2)\n",
    "            value_range = analyzer.get_most_frequent_range()\n",
    "\n",
    "            print(f\"Detected Peaks at: {peaks}\")\n",
    "            print(f\"Most Frequent Value Range: {value_range}\")\n",
    "\n",
    "            analyzer.plot_histogram_with_peaks(title=f\"{file[:-4]}\", save_path=f\"{save_dir}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# decimal_place = 3\n",
    "# min_bw = 10 ** -(decimal_place + 1)\n",
    "# max_bw = 10 ** -(decimal_place)\n",
    "# # bw_list = np.linspace(min_bw, max_bw, 50)\n",
    "# step=10**-(decimal_place+2)\n",
    "# bw_list = range(min_bw, max_bw,step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code: utilizes the PreProcessAll Methods and produce Results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "from path_manager import addpath\n",
    "path = addpath()\n",
    "from listspecificfiles import readlistFiles\n",
    "import json\n",
    "# from preprocessAll import DataPreprocessor\n",
    "\n",
    "from preprocessAll import BinWidthExplorer\n",
    "filepath1 = r\"data\\raw_npyData\"\n",
    "\n",
    "fpath = readlistFiles(filepath1,'.npy').file_with_Path()\n",
    "\n",
    "filepath2Target = r\"results\\Data_SignificantDecimalDigits\"\n",
    "fpath2 = readlistFiles(filepath2Target,'.json').file_with_Path()\n",
    "\n",
    "for file in fpath2:\n",
    "    fileNameExtracted = os.path.basename(file) \n",
    "    if fileNameExtracted == \"extracted_summary.json\":\n",
    "        print(f\"fileNameExtracted: {fileNameExtracted}\")\n",
    "        with open(file, 'r') as f:\n",
    "            summarydata = json.load(f)\n",
    "\n",
    "\n",
    "# print(f\"summary data: {summarydata}\")\n",
    "base_save_dir = RES_DIR / \"BinWidthExplorer\"\n",
    "os.makedirs(base_save_dir, exist_ok=True)  # Create if missing      \n",
    "\n",
    "        \n",
    "\n",
    "countf = 0\n",
    "for file in fpath:\n",
    "    countf += 1\n",
    "    fileName = os.path.basename(file)  # these are the original data files.\n",
    "    key = fileName[:-4] \n",
    "    print(f\" Filename     :{fileName}\\n Fullfilepath  :{file}\\n key:     {key} and type of key: {type(key)}\")\n",
    "    data = np.load(file)\n",
    "    \n",
    "    explorer = BinWidthExplorer(data, metadata=None)\n",
    "    # if key == \"tomo_grafene_48h\":\n",
    "    #     # target_peak = summarydata[key]['mean'] \n",
    "    #     continue # ignoring the 'tomo_grafene_48h' file.\n",
    "    # else:\n",
    "    #     # target_peak = summarydata[key]['mode'] \n",
    "    #     target_peak = summarydata[key]['mean'] \n",
    "\n",
    "    if key !=\"tomo_grafene_48h\":\n",
    "        print(f\" <-------------I am ignoring this key Value: {key}--> because key is <- tomo_grafene_48h ------> \")\n",
    "        target_peak = summarydata[key]['mean'] \n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "    print(f\"target_peak: {target_peak} for key: {key}\")  \n",
    "\n",
    "    decimal_place = summarydata[key]['max_decimal_digit'] \n",
    "    print(f\"decimal_place: {decimal_place}\")\n",
    "\n",
    "    decimal_place = int(decimal_place)  # Convert to integer because it is stored as a string in the JSON file\n",
    "\n",
    "    tolerance = 0.00001 # None  # Adjust tolerance as needed\n",
    "    save_dirpeakIteration =  base_save_dir/ \"peak_fitting\" / fileName[:-4]\n",
    "    os.makedirs(save_dirpeakIteration, exist_ok=True)  # Create if missing\n",
    "    save_dir = save_dirpeakIteration\n",
    "    print(f\"save_dir: {save_dir}\")\n",
    "\n",
    "    result = explorer.evaluate_binwidth_range(decimal_place=decimal_place, bw_listLength = 10, plot=True, save_dir=None, target_peak=target_peak, tolerance=tolerance)\n",
    "\n",
    "           \n",
    "   ## ------------ this below section for the gaussian fit ----------------\n",
    "    peak_range = result['peak_bin_edges']\n",
    "    print(f\"peak_range: {peak_range} for file: {fileName}\")\n",
    "    save_guassian = base_save_dir / \"gaussian_fit\" / fileName[:-4]\n",
    "    os.makedirs(save_guassian, exist_ok=True)  # Create if missing   \n",
    "    print(f\"save_dir for gaussian_fit: {save_guassian}\")\n",
    "    save_dir = save_guassian  # None means current directory\n",
    "    # # print(f\"save_dir: {save_dir}\")\n",
    "    mu, std, peak_data = explorer.fit_gaussian_to_peak(result['peak_bin_edges'], result['binwidth'], key, save_dir = save_dir)\n",
    "   # --------------------------- END of gaussian fit  ----------------\n",
    "\n",
    "\n",
    "    ##  ------------ this below section for the kde fit  ------------\n",
    "    save_kde = base_save_dir / \"kde_fit\" / fileName[:-4]    \n",
    "    os.makedirs(save_kde, exist_ok=True)  # Create if missing\n",
    "    print(f\"save_dir for kde_fit: {save_kde}\")\n",
    "    save_dir = save_kde  # None means current directory\n",
    "    print(f\"save_dir: {save_dir}\")\n",
    "    kde_vals, kde_x = explorer.plot_kde_comparison( result['peak_bin_edges'], result['binwidth'], key, save_dir=save_dir)\n",
    "    # print(f\"kde_vals: {kde_vals} and kde_x: {kde_x} for file: {fileName}\")\n",
    "   # --------------------------- END of kde fit ----------------\n",
    "\n",
    "    #  ------------ this  below section for the GMM fit  ------------\n",
    "    save_gmm = base_save_dir / \"gmm_fit\" / fileName[:-4]\n",
    "    os.makedirs(save_gmm, exist_ok=True)  # Create if missing\n",
    "    print(f\"save_dir for gmm_fit: {save_gmm}\")\n",
    "    save_dir = save_gmm  # None means current directory\n",
    "    print(f\"save_dir: {save_dir}\")\n",
    "    explorer.fit_gmm_and_save(result['peak_bin_edges'], key, n_components=2, ForAllData=None, save_dir=save_dir,save_mat=True, plot=True)\n",
    "   # --------------------------- END of gmm fit ----------------\n",
    "    \n",
    " \n",
    "    # if countf == 8:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This below code is for the utilizing the DatPreprocessor class for qunatile based thresholding and plotting the all in one and showing the qunatile threshold vertical lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import os \n",
    "# from path_manager import addpath\n",
    "# path = addpath()\n",
    "\n",
    "# from preprocessAll import DataPreprocessor\n",
    "# from preprocessAll import BinWidthExplorer\n",
    "\n",
    "# from listspecificfiles import readlistFiles\n",
    "# filepath1 = r\"data\\raw_npyData\"\n",
    "\n",
    "# fpath = readlistFiles(filepath1,'.npy').file_with_Path()\n",
    "\n",
    "# countf = 0\n",
    "# d_test = np.random.normal(0, 1, 100)\n",
    "\n",
    "# for file in fpath:\n",
    "#     countf += 1\n",
    "#     print(f\"{file} and filepath : \")\n",
    "#     # data = np.load(file).flatten()\n",
    "#     # data = data[data>1.336]\n",
    "#     data = d_test  # arbitary data \n",
    "#     DataPreprocessor.quantile_based_auto_threshold_plot(data, n_peaks=6, colors=None, show_subplots=False, save_dir=None)\n",
    "#     if countf == 1:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Testing the certain things and doubts ------------------------------------ \"\"\"\n",
    "\n",
    "# data = np.array([1.0, 1.1, 1.1, 1.2, 1.3, 1.3, 1.3, 1.35, 1.4, 1.5, 1.5, 1.6])\n",
    "# counts, edges = np.histogram(data, bins=5)\n",
    "# print(f\"counts:{counts}\\n edges:{edges}\\n peak:{max(counts)} \")\n",
    "# print(f\"ed : {edges[:-1]} \\n ed_all:{edges[1:]}\")\n",
    "# fig, ax = plt.subplots(figsize=(4,4))\n",
    "# x = np.linspace(0,6,20)\n",
    "# y1 = np.sin(x)\n",
    "# ax.plot(x,y1)\n",
    "# ax.plot(x,np.cos(x))\n",
    "# # plt.show()\n",
    "# x = np.random.randint(0,5,(2,3))\n",
    "# print(x)\n",
    "# # x1= np.ones([2,3])\n",
    "# x1= np.ones_like(x)\n",
    "# x1\n",
    "\n",
    "########## --------------- test 2 -----------------------\n",
    "# import numpy as np\n",
    "# data = np.random.normal(loc=5, scale=1.5, size=1000)\n",
    "# counts, bin_edges = np.histogram(data, bins='auto')\n",
    "# plt.hist(data,bins='auto')\n",
    "# plt.show()\n",
    "# peak_index = np.argmax(counts)\n",
    "# peak_range = (bin_edges[peak_index], bin_edges[peak_index + 1])\n",
    "# peak_center = np.mean(peak_range)\n",
    "\n",
    "# print(\"Peak bin range:\", peak_range)\n",
    "# print(\"Peak bin center (most common value approx.):\", peak_center)\n",
    "\n",
    "# def test3(val1, val2 =None,val3 = False):\n",
    "#     if val2 is not None:\n",
    "#         print(f\"val2 :{val2} amd val1:{val1}\")\n",
    "#     if val3 :\n",
    "#         print(\"val3 is true not False\")\n",
    "# val3 = 12\n",
    "# test3(10, val2 = 5,val3=val3)\n",
    "\n",
    "# data = np.random.randint(1,4,size=(2,2,2))\n",
    "# data1 = data.copy()\n",
    "# # print(data)\n",
    "# mask = (data1 == 1)\n",
    "# print(mask)\n",
    "# data1[mask] = 0\n",
    "\n",
    "# print(f\" After data:\\n{data1}\\n data Previous:\\n{data}\")\n",
    "\n",
    "# class NumberDoubler:\n",
    "#     def __init__(self, value):\n",
    "#         self.value1 = value\n",
    "#         print(f\"before:{self.value1}\")\n",
    "#         # self.value = self.value * 3  # Reassigning to modified value  \n",
    "#         self.value = value * 3  # Reassigning to modified value  \n",
    "#         print(f\"After:{self.value1} and val:{self.value}\")\n",
    "\n",
    "#     def __str__(self):\n",
    "#         return(f\"{self.value1}\")\n",
    "\n",
    "# c = NumberDoubler(8)\n",
    "# print(f\"c:{c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# cov = [[0,0,0],[0,1,0],[0,0,1]]\n",
    "# mean = [0,2,4]\n",
    "# # d_test = np.random.multinomial(100, [1.0 / 3, 2.0 / 3])\n",
    "# d_test = np.random.rand(10000)\n",
    "d_test = np.random.normal(0, 1, 100)\n",
    "\n",
    "plt.hist(d_test)\n",
    "# plt.hist(d_test)\n",
    "plt.show()\n",
    "d1_test = np.copy(d_test)\n",
    "\n",
    "Qvalue = []\n",
    "for i in range(1, 5):\n",
    "    Qi = 1/i\n",
    "    qunatileVal = np.quantile(d_test, Qi)\n",
    "    # Qdata = {f\"Q_{Qi}\": qunatileVal}\n",
    "    print(f\" qi percentile: {Qi*100:.2f} and n\\ Threshold Value for {Qi*100:.2f} percentile: {qunatileVal} \\n --> \") # {d_test[d_test>qunatileVal]}\"\n",
    "    dataP = d_test[d_test>qunatileVal] # data after removing the background.\n",
    "    Qdata = {f\"Q_thres\": qunatileVal, f\"fg_data\":dataP}\n",
    "\n",
    "    Qvalue.append(Qdata)\n",
    "\n",
    "def plot_combined(Qval, fgdata):\n",
    "    # Qy = Qval*np.ones_like(d_test)\n",
    "    x_axis = np.linspace(min(d_test),max(d_test),len(d_test))\n",
    "    \n",
    "    x = Qval*(np.ones_like(x_axis))\n",
    "    Qy = np.linspace(0,20,len(x_axis))\n",
    "    # x = np.ones_like()\n",
    "    \n",
    "    # ax = plt.figure(figsize=(4,4))\n",
    "    ax.hist(d1_test)\n",
    "    ax.hist(fg_data)\n",
    "    ax.plot(x,Qy)\n",
    "    # ax.hist(fg_data,bins='fd')\n",
    "    # plt.show()\n",
    "\n",
    "fig,ax= plt.subplots()\n",
    "\n",
    "for iteration in Qvalue:\n",
    "    QThresValue = iteration[\"Q_thres\"]\n",
    "    fg_data = iteration[\"fg_data\"]\n",
    "\n",
    "    plot_combined(QThresValue,fg_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# x = np.linspace(0, 10, 10)\n",
    "# y1 = np.sin(x)\n",
    "# y2 =  y1 + 2\n",
    "# plt.plot(x, y1, label='sin(x)')\n",
    "# plt.plot(x, y2, label='sin(x) + 2')\n",
    "# plt.show()\n",
    "# file[-5:-4]\n",
    "filepath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw_npyData\\Tomogramma_Cell.npy\"\n",
    "import numpy as np\n",
    "import os\n",
    "filename = os.path.basename(filepath)\n",
    "print(f\"Extracted name is Tomogramam : {filename[0:10]} and length : {len(filename[0:10])}\")\n",
    "\n",
    "# filename = os.path.basename(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# finding resolution of data, so not to skip small varaition,\n",
    "# First just plot Raw Data after making flat, if it is equal and more than 2 dimensions -> data = data.flatten()\n",
    "# x_data = np.linspace(min(data),max(data),len(data)), data = data.flatten() ---> y_data  =  sorted_data = np.sort(data)\n",
    "\n",
    "# binsize = diff(choosen smallest values after sorting the data) : data = data.flatten().reshape(-1,1)\n",
    "# 201*201*201 = 8120601; 200*200*200 = 8000000;\n",
    "# :.2e\t1.23e-05\t2 decimal places\n",
    "# :.8e\t1.23456000e-11\t8 decimal places\n",
    "# # Print with scientific notation and 8 decimal places\n",
    "# print(f\"{value:.8e}\n",
    "\n",
    "from pathlib import Path\n",
    "from listspecificfiles import readlistFiles\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "data_path = r\"data\\raw_npyData\"  # relative path with r\"\"relativepath\"\n",
    "files = readlistFiles(data_path, '.npy')\n",
    "fpath = files.file_with_Path()  # file with path name.\n",
    "BASE_DIR = Path.cwd().parent\n",
    "print(f\"BASE_DIR:--------> {BASE_DIR}\")\n",
    "save_pathtTEXT = BASE_DIR/\"results\"\n",
    "\n",
    "################################ for simple plot of data ##############################################\n",
    "\n",
    "def simple_plot(data, file, savepathplot):\n",
    "    x_data = np.linspace(min(data), max(data), len(data))\n",
    "    # fig = plt.figure(figsize=(10,6))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_data, data, s=0.1)\n",
    "    plt.title(f\"{os.path.basename(file)[:-4]} sortred Data \")\n",
    "    plt.xlabel('Data: linearly spaced data itself')\n",
    "    plt.ylabel('Data:RI_value')\n",
    "    plt.grid(True)\n",
    "    save_dir = BASE_DIR/\"results\"/\"rawData_XYPlot\"/\"plotSortedData\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"{os.path.basename(file)[:-4]}\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "#####################################################################################################\n",
    "\n",
    "textFilesSave = os.path.join(save_pathtTEXT, 'details_Datafile.txt')\n",
    "with open(textFilesSave, 'w') as f:\n",
    "    f.close()\n",
    "# count = 0\n",
    "for file in fpath:\n",
    "    # count +=1\n",
    "    fileName = os.path.basename(file)\n",
    "    # print(f\"FullfilePath:{file} \\n fileName:{fileName}\")\n",
    "\n",
    "    data = np.load(file)\n",
    "    Data_shape = data.shape\n",
    "    data = data.flatten()\n",
    "    sorted_data = np.sort(data)\n",
    "    # simple_plot(sorted_data,file,BASE_DIR)\n",
    "\n",
    "    diff_1data = np.diff(sorted_data)\n",
    "    # diff_2data = np.diff(sorted_data)\n",
    "\n",
    "# ------------------------  found zeros in the diff1, Now filter zeros then go for first and second nonzero values --------------\n",
    "    # Filter out exact or near-zero differences to avoid duplicates\n",
    "    prcsn = 1e-9  # set precison\n",
    "    nonzero_diffs = diff_1data[np.abs(diff_1data) > prcsn]\n",
    "\n",
    "    # Use np.unique to get distinct difference values\n",
    "    unique_diffs = np.unique(nonzero_diffs)\n",
    "\n",
    "    # Extract min and second min, keeping 9 decimal precision\n",
    "    # for unique_val in range(unique_diffs):\n",
    "    min_val = unique_diffs[0]\n",
    "    second_min_val = unique_diffs[1] if len(unique_diffs) > 1 else None\n",
    "\n",
    "    # Print values with high precision\n",
    "    print(\n",
    "        f\"Minimum difference {fileName}  : {min_val:.12f} i.e. {min_val:.12e}\")\n",
    "    if second_min_val is not None:\n",
    "        print(\n",
    "            f\"Second minimum difference {fileName}: {second_min_val:.12f} i.e. {second_min_val:.12e}\")\n",
    "    else:\n",
    "        print(\"No second unique non-zero difference found.\")\n",
    "\n",
    "    # print(f\"RESOLUTION_DATA: {min(diff_1data):.8f} DATA SIZE: {sorted_data.shape} \\n lower value:{sorted_data[0:3]} and upper value:{sorted_data[-4:]}\")\n",
    "    # text_data = f\"{fileName}: Data_shape:{Data_shape}, DATA SIZE: {sorted_data.shape}, RESOLUTION_DATA: {min(diff_1data)},lower value:{sorted_data[0:3]}, upper value:{sorted_data[-3:]}\\n\"\n",
    "    # with open(textFilesSave,'a') as f:\n",
    "    #     f.write(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # np.unique_counts(nonzero_diffs)\n",
    "# a = np.array([1, 2, 4, 3, 2])\n",
    "# # values,count = np.unique(a, return_counts=True) # return_counts -returns the unique value counts.\n",
    "# # print(f\"a:{a.shape} \\n values:{values} \\n count:{count} \\n\")\n",
    "# print(f\"a:{a.shape} \\n\")\n",
    "# # a = np.array([1.3312498765,1.3312498761,1.3312498712,1.3312498763,1.3312498768,1.3312498765])\n",
    "# # a  = np.sort(a)\n",
    "# # a = list(a)\n",
    "# f,count,inv,count1 = np.unique(np.round(a,decimals= 11), return_counts=True,return_inverse=True,return_index = True)\n",
    "# # print(f\"a : {float(a)} and its size: {len(a)}\")\n",
    "# print(f\"a : {a} and its size: {len(a)}\")\n",
    "\n",
    "# # f,idx1,count = np.unique(a,return_index = True, return_counts=True)\n",
    "# f,count1,count,inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# x = 1.01e-9\n",
    "# print(f\"val:{x:.11f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# str1 = '1.000010200000'\n",
    "# tt1 = str1.rstrip('0')   # eliminate the trailing zeros.\n",
    "# tt1\n",
    "# # 'Hello world'.title()  # 'Hello World'\n",
    "# # \"they're bill's friends from the UK\".title() #\"They'Re Bill'S Friends From The Uk\"\n",
    "# # \"42\".zfill(5)  # padding of total 5 including the \"42\", so three zeros (000) will be added at starting. \n",
    "# str2 = 'janakak'\n",
    "# str2.replace('k','R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## - below code find the significant digits in my data and plot the histogram: \n",
    "##💡  it tells you how many digits are non-zero or meaningful in the float’s scientific representation.\n",
    "\n",
    "##💡 It’s a heuristic method — useful for checking how many digits \"matter\" in terms of numerical precision.\n",
    "\n",
    "##💡 np.unique(data): --> it always return the results in the order uniqValues(sorted array of unique values in increasing Order).\n",
    "\n",
    "##💡idx: (first index of each unique values in the original data).\n",
    "\n",
    "##💡inverse_idx:  \n",
    "\n",
    "##💡counts : returns counts of each values in unique array.\n",
    "\n",
    "<!-- Total_res = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    uniqValues,idx,inverse_idx,counts = Total_res  #  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# useful for checking how many digits \"matter\" in terms of numerical precision.\n",
    "\n",
    "from  pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from collections import defaultdict\n",
    "import json\n",
    "from scipy import stats\n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "\n",
    "import csv\n",
    "from listspecificfiles import readlistFiles\n",
    "\n",
    "from plot_dataModule import DataPlotter  # import DataPlotter from plot_dataModule.py file, all the functions are defined realted to plot the histogram and save it in the directory and simple raw data plot.\n",
    "\n",
    "from data_saver_module import DataSaver # import DataSaver from data_saver_module.py file, all the functions are defined related to save the data in the directory.\n",
    "\n",
    "data_path = r\"data\\raw_npyData\"  # relative path with r\"\"relativepath\"\n",
    "files = readlistFiles(data_path,'.npy')\n",
    "fpath = files.file_with_Path()  # file with path name.\n",
    "\n",
    "# Directory to save histograms\n",
    "BASE_DIR = Path.cwd().parent\n",
    "save_dir = BASE_DIR/\"results\"/\"histogram_significantDigits\"\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "\n",
    "\n",
    "############################# this is the function to find number of decimal points values after decimal in right side of decimal (show precision) #####################\n",
    "import math\n",
    "from decimal import Decimal\n",
    "def actual_significant_digits_after_decimal(val):\n",
    "    \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\n",
    "        handle the inf and NaN values also in the data. return zeros in this case.\n",
    "    \"\"\"\n",
    "\n",
    "    if math.isinf(val) or math.isnan(val):\n",
    "        return 0  # Or return None to indicate 'not countable'\n",
    "    \n",
    "    if val == 0:\n",
    "        return 0\n",
    "    d = Decimal(str(val)).normalize()\n",
    "    if '.' not in str(d):\n",
    "        return 0\n",
    "    return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "####################################### This is for the Find the Unique values and used to find Maximum precision value in data. ###########################################################\n",
    "import numpy as np\n",
    "def UniqueValueCount(data):\n",
    "    \"\"\"\n",
    "    give your data in numpy array format 1d: \n",
    "    uniqValues,idx,inverse_idx,counts reurn values in this order\n",
    "    get your results:, it will return Total_res then extract these: uniqValues,idx,inverse_idx,counts\n",
    "    extract like this : uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    \"\"\"\n",
    "    significant_digit_data = data\n",
    "    # uniqValues,idx,inverse_idx,counts = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    Total_res = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    return Total_res\n",
    "    print(f\"\\n unique values:{uniqValues},\\n  Unique counts: {counts} and \\n indexes:{idx}, \\n reverse_idx : {inverse_idx} and \\n orginal Array :{uniqValues[inverse_idx]}\")\n",
    "\n",
    "# data = np.array([1,2,4,3,2])\n",
    "# UniqueValueCount(data)\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "# proceess all files in the directory and generate histogram for each file and save it in the directory. all the auxiliary functions are defined in the same Cell, are used in this code.\n",
    "save_dir1 = BASE_DIR/\"results\"/\"histogram_3splitSgnfctDigits\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "countfile = 0\n",
    "for FileWithPath in fpath:\n",
    "    countfile +=1\n",
    "    print(f\"FileWithPath: {FileWithPath} and countfile: {countfile}\")\n",
    "\n",
    "    # this was just to test the code, so not to process all files in the directory.\n",
    "    # if countfile <=4 or countfile >=6 :\n",
    "    #     print(f\"Ignored file .: {countfile}\")\n",
    "    #     # break\n",
    "    #     continue\n",
    "    \n",
    "    print(f\"Processed file .: {countfile}\")\n",
    "\n",
    "    filename = os.path.basename(FileWithPath)\n",
    "    print(f\"Processing filename: {filename}\")\n",
    "    \n",
    "    # Load and flatten data\n",
    "    data = np.load(FileWithPath)\n",
    "    data = data.flatten()\n",
    "    print(f\"Shape of data: {data.shape[0]}\")\n",
    "    \n",
    "    # Extract first significant digit\n",
    "    # significant_digit_data = [significant_digits(val) for val in data if val != 0]\n",
    "    # significant_digit_data = [actual_significant_digits_after_decimal(val) for val in data if val != 0]\n",
    "\n",
    "    # Step 2: Build significant_digit_data + track actual values using defaultdict\n",
    "    decimal_digit_groups = defaultdict(list)\n",
    "    significant_digit_data = []\n",
    "    csv_rows = []\n",
    "\n",
    "    for val in data:\n",
    "        if val != 0:\n",
    "            d = actual_significant_digits_after_decimal(val)\n",
    "            decimal_digit_groups[d].append(val)\n",
    "            significant_digit_data.append(d)\n",
    "\n",
    "    # Plot histogram\n",
    "    UniqueValues = UniqueValueCount(significant_digit_data)[0]\n",
    "    Counts_OFUniqueValues = UniqueValueCount(significant_digit_data)[3]\n",
    "    x_axis_maxValueHist =  max(UniqueValues)\n",
    "\n",
    "    ##############  for finding the corresponding statistics of each decimal digits precision and saving in json #############################\n",
    "\n",
    "    # BASE_DIR = Path(\".\")  # replace with actual base dir if needed\n",
    "    Data_sgnDecimalDgts_save_dir = BASE_DIR / \"results\" / \"Data_SignificantDecimalDigits\"\n",
    "    os.makedirs(Data_sgnDecimalDgts_save_dir, exist_ok=True)\n",
    "\n",
    "    # Final dictionary to store stat_summary + values\n",
    "    decimal_digit_group_stats = {}   # dictionary with stats and values list \n",
    "    decimal_digit_group_stats_OnlySummary = {} # dictionary of stat summary only\n",
    "\n",
    "    for d, values in decimal_digit_groups.items():\n",
    "        values_arr = np.array(values, dtype=np.float64)\n",
    "\n",
    "        mode_result = stats.mode(values_arr,keepdims=True)\n",
    "        \n",
    "        stat_summary = {\n",
    "            \"count\": int(len(values_arr)),\n",
    "            \"min\": float(np.min(values_arr)),\n",
    "            \"max\": float(np.max(values_arr)),\n",
    "            \"mean\": float(np.mean(values_arr)),\n",
    "            \"median\": float(np.median(values_arr)),\n",
    "            \"mode\": float(mode_result.mode[0]),  \n",
    "            \"modeCounts\": int(mode_result.count[0]),\n",
    "            \"std\": float(np.std(values_arr)),\n",
    "            \"values\": [float(f\"{v:.17g}\") for v in values_arr]\n",
    "        }\n",
    "\n",
    "        decimal_digit_group_stats[str(d)] = stat_summary\n",
    "\n",
    "        print(f\"Decimal Digit: {d}\")\n",
    "        print(f\"  Count         : {stat_summary['count']}\")\n",
    "        print(f\"  Min           : {stat_summary['min']}\")\n",
    "        print(f\"  Max           : {stat_summary['max']}\")\n",
    "        print(f\"  Mean          : {stat_summary['mean']}\")\n",
    "        print(f\"  Median        : {stat_summary['median']}\")\n",
    "        print(f\"  Mode          : {stat_summary['mode']}\")\n",
    "        print(f\"  ModeCounts    : {stat_summary['modeCounts']}\")\n",
    "        print(f\"  Std Dev       : {stat_summary['std']}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    #-------------------  This below section is for saving the summary ONLY  in csv and json file. Each available significant digits details summary only is saved in a corresponding file. \n",
    "          \n",
    "        stats_summary_Without_ValueList = {\n",
    "            \"decimal_digit\": d,\n",
    "            \"count\": int(len(values_arr)),\n",
    "            \"min\": float(np.min(values_arr)),\n",
    "            \"max\": float(np.max(values_arr)),\n",
    "            \"mean\": float(np.mean(values_arr)),\n",
    "            \"median\": float(np.median(values_arr)),\n",
    "            \"mode\": float(mode_result.mode[0]),\n",
    "            \"modeCounts\": int(mode_result.count[0]),\n",
    "            \"std\": float(np.std(values_arr))\n",
    "        }\n",
    "        # Save summary stats for JSON (without raw values)\n",
    "        decimal_digit_group_stats_OnlySummary[str(d)] = stats_summary_Without_ValueList\n",
    "\n",
    "        # Save summary stats for CSV\n",
    "        csv_rows.append(stats_summary_Without_ValueList)\n",
    "\n",
    "        # --------------- Save to JSON and csv for SUMMARY ONLY  -------------------------------------\n",
    "    json_path = Data_sgnDecimalDgts_save_dir / f\"summaryStats_ONLY{filename[:-4]}.json\"\n",
    "    with open(json_path, \"w\") as jf:\n",
    "        json.dump(decimal_digit_group_stats_OnlySummary, jf, indent=4)\n",
    "\n",
    "    # Save CSV\n",
    "    csv_path = Data_sgnDecimalDgts_save_dir / f\"summaryStats_ONLY{filename[:-4]}.csv\"\n",
    "    with open(csv_path, \"w\", newline=\"\") as cf:\n",
    "        writer = csv.DictWriter(cf, fieldnames=csv_rows[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_rows)\n",
    "\n",
    "    json_path, csv_path\n",
    "\n",
    "    # --------------- Save to JSON  for all summary and stats -------------------------------------\n",
    "    save_path = Data_sgnDecimalDgts_save_dir / f\"significantDec{filename[:-4]}.json\"\n",
    "\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(decimal_digit_group_stats, f, indent=4)\n",
    "\n",
    "    print(f\"Stats and values saved to: {save_path}\")   \n",
    "\n",
    "# ---------- !!!!!!!!!!!!!!!!!!!! from here I use continue to avoid further processing ................................ !!!!!!!!!!!!!!!!!\n",
    "    if countfile >= 1:   # For ignoring the saving the histogram and its plotting.\n",
    "        print(f\"will not continue forward from here .....  metdata list\")\n",
    "        continue\n",
    "        \n",
    "    metadata_list = [\n",
    "        {\n",
    "            \"filename\": f\"{filename[:-4]}\",\n",
    "\n",
    "            \n",
    "            \"unique_values\": UniqueValues.tolist(),  # Convert numpy array to list\n",
    "            \"counts\": Counts_OFUniqueValues.tolist(),  # Convert numpy array to list\n",
    "            \"significant_DecimalDigits\": int(x_axis_maxValueHist),  # Convert numpy.int64 to int\n",
    "            \"significant_digit_data\": significant_digit_data  # significant decimal digits for each data value in the data array (which are converted from 3d numpy to 1d array).\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "################################### this is to save the metadata in the json file. #######################################################################################\n",
    "\n",
    "    from data_saver_module import DataSaver\n",
    "    Data_sgnDecimalDgts_save_dir = BASE_DIR/\"results\"/\"Data_SignificantDecimalDigits\"\n",
    "    os.makedirs(Data_sgnDecimalDgts_save_dir, exist_ok=True)  # Create if missing\n",
    "    saver = DataSaver(save_dir=Data_sgnDecimalDgts_save_dir, include_fields=[], filename=f\"{filename[:-4]}.json\")\n",
    "    saver.save(metadata_list)\n",
    "\n",
    "    print(F\"HERE precision  digits in the data values: {x_axis_maxValueHist}\")\n",
    "\n",
    "    # <----------------  JUST CHOOSE ONE OF THE FOLLOWING HISTOGRAMS TO PLOT AND SAVE IT. JUST UNCOMMENT ANY ONE ----------- >\n",
    "\n",
    " ############### 1ST --  invoke the function to plot histogram with color coding and annotation\n",
    "\n",
    "    # if countfile >=1:   # for ignoring the saving the histogram and its plotting.\n",
    "    #     continue\n",
    "\n",
    "    DataPlotter.plot_histogram_with_annotate_counts(\n",
    "        Counts_OFUniqueValues,\n",
    "        UniqueValues,\n",
    "        title=f\"{filename[:-4]}: Annotated Histogram\",\n",
    "        xlabel=f\"Significant Digit (1–{max(UniqueValues)})\",\n",
    "        ylabel=\"Normalized Frequency (Fixed Height)\",\n",
    "        saveplot=False,\n",
    "        filename=filename[:-4],\n",
    "        save_dir=save_dir,\n",
    "        dpi=600  # Publication-ready resolution\n",
    "    )\n",
    "\n",
    "# 2ND -- invoke the function to plot histogram Here HORIZONTAL SPLIT HISTOGRAM : with color coding and annotation\n",
    "    DataPlotter.plot_horizontal_split_histogram(\n",
    "        significant_digit_data=significant_digit_data,\n",
    "        title=f\"{filename[:-4]}: Annotated Histogram\",\n",
    "        xlabel=\"Frequency of Occurrence\",\n",
    "        ylabel=f\"Significant Digit (1–{max(UniqueValues)})\",\n",
    "        saveplot=False,\n",
    "        filename=filename[:-4],\n",
    "        save_dir=save_dir1\n",
    "    )\n",
    "    \n",
    "print(f\"Total number of files processed: {countfile} and plot generated for each file if saveplot is True.\")\n",
    "\n",
    "################################ older version of histogram plot with color coding and annotation ###############\n",
    "    # plt.figure(figsize=(8, 5))\n",
    "    # plt.hist(significant_digit_data,bins='auto')\n",
    "    # plt.title(f\"Significant Digit Histogram: {filename}\")\n",
    "    # plt.xlabel(f\"Significant Digit (1-{max(UniqueValues)})\")\n",
    "    # plt.ylabel(\"Frequency\")\n",
    "    # plt.xticks(range(1, max(UniqueValues)+1))\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    # Save the figure\n",
    "    # hist_path = os.path.join(save_dir, f\"{filename[:-4]}_histogram.png\")\n",
    "    # plt.savefig(hist_path)\n",
    "    # plt.close()\n",
    "    # Optional: Print most common digit\n",
    "    # most_common_digit = np.argmax(counts) + 1  # bins are 1-indexed\n",
    "    # print(f\"Most frequent significant digit: {most_common_digit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def UniqueValueCount(data):\n",
    "    \"\"\"\n",
    "    give your data in numpy array format 1d: \n",
    "    uniqValues,idx,inverse_idx,counts reurn values in this order\n",
    "    get your results:, it will return Total_res then extract these: uniqValues,idx,inverse_idx,counts\n",
    "    extract like this : uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    \"\"\"\n",
    "    significant_digit_data = data\n",
    "    # uniqValues,idx,inverse_idx,counts = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    Total_res = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    print(f\"\\n unique values:{uniqValues},\\n  Unique counts: {counts} and \\n indexes:{idx}, \\n reverse_idx : {inverse_idx} and \\n orginal Array :{uniqValues[inverse_idx]}\")\n",
    "    return Total_res\n",
    "\n",
    "data = np.array([1,2,4,3,2])\n",
    "UniqueValueCount(data)\n",
    "\n",
    "############################# DOES NOT WORKED ALWAYS GAVE 18/19 DECIMAL DIGITS PRECISION FOR ALL DATA --> becasue of formatting f\"{val:.18f}\" \n",
    "\n",
    "# def significant_digits(val):\n",
    "#     \"\"\"Extract first significant digit (ignores sign and zeros).\"\"\"\n",
    "#     val = float(val)\n",
    "#     if val == 0:\n",
    "#         return 0\n",
    "#     my_strVal = str(val)  \n",
    "#     return int(my_strVal.lstrip('-0.')[0])\n",
    "\n",
    "# def decimal_places(val):\n",
    "#         s = f\"{val:.18f}\".rstrip('0')\n",
    "#         if '.' in s:\n",
    "#             return len(s.split('.')[1])\n",
    "#         return 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pathlib import Path\n",
    "# from listspecificfiles import readlistFiles\n",
    "\n",
    "# datapath = r\"results\\Data_SignificantDecimalDigits\"\n",
    "# files = readlistFiles(datapath, '.json')\n",
    "# # files = [filename for filename in files if filename[:]]\n",
    "# fpath = files.file_with_Path()  # fileName with path name.\n",
    "# actualfilename  = []\n",
    "# for file in fpath:\n",
    "#     fileName = os.path.basename(file)\n",
    "#     # print(f\"Processing fileName: {fileName}\")\n",
    "#     if fileName.startswith('summaryStats_ONLY'):\n",
    "\n",
    "#         actualfilename.append(fileName[len('summaryStats_ONLY'):])\n",
    "#         print(f\"Processing fileName: {fileName} and actual filename: {actualfilename[-1]}\")\n",
    "#         with open(file, 'r') as f:\n",
    "#             data = json.load(f)\n",
    "#             if fileName == 'summaryStats_ONLYtomo_grafene_48h.json':\n",
    "#                 print(f\"\\n📂 Processing file: {fileName}\")\n",
    "#                 with open(file, 'r') as f:\n",
    "#                     data = json.load(f)\n",
    "\n",
    "#                 max_count = -1\n",
    "#                 max_key = None\n",
    "\n",
    "#                 for key, stats in data.items():\n",
    "#                     count_val = stats.get(\"count\")\n",
    "#                     if count_val is not None and count_val > max_count:\n",
    "#                         max_count = count_val\n",
    "#                         max_key = key\n",
    "\n",
    "#                 if max_key:\n",
    "#                     stats = data[max_key]\n",
    "#                     print(f\" Key with maximum count: {max_key}\")\n",
    "#                     print(f\" Mean: {stats['mean']}\")\n",
    "#                     print(f\" Std Dev: {stats['std']}\")\n",
    "#                     print(f\"📌 Mode: {stats['mode']}\")\n",
    "#                 else:\n",
    "#                     print(\"⚠️ No valid 'count' found in data.\")\n",
    "\n",
    "\n",
    "#             # Extract statistics for significant digit 3\n",
    "#             else:  \n",
    "#                 meanVal = data[\"3\"][\"mean\"]\n",
    "#                 mode = data[\"3\"][\"mode\"]\n",
    "#                 std = data[\"3\"][\"std\"]\n",
    "#                 print(f\"Mean: {meanVal}, Mode: {mode}, Std Dev: {std}\")\n",
    "            \n",
    "\n",
    "#     # print(f\"Processing fileName:  and filename: {filen1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" this code  read all the json extrcated files with summary and stats of ecah significant decimal digits and extract the maximum count and its corresponding key, mean, std and mode values.\n",
    " and save it in a extracted_summary.json file with only maximum counts statistics values like key filename withot extension , max_decimal_digit: as you can see example :\n",
    " {\n",
    "    \"AML2_cell11\": {\n",
    "        \"max_decimal_digit\": \"3\",\n",
    "        \"max_count\": 7604888,\n",
    "        \"mean\": 1.3339999999999894,\n",
    "        \"std\": 1.0658141036401503e-14,\n",
    "        \"mode\": 1.334\n",
    "    },\n",
    "    .\n",
    "    .\n",
    "    .\n",
    "    \"tomo_grafene_48h\": {\n",
    "        \"max_decimal_digit\": \"18\",\n",
    "        \"max_count\": 5670279,\n",
    "        \"mean\": 0.00858388297105225,\n",
    "        \"std\": 0.016879311489506044,\n",
    "        \"mode\": -0.062485889286222465\n",
    "    }\n",
    "}\n",
    "this  is the content of the extracted_summary.json file.\n",
    "\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "from listspecificfiles import readlistFiles\n",
    "import os\n",
    "datapath = r\"results\\Data_SignificantDecimalDigits\"\n",
    "files = readlistFiles(datapath, '.json')\n",
    "# files = [filename for filename in files if filename[:]]\n",
    "fpath = files.file_with_Path()  # fileName with path name.\n",
    "actualfilename  = []\n",
    "extracted_data = {}  # Dictionary to store extracted data\n",
    "for file in fpath:\n",
    "    fileName = os.path.basename(file)\n",
    "    # print(f\"Processing fileName: {fileName}\")\n",
    "    if fileName.startswith('summaryStats_ONLY'):\n",
    "\n",
    "        actualfilename.append(fileName[len('summaryStats_ONLY'):])\n",
    "        print(f\"Processing fileName: {fileName} and actual filename: {actualfilename[-1]}\")\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "            max_count = -1\n",
    "            max_key = None\n",
    "\n",
    "            for key, stats in data.items():\n",
    "                count_val = stats.get(\"count\")\n",
    "                if count_val is not None and count_val > max_count:\n",
    "                    max_count = count_val\n",
    "                    max_key = key\n",
    "\n",
    "            if max_key:\n",
    "                stats = data[max_key] # here [:-5] because i will just take name without .json extension\n",
    "                extracted_data[actualfilename[-1][:-5]] = {\n",
    "                    \"max_decimal_digit\": max_key,\n",
    "                    \"max_count\": max_count,\n",
    "                    \"mean\": stats['mean'],\n",
    "                    \"std\": stats['std'],\n",
    "                    \"mode\": stats['mode']\n",
    "                }\n",
    "\n",
    "        # Save to a new JSON file\n",
    "# Define the save directory path as a plain string\n",
    "BASE_DIR = os.path.dirname(os.getcwd())  # Parent of current working directory\n",
    "savepath = os.path.join(BASE_DIR, \"results\", \"Data_SignificantDecimalDigits\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "# Define the full path for the JSON file to be saved\n",
    "jsonFilename = 'extracted_summary.json'\n",
    "jsonfilesave = os.path.join(savepath, jsonFilename)\n",
    "\n",
    "# Save the data\n",
    "with open(jsonfilesave, 'w') as out_file:\n",
    "    json.dump(extracted_data, out_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "# x = [1,1,1,1,2,5,6,7,8,9]\n",
    "# # mode_result = stats.mode(x,keepdims=True)\n",
    "# # print(mode_result)\n",
    "# # mode_result = stats.mode(values_arr, keepdims=True)\n",
    "# # print(\"Mode:\", mode_result.mode[0])\n",
    "# # print(\"Count:\", mode_result.count[0])\n",
    "# print(f\"Mode:\", {float(stats.mode(x,keepdims=True).mode[0])})\n",
    "# # print(f\"Count:\", int({stats.mode(x,keepdims=True).count[0]}))\n",
    "# print(40*\"-\")\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# # Folder containing your CSV files\n",
    "# folder_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\Data_SignificantDecimalDigits\"\n",
    "\n",
    "# # Set your folder path\n",
    "\n",
    "\n",
    "# output_file = os.path.join(folder_path, \"combined_file.csv\")\n",
    "\n",
    "# # Open the output file in write mode\n",
    "# with open(output_file, 'w', encoding='utf-8', newline='') as f_out:\n",
    "#     for file in os.listdir(folder_path):\n",
    "#         if file.endswith(\".csv\") and file[:-4] != 'combined_file':\n",
    "#             file_path = os.path.join(folder_path, file)\n",
    "            \n",
    "#             # Write file name as a row\n",
    "#             f_out.write(f\"{file[17:-4]}\\n\")\n",
    "            \n",
    "#             # Read data and write to file\n",
    "#             df = pd.read_csv(file_path)\n",
    "#             df.to_csv(f_out, index=False)\n",
    "            \n",
    "#             # Write an empty line as separator\n",
    "#             f_out.write(\"\\n\")\n",
    "\n",
    "# ---------------------------------------!!!!!!!!!!!! ---------------------------------------------------------\n",
    "\n",
    "# print(f\"file is cretaed with combined data: \\n ------\")\n",
    "# ff = \"summaryStats_ONLY\"\n",
    "# count = 0 \n",
    "# for i in ff:\n",
    "#     count +=1\n",
    "#     print(i,count,end=' ')\n",
    "\n",
    "# print(f\" \\n {count}, {len(ff)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Re-import necessary libraries after reset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import label\n",
    "\n",
    "def Peak_Detect_BackgroundThreshold(flat_data, save_path=None):\n",
    "    \"\"\"\n",
    "    Analyze a flat data array, plot its histogram, and save the plot.\n",
    "    \n",
    "    Parameters:\n",
    "        flat_data (numpy.ndarray): 1D array of data values.\n",
    "        save_path (str): Path to save the histogram plot.\n",
    "    \"\"\"\n",
    "    # Compute histogram\n",
    "    # Choose binning method\n",
    "    # binning_method = input(\"Choose binning method (sturges, doanes, freedman-diaconis, auto): \").strip().lower()\n",
    "    if binning_method in ['sturges', 'doanes', 'fd']:\n",
    "        counts, bin_edges = np.histogram(flat_data, bins=binning_method)\n",
    "    else:\n",
    "        counts, bin_edges = np.histogram(flat_data, bins=5000)  # Default to 'auto' if input is invalid\n",
    "\n",
    "    peak_idx = np.argmax(counts)  # index of the peak\n",
    "\n",
    "    # Thresholding and labeling\n",
    "    threshold = 0.6 * counts[peak_idx]  # 60% of peak value\n",
    "    mask = counts >= threshold\n",
    "    labeled, num_features = label(mask)\n",
    "\n",
    "    peak_label = labeled[peak_idx]\n",
    "    range_idxs = np.where(labeled == peak_label)[0]\n",
    "    range_start = bin_edges[range_idxs[0]]\n",
    "    range_end = bin_edges[range_idxs[-1] + 1]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', label='Histogram', align='edge')\n",
    "    plt.axhline(threshold, color='red', linestyle='--', label=f'Threshold ({threshold:.1f})')\n",
    "    plt.axvspan(range_start, range_end, color='orange', alpha=0.3, label=f'Connected Region ({range_start:.2f}-{range_end:.2f})')\n",
    "    plt.scatter(bin_edges[peak_idx], counts[peak_idx], color='blue', zorder=5, label='Peak')\n",
    "    plt.title(f'Connected Region Around Peak{filename[:-4]}')\n",
    "    plt.xlabel('RI Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.ylim(0, 900)  # Extend y-axis for better visibility\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Histogram saved to {save_path}\")\n",
    "    return range_start, range_end, threshold, peak_label\n",
    "\n",
    "\n",
    "\n",
    "# Re-import necessary packages after code execution state rese\n",
    "\n",
    "# Simulate example data with background noise and signal\n",
    "# np.random.seed(0)\n",
    "# background = np.random.normal(loc=1.334, scale=1e-12, size=100000)  # very fine, dominant background\n",
    "# signal = np.random.normal(loc=1.335, scale=1e-10, size=5000)        # weaker signal cluster\n",
    "# data = np.concatenate([background, signal])\n",
    "def peak_range(data, approx_bin_width = None):\n",
    "        # Use a more memory-efficient bin width strategy\n",
    "    if approx_bin_width:\n",
    "        approx_bin_width = approx_bin_width\n",
    "        print(f\"choosing the given approx_bin_width value: {approx_bin_width}\")\n",
    "\n",
    "    else:\n",
    "        approx_bin_width = 0.01  # e.g., good for typical float precision\n",
    "\n",
    "    bins = np.arange(np.min(data), np.max(data) + approx_bin_width, approx_bin_width)\n",
    "\n",
    "    # Recompute histogram with adjusted bin width\n",
    "    counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    peak_idx = np.argmax(counts)\n",
    "    peak_val = bin_centers[peak_idx]\n",
    "\n",
    "    # Iteratively find range around the peak\n",
    "    left = peak_idx\n",
    "    while left > 0 and counts[left] >= counts[left - 1] * 0.5:\n",
    "        left -= 1\n",
    "    right = peak_idx\n",
    "    while right < len(counts) - 1 and counts[right] >= counts[right + 1] * 0.5:\n",
    "        right += 1\n",
    "\n",
    "    peak_range = (bin_centers[left], bin_centers[right])\n",
    "    filtered_data = data[(data < peak_range[0]) | (data > peak_range[1])]\n",
    "\n",
    "    # Plotting updated histograms\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "    # Original histogram\n",
    "    axs[0].hist(data, bins=bins, color='gray', edgecolor='black')\n",
    "    axs[0].axvspan(peak_range[0], peak_range[1], color='red', alpha=0.3, label='Filtered Background')\n",
    "    axs[0].set_title(\"Original Histogram with Background\")\n",
    "    axs[0].set_xlabel(\"Value\")\n",
    "    axs[0].set_ylabel(\"Frequency\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Filtered histogram\n",
    "    axs[1].hist(filtered_data, bins=bins, color='blue', edgecolor='black')\n",
    "    axs[1].set_title(\"Histogram After Background Removal\")\n",
    "    axs[1].set_xlabel(\"Value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define filtering function using decimal precision\n",
    "def filter_background_by_precision(data, decimal_threshold=4, width_scale=2):\n",
    "    bin_width = width_scale * 10**(-decimal_threshold)\n",
    "    bins = np.arange(min(data), max(data) + bin_width, bin_width)\n",
    "    \n",
    "    counts, edges = np.histogram(data, bins=bins)\n",
    "    mode_bin_index = np.argmax(counts)\n",
    "    \n",
    "    # Identify background bin range\n",
    "    bg_min = edges[mode_bin_index]\n",
    "    bg_max = edges[mode_bin_index + 1]\n",
    "    \n",
    "    # Filter out background values\n",
    "    foreground = data[(data < bg_min) | (data > bg_max)]\n",
    "    \n",
    "    return foreground, (bg_min, bg_max), bin_width, bins\n",
    "\n",
    "# Apply filtering\n",
    "# filtered_data, (bg_min, bg_max), bin_width, bins_used = filter_background_by_precision(data)\n",
    "\n",
    "# # Plot histogram: Before and After filtering\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# # Original\n",
    "# axs[0].hist(data, bins=bins_used, color='gray', edgecolor='black')\n",
    "# axs[0].set_title(\"Original Data Histogram\")\n",
    "# axs[0].set_xlabel(\"RI Value\")\n",
    "# axs[0].set_ylabel(\"Frequency\")\n",
    "# axs[0].axvspan(bg_min, bg_max, color='red', alpha=0.3, label='Filtered Background')\n",
    "# axs[0].legend()\n",
    "\n",
    "# # Filtered\n",
    "# axs[1].hist(filtered_data, bins=bins_used, color='blue', edgecolor='black')\n",
    "# axs[1].set_title(\"Filtered Data Histogram\")\n",
    "# axs[1].set_xlabel(\"RI Value\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.ndimage import label\n",
    "from path_manager import addpath    \n",
    "paths = addpath()\n",
    "\n",
    "from listspecificfiles import readlistFiles\n",
    "data_path = r\"data\\raw_npyData\"  # relative path with r\"relativepath\"\n",
    "files = readlistFiles(data_path,'.npy')\n",
    "fpath = files.file_with_Path()  # file with path name.  \n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd().parent    \n",
    "print(f\"BASE_DIR:--------> {BASE_DIR}\")\n",
    "save_dir = BASE_DIR/\"results\"/\"peakDetect_thresBackground\"\n",
    "os.makedirs(save_dir,exist_ok=True)  # Create if missing    \n",
    "\n",
    "# binning_method = input(\"Choose binning method (sturges, doanes, freedman-diaconis, auto): \").strip().lower()\n",
    "\n",
    "countfile = 0\n",
    "for FileWithPath in fpath:\n",
    "    countfile +=1\n",
    "    \n",
    "    print(f\"FileWithPath: {FileWithPath} and countfile: {countfile}\")\n",
    "    filename = os.path.basename(FileWithPath)\n",
    "    if countfile >= 3 and countfile <= 5:\n",
    "        print(f\"ignored File : {filename}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing filename: {filename}\")\n",
    "    # Load and flatten data\n",
    "    data = np.load(FileWithPath)\n",
    "    data = data.flatten()\n",
    "    print(f\"Shape of data: {data.shape[0]}\")\n",
    "    \n",
    "    # Call the function to analyze the data and save the histogram\n",
    "    # save_path = os.path.join(save_dir, f\"{filename[:-4]}_peakBack.png\")\n",
    "    \n",
    "    # range_start, range_end, threshold, peak_label = Peak_Detect_BackgroundThreshold(data, save_path=save_path)\n",
    "    peak_range(data, approx_bin_width=0.0001)\n",
    "\n",
    "    # filtered_data, (bg_min, bg_max), bin_width, bins_used = filter_background_by_precision(data)\n",
    "       \n",
    "    # # Compute max frequencies separately for y-axis limit\n",
    "    # original_counts, _ = np.histogram(data, bins=bins_used)\n",
    "    # filtered_counts, _ = np.histogram(filtered_data, bins=bins_used)\n",
    "\n",
    "    # original_ymax = original_counts.max()\n",
    "    # filtered_ymax = filtered_counts.max()\n",
    "\n",
    "    # # Add some padding\n",
    "    # original_ymax *= 1.1\n",
    "    # filtered_ymax *= 1.1\n",
    "\n",
    "    # # Plot histogram: Before and After filtering\n",
    "    # fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # # Original\n",
    "    # axs[0].hist(data, bins=bins_used, color='gray', edgecolor='black')\n",
    "    # axs[0].set_title(\"Data Histogram\")\n",
    "    # axs[0].set_xlabel(\"RI Value\")\n",
    "    # axs[0].set_ylabel(\"Frequency\")\n",
    "    # axs[0].axvspan(bg_min, bg_max, color='red', alpha=0.3, label='Filtered Background')\n",
    "    # axs[0].set_ylim(0, original_ymax)\n",
    "\n",
    "    # # Filtered\n",
    "    # axs[1].hist(filtered_data, bins=bins_used, color='blue', edgecolor='black')\n",
    "    # axs[1].set_title(\"Filtered Histogram\")\n",
    "    # axs[1].set_xlabel(\"RI Value\")\n",
    "    # axs[1].set_ylim(0, filtered_ymax)\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# UniqueValueCount\n",
    "# Counts_OFUniqueValues\n",
    "\n",
    "# print(f\"min val: {min(significant_digit_data)} and max val :{max(significant_digit_data)},len: {len(significant_digit_data)}\")\n",
    "# most_common_digit = np.argmax(significant_digit_data) + 1\n",
    "# print(most_common_digit)\n",
    "# significant_digit_data[most_common_digit -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tested working fin eto count the number after the decimal(howmany decimal place number exist in a number.) --> give the precision plac eof the given number. \n",
    "#  AS I HAVE Seen in Matlab , it was 16 digits precision in data values. upper method is good named as actual_significant_digits_after_decimal, as shown below also.\n",
    "# from decimal import Decimal\n",
    "# def actual_significant_digits_after_decimal(val):\n",
    "#     \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\"\"\"\n",
    "#     if val == 0:\n",
    "#         return 0\n",
    "#     d = Decimal(str(val)).normalize()\n",
    "#     if '.' not in str(d):\n",
    "#         return 0\n",
    "#     return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "# c =0\n",
    "# cdeclist =[]\n",
    "# for val in data:\n",
    "#     c +=1\n",
    "    \n",
    "#     strval = str(val)\n",
    "#     # rawstrip = strval.split('.')\n",
    "#     cdec1 = strval.split('.')[1].rstrip('0')\n",
    "#     # cdec0 = strval.split('.')[0].lstrip('0')\n",
    "#     # print(f\"cdecl: {cdec1} and cdec0:{cdec0} and original val: {strval}     {rawstrip}\")\n",
    "#     print(cdec1)\n",
    "#     cdeclist.append(len(cdec1))\n",
    "#     # print(f\"{val} and {type(val)}\")\n",
    "    \n",
    "#     # if len(cdec) >= 14:\n",
    "#     #     print(f\"decimal value till 7 place:{cdec}\")\n",
    "#     if c == 10:\n",
    "#         print(f\"decimal value till 7 place:{c}\")\n",
    "#         break\n",
    "    \n",
    "# print(f\"{cdeclist} and {type(cdeclist[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "# val = 1.25e-5\n",
    "val = 1.25001200100\n",
    "d = Decimal(str(val)).normalize()\n",
    "\n",
    "# 2.0**0.5\n",
    "# ------------------------ joned cell ----------------------------------\n",
    "x =\"1.334000000000000\"\n",
    "len(x)\n",
    "for val in data:\n",
    "    print\n",
    "x.split('.')[1].rstrip('0')\n",
    "from decimal import Decimal\n",
    "def actual_significant_digits_after_decimal(val):\n",
    "    \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    d = Decimal(str(val)).normalize()\n",
    "    if '.' not in str(d):\n",
    "        return 0\n",
    "    return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "dec_place = []\n",
    "for val in data:\n",
    "      decimal_placesinEcahVal = actual_significant_digits_after_decimal(val)\n",
    "      dec_place.append(decimal_placesinEcahVal)\n",
    "      print(f\"decimal values:{decimal_placesinEcahVal}\")\n",
    "\n",
    "# data.shape\n",
    "# c =0\n",
    "# for val in data:\n",
    "#     c +=1\n",
    "#     print(f\"{val} and {type(val)}\")\n",
    "    \n",
    "#     if c == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# test \n",
    "float_array = np.array([0.0122502, 1.2302355, -1.02415, 0.00034001, 10.000123,2,2,2,2,10,2,3,9,5,8,6,7,2,2,24,6,71,2])\n",
    "from decimal import Decimal\n",
    "def actual_significant_digits_after_decimal(val):\n",
    "    \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    d = Decimal(str(val)).normalize()\n",
    "    if '.' not in str(d):\n",
    "        return 0\n",
    "    return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "dec_place = []\n",
    "for val in float_array:\n",
    "      decimal_placesinEcahVal = actual_significant_digits_after_decimal(val)\n",
    "      dec_place.append(decimal_placesinEcahVal)\n",
    "      print(f\"decimal values:{decimal_placesinEcahVal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "val = 123.782\n",
    "print(f\"val : {val:.9e}\")\n",
    "# res= significant_digits(val)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_decimal_precision_npy(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    print(folder_path)\n",
    "    result_rows = []\n",
    "\n",
    "    def decimal_places(val):\n",
    "        s = f\"{val:.18f}\".rstrip('0')\n",
    "        if '.' in s:\n",
    "            return len(s.split('.')[1])\n",
    "        return 0\n",
    "\n",
    "    def significant_digits(val):\n",
    "        s = f\"{val:.18e}\".split('e')[0].replace('.', '').lstrip('0')\n",
    "        return len(s)\n",
    "\n",
    "    for file in folder.glob(\"*.npy\"):\n",
    "        data = np.load(file)\n",
    "        flat_data = data.flatten()\n",
    "        nonzero_data = flat_data[flat_data != 0]\n",
    "\n",
    "        if len(nonzero_data) == 0:\n",
    "            continue\n",
    "\n",
    "        dec_places_list = [decimal_places(v) for v in nonzero_data]\n",
    "        sig_digits_list = [significant_digits(v) for v in nonzero_data]\n",
    "\n",
    "        max_dec_places = max(dec_places_list)\n",
    "        max_sig_digits = max(sig_digits_list)\n",
    "\n",
    "        # Get values with the most decimal places\n",
    "        max_dec_values = nonzero_data[np.array(dec_places_list) == max_dec_places]\n",
    "\n",
    "        # Group by decimal place count\n",
    "        unique_dec_counts, dec_counts = np.unique(dec_places_list, return_counts=True)\n",
    "\n",
    "        # Prepare summary row\n",
    "        row = {\n",
    "            'File': file.name,\n",
    "            'Max Decimal Places': max_dec_places,\n",
    "            'Max Significant Digits': max_sig_digits,\n",
    "            'Total Elements': len(flat_data),\n",
    "            'Nonzero Elements': len(nonzero_data),\n",
    "            'Values with Max Decimal Places': \"; \".join([f\"{v:.18f}\" for v in max_dec_values[:3]]) + (\" ...\" if len(max_dec_values) > 3 else \"\")\n",
    "        }\n",
    "\n",
    "        # Add grouped counts\n",
    "        for d, count in zip(unique_dec_counts, dec_counts):\n",
    "            row[f\"Decimals={d} Count\"] = count\n",
    "\n",
    "        result_rows.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(result_rows)\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv = Path(BASE_DIR/\"results\"/\"decimal_precision_report.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    return df, output_csv\n",
    "\n",
    "# Run the function on a sample folder (adjust path accordingly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sample_folder = data_path\n",
    "df_result, csv_path = analyze_decimal_precision_npy(sample_folder)\n",
    "\n",
    "# import ace_tools as tools \n",
    "# tools.display_dataframe_to_user(name=\"Decimal Precision Analysis\", dataframe=df_result)\n",
    "# csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# x = range(5)\n",
    "x = [0,1,2,3,4,5]\n",
    "print(x)\n",
    "ll1 = [1,1,3,4,4,6,5,4,4,0,0,0,55,65]\n",
    "for val in ll1:\n",
    "    x.append(val)\n",
    "un1 = np.unique(x)\n",
    "count = np.unique_counts(x)\n",
    "print(f\"x as original:{x} \\n un1:{un1} and count:{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Compute the non-zero differences in sorted data\n",
    "    Plot a histogram of spacing (difference values)\n",
    "    Optionally zoom in on small-scale structure (e.g., 99th percentile or top N smallest)\n",
    " \"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_diff_distribution(data_array, file_label=\"Data\", show_log=False, zoom_percentile=None):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of spacing (differences) in sorted data.\n",
    "\n",
    "    Parameters:\n",
    "        data_array (np.ndarray): Flattened array of values.\n",
    "        file_label (str): Label for title or saving (default \"Data\").\n",
    "        show_log (bool): If True, plot x-axis in log scale.\n",
    "        zoom_percentile (float): Zoom into differences below this percentile (e.g., 99.0).\n",
    "    \"\"\"\n",
    "    # Flatten and sort\n",
    "    sorted_data = np.sort(data_array.flatten())\n",
    "\n",
    "    # Compute differences\n",
    "    diffs = np.diff(sorted_data)\n",
    "\n",
    "    # Filter out near-zero diffs (due to float precision or repeats)\n",
    "    diffs = diffs[np.abs(diffs) > 1e-12]\n",
    "\n",
    "    # Optional zoom\n",
    "    if zoom_percentile:\n",
    "        cutoff = np.percentile(diffs, zoom_percentile)\n",
    "        diffs = diffs[diffs <= cutoff]\n",
    "        print(f\"Zooming into differences <= {cutoff:.9f} (percentile {zoom_percentile})\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(diffs, bins=100, color='skyblue', edgecolor='gray')\n",
    "    plt.title(f\"Histogram of Value Differences ({file_label})\")\n",
    "    plt.xlabel(\"Spacing Between Consecutive Values\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    if show_log:\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Log-scaled Spacing\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from listspecificfiles import readlistFiles\n",
    "fpaths = readlistFiles(data_path,'.npy').file_with_Path()\n",
    "for fpath in fpaths:\n",
    "    data_array = np.load(fpath)\n",
    "    plot_diff_distribution(data_array, file_label=\"Data\", show_log=False, zoom_percentile=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data.shape\n",
    "from posixpath import basename\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# pathModule = BASE_DIR/\"src\"/\"modules\"\n",
    "# sys.path.append(str(pathModule))\n",
    "# print(f\"pathofmodules:{pathModule}\")\n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "\n",
    "from listspecificfiles import*\n",
    "Relative_data_path = r\"data\\raw_npyData\"\n",
    "\n",
    "# Relative_data_path = os.path.normpath()\n",
    "# files = readlistFiles(Relative_data_path,'.npy')\n",
    "# print(f\"full output: {readlistFiles(Relative_data_path,'.npy').file_with_Path()}\")\n",
    "fpath = readlistFiles(Relative_data_path,'.npy').file_with_Path() \n",
    "for file in fpath:\n",
    "    data = np.load(file)\n",
    "    print(file)\n",
    "    uniquedata = np.unique_counts(data)\n",
    "    print(f\"data shape : {data.shape} \\n unique Count: {uniquedata.counts} \\n unique_values: {uniquedata.values} and \\n now see the differencs: { data.shape[0] - uniquedata.counts.shape[0]} \")\n",
    "    # datadic = {f\"{basename[:-4]}_shape\":{data.shape}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "most_valuesInData = data[data == 1.33] ; print(f\"total values in this:{most_valuesInData.shape} and data : {data.shape} \");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # hybrid_kmeans_dbscan.py\n",
    "\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import scipy.io as sio\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # THRESHOLD_VALUE = 1.334\n",
    "# def load_volume(filepath,THRESHOLD_VALUE):\n",
    "#     if filepath.endswith('.npy'):\n",
    "#         volume = np.load(filepath)\n",
    "#         volume[volume <= THRESHOLD_VALUE] = 0  # Threshold to remove background\n",
    "#     elif filepath.endswith('.mat'):\n",
    "#         mat = sio.loadmat(filepath)\n",
    "#         # Assuming your volume variable is named 'volume' in .mat\n",
    "#         volume = next(v for v in mat.values() if isinstance(v, np.ndarray) and v.ndim == 3)\n",
    "#         volume[volume <= THRESHOLD_VALUE] = 0  # Threshold to remove background\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported file format. Use .mat or .npy\")\n",
    "#     return volume\n",
    "\n",
    "# def extract_features(volume):\n",
    "#     coords = np.array(np.nonzero(volume)).T\n",
    "#     intensities = volume[volume > 0].flatten().reshape(-1, 1)\n",
    "#     return np.hstack((coords, intensities))\n",
    "\n",
    "# def run_kmeans(X_scaled, n_clusters=4):\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "#     return kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# def run_dbscan_per_cluster(X_scaled, kmeans_labels, eps=0.6, min_samples=5):\n",
    "#     final_labels = -np.ones(len(X_scaled), dtype=int)\n",
    "#     label_offset = 0\n",
    "#     for cluster_id in np.unique(kmeans_labels):\n",
    "#         indices = np.where(kmeans_labels == cluster_id)[0]\n",
    "#         db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#         sub_labels = db.fit_predict(X_scaled[indices])\n",
    "#         sub_labels[sub_labels != -1] += label_offset\n",
    "#         final_labels[indices] = sub_labels\n",
    "#         label_offset += sub_labels.max() + 1 if sub_labels.max() != -1 else 0\n",
    "#     return final_labels\n",
    "\n",
    "# def save_results(output_dir, labels, coords):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     np.save(os.path.join(output_dir, \"cluster_labels.npy\"), labels)\n",
    "#     sio.savemat(os.path.join(output_dir, \"cluster_labels.mat\"), {\"labels\": labels})\n",
    "#     np.save(os.path.join(output_dir, \"voxel_coords.npy\"), coords)\n",
    "\n",
    "\n",
    "# def plot_clusters(coords, labels, title=\"Cluster Visualization\"):\n",
    "#     fig = plt.figure(figsize=(10, 7))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "#     scatter = ax.scatter(coords[:, 0], coords[:, 1], coords[:, 2], c=labels, cmap='tab20', s=2)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar(scatter)\n",
    "#     plt.show()\n",
    "\n",
    "# # # Example use:\n",
    "# volume = load_volume(\"yourfile.mat\")\n",
    "# X = extract_features(volume)\n",
    "# X_scaled = StandardScaler().fit_transform(X)\n",
    "# kmeans_labels = run_kmeans(X_scaled, n_clusters=4)\n",
    "# final_labels = run_dbscan_per_cluster(X_scaled, kmeans_labels)\n",
    "# plot_clusters(X[:, :3], final_labels)\n",
    "# save_results(\"output_dir\", final_labels, X[:, :3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "# from basicstatics import basicstat\n",
    "from scipy.io import savemat\n",
    "\n",
    "from pathlib import Path\n",
    "from path_manager import AddPath\n",
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\")\n",
    "from createmat2npyViceVersa import npy2mat \n",
    "\n",
    "datapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\"\n",
    "outputdatapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\\convertedmatfiles\"\n",
    "\n",
    "npy2mat(datapath=datapath,outputdatapath=outputdatapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I've created a working .py script that:\n",
    "-  #  load either .mat or .npy files,\n",
    "-  # Applies K-Means followed by DBSCAN\n",
    "-  #  Visualizes clusters using matplotlib in 3D\n",
    "-  #  Saves cluster labels and voxel coordinates to both .npy and .mat formats\n",
    "-  # Created an output directory automatically for saved files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from hybrid_kmeans_dbscan import *\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "SRCFILES  = Path.cwd().parent\n",
    "DATAFILES  = SRCFILES/\"data\"/\"raw_npyData\"\n",
    "\n",
    "listfiles = os.listdir(str(DATAFILES))\n",
    "Datafile = random.choice(listfiles)\n",
    "\n",
    "print(f\" --------------->  check step by step : SRCFILES: {SRCFILES} --> DATAFILES: {DATAFILES} --> \\n listfiles: {listfiles} \\n --> Datafile random choice: {Datafile}\")\n",
    "# Datafile = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\\Tomogramma_BuddingYeastCell.mat\"\n",
    "FileCompletePath = os.path.join(DATAFILES,Datafile)\n",
    "THRESHOLD_VALUE = 1.334\n",
    "\n",
    "volume = load_volume(FileCompletePath,THRESHOLD_VALUE)  # or .npy\n",
    "\n",
    "X = extract_features(volume)\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "kmeans_labels = run_kmeans(X_scaled, n_clusters=4)\n",
    "final_labels = run_dbscan_per_cluster(X_scaled, kmeans_labels, eps=0.6, min_samples=20)\n",
    "\n",
    "plot_clusters(X[:, :3], final_labels, title=\"Final Clusters\")\n",
    "\n",
    "save_results(\"clustering_output\", final_labels, X[:, :3])\n",
    "\n",
    "# for invoking the open3d plot function ------>\n",
    "# SRCFILES  = Path.cwd()\n",
    "# RESFilesINsrc = SRCFILES/\"clustering_output/\"\n",
    "# print(RESFilesINsrc)\n",
    " \n",
    "# clusteredNPY_Path = os.path.join(str(RESFilesINsrc),'cluster_labels.npy')\n",
    "# clusteredNPY_Coords = os.path.join(str(RESFilesINsrc),'voxel_coords.npy')\n",
    "\n",
    "# VisualizeOpen3d(clusteredNPY_Path,clusteredNPY_Coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "coordCluster = np.load(r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\\voxel_coords.npy\")\n",
    "clusterLabels = np.load(r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\\cluster_labels.npy\")\n",
    "# clusterLabels.shape\n",
    "print(f\"coord: {coordCluster.shape} and \\n {coordCluster[1:5,:]} \\n  and \\n cluster labels: {clusterLabels.shape}\\n {clusterLabels[1:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# SRCFILES  = Path.cwd()\n",
    "# RESFilesINsrc = SRCFILES/\"clustering_output/\"\n",
    "# print(RESFilesINsrc)\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SRCFILES  = Path.cwd()\n",
    "RESFilesINsrc = SRCFILES/\"clustering_output/\"\n",
    "print(RESFilesINsrc)\n",
    " \n",
    "clusteredNPY_Path = os.path.join(str(RESFilesINsrc),'cluster_labels.npy')\n",
    "clusteredNPY_Coords = os.path.join(str(RESFilesINsrc),'voxel_coords.npy')\n",
    "\n",
    "print(f\" here to check the final path : {clusteredNPY_Path},\\n --{clusteredNPY_Coords} <----------------------\\n\" )\n",
    "\n",
    "\n",
    "label_path = clusteredNPY_Path\n",
    "coord_path = clusteredNPY_Coords \n",
    "\n",
    "from meshvisClustCordLabels import *\n",
    "visualize_and_save_clusters(label_path, coord_path, output_dir=\"o3d_clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here below the code which test only k-means with coordinates and intensity as features save all parameters in .mat format  \n",
    "### in second run : I just consider the intesnsity values for k-means clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Hybrid Clustering with K-Means + DBSCAN for 3D Volume Data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- USER OPTIONS ---\n",
    "\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "SRCFILES  = Path.cwd().parent\n",
    "# SRCFILES  = Path(__file__).resolve().parent.parent\n",
    "\n",
    "DATAFILES  = SRCFILES/\"data\"/\"raw_npyData\"\n",
    "\n",
    "listfiles = os.listdir(str(DATAFILES))\n",
    "\n",
    "for filename in listfiles:\n",
    "    if filename.endswith('.npy'):\n",
    "        DATAFILE_NAME = filename\n",
    "# Datafile = random.choice(listfiles)\n",
    "# DATAFILE_NAME = \"AML2_cell11.npy\"  # data\\raw_npyData\\tomo_Grafene_24h.npy\n",
    "# DATAFILE_NAME = \"tomo_Grafene_24h.npy\"  #  data\\raw_npyData\\tomo_Grafene_24h.npy\n",
    "        DATAFILES = str(DATAFILES)\n",
    "        input_path = os.path.join(DATAFILES,DATAFILE_NAME)\n",
    "        print(f\"input path : {input_path}\")\n",
    "        \n",
    "\n",
    "# for file in listfiles:\n",
    "#     if file == DATAFILE_NAME:\n",
    "#         DATAFILES = str(DATAFILES)\n",
    "#         input_path = os.path.join(DATAFILES,DATAFILE_NAME)\n",
    "#         print(f\"input path : {input_path}\")\n",
    "        \n",
    "# print(f\" --------------->  check step by step : SRCFILES: {SRCFILES} --> DATAFILES: {DATAFILES} --> \\n listfiles: {listfiles} \\n --> Datafile random choice: {Datafile}\")\n",
    "# Datafile = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\\Tomogramma_BuddingYeastCell.mat\"\n",
    "\n",
    "        # Choose input type\n",
    "        input_type = \"npy\"   #\"mat\"   or \"npy\"\n",
    "        input_path = input_path  # or .npy\n",
    "        volume_key = \"volume\"  # for .mat file: key inside the .mat dict\n",
    "\n",
    "        # output_dir = \"cluster_output\"\n",
    "        output_dir = SRCFILES/\"results\"/\"hybrid_Kdbcluster\"\n",
    "        kmeans_k = 7\n",
    "\n",
    "        # --- LOAD VOLUME DATA ---\n",
    "        if input_type == \"mat\":\n",
    "            mat_data = sio.loadmat(input_path)\n",
    "            volume = mat_data[volume_key]\n",
    "        elif input_type == \"npy\":\n",
    "            volume = np.load(input_path)\n",
    "            # to reduce the size of data volume.\n",
    "            # x_row,y_row,z_row = volume.shape\n",
    "            # volume = volume[:x_row/2,:y_row/2,:z_row/2]\n",
    "            # volume = volume[::5, ::5, ::5]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input_type. Choose 'mat' or 'npy'.\")\n",
    "\n",
    "        # --- EXTRACT NONZERO VOXELS AS POINT CLOUD ---\n",
    "        coords = np.array(np.nonzero(volume)).T  \n",
    "        # it will returns the coordinate of each nonzero values in volume and formate will be like this [[]\n",
    "        # coords =\n",
    "        # [z1,x1,y1]\n",
    "        # [z2,x2,y2]\n",
    "        # ........\n",
    "        # [zn,xn,yn]]\n",
    "\n",
    "        # intensities = volume[volume > 0].reshape(-1, 1)\n",
    "        intensities = volume[volume != 0].reshape(-1, 1)  # beacuse 48 hour data has some negative values.\n",
    "        # X = np.hstack((coords, intensities))  # shape: (N, 4)\n",
    "        X = np.hstack((coords,intensities))  # shape: (N, 4)\n",
    "        # np.hstack() horizontally stacks arrays (i.e., along columns / axis=1), meaning it concatenates them side by side. a = [[1],[2],[3]] , b = [[10],[20],[30]]\n",
    "        # np.hstack(a,b) --> results will be [[1,10],[2,20],[3,30]]\n",
    "        X1 = X[:,3]\n",
    "        X2 = X1.reshape(-1, 1)\n",
    "        # --- SCALE FEATURES ---\n",
    "        X_scaled = StandardScaler().fit_transform(X2)   # Z-score scaling/normalization -> zero mean, unit variance\n",
    "\n",
    "        # --- APPLY K-MEANS ---\n",
    "        kmeans = KMeans(n_clusters=kmeans_k,init = 'k-means++', random_state=42).fit(X_scaled)\n",
    "        # kmeans = KMeans(n_clusters=kmeans_k, init = 'k-means++', random_state=42).fit(X)\n",
    "        kmeans_labels = kmeans.labels_   # kmeans.labels_ --> kmeans_labels is one row (1xN) of labels (0,1,.., n_clusters -1) as output → array of cluster assignments for each data point, storing the cluster labels for all samples in the kmeans_labels variable, so you can use them later for saving or analyzing clusters / future use.\n",
    "\n",
    "        # <------------ For saving the k-means cluster and corresponding coordinates results -------- >\n",
    "        kmeans_coords_with_labels = np.hstack((coords, kmeans_labels.reshape(-1, 1)))  # [x, y, z, kmeans_label]\n",
    "        # Save as .npy\n",
    "        kmeans_intResultDir = os.path.join(output_dir,f\"kmIntensity{DATAFILE_NAME[:-4]}\")\n",
    "        os.makedirs(kmeans_intResultDir,exist_ok=True)\n",
    "        np.save(os.path.join(kmeans_intResultDir, \"kmeans_coords_labels.npy\"), kmeans_coords_with_labels)\n",
    "        # Save as .mat\n",
    "        sio.savemat(os.path.join(kmeans_intResultDir, \"kmeans_coords_labels.mat\"), {\"kmeans_coords_labels\": kmeans_coords_with_labels})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## here test the code at each satement one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "volume = np.array([\n",
    "    [[0.5, 1.4],\n",
    "     [1.0, 1.5]],\n",
    "\n",
    "    [[1.3, 1.2],\n",
    "     [1.6, 0]]\n",
    "])\n",
    "\n",
    "volume1 = volume.reshape(4,2)\n",
    "volume2 = volume1[:,1]\n",
    "print(f\"reshape: {volume1},\\n  and \\n {volume1[:,1]} and {volume2.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a small 3D array\n",
    "volume = np.array([\n",
    "    [[0.5, 1.4],\n",
    "     [1.0, 1.5]],\n",
    "\n",
    "    [[1.3, 1.2],\n",
    "     [1.6, 0]]\n",
    "])\n",
    "print(f\"size of vol: {volume.shape} \\n\")\n",
    "coords = np.array(np.nonzero(volume))\n",
    "print(f\"size of nonzeros : {np.nonzero(volume)} \\n\")\n",
    "#  here np.nonzero(volume) -> returns the (z,x,y) coordinates of all nonzero points --> \n",
    "#  output size of nonzeros : (array([0, 0, 0, 0, 1, 1, 1]), array([0, 0, 1, 1, 0, 0, 1]), array([0, 1, 0, 1, 0, 1, 0]))\n",
    "# coord:\n",
    "#  [[0 0 0 0 1 1 1]\n",
    "#  [0 0 1 1 0 0 1]\n",
    "#  [0 1 0 1 0 1 0]]\n",
    "\n",
    "coordst = np.array(np.nonzero(volume)).T\n",
    "print(f\"coord:\\n {coords}\\n  and \\n taranspose: \\n \\n {coordst}\")\n",
    "# taranspose: \n",
    "#  [[0 0 0]\n",
    "#  [0 0 1]\n",
    "#  [0 1 0]\n",
    "#  [0 1 1]\n",
    "#  [1 0 0]\n",
    "#  [1 0 1]\n",
    "#  [1 1 0]]\n",
    "\n",
    "# thres = 1.3\n",
    "# volume[volume <= thres] = 0\n",
    "# print(\"Original Volume:\\n\",volume)\n",
    "# volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create 3D grid coordinates (4,3,3) = 36 points\n",
    "x, y, z = np.meshgrid(np.arange(4), np.arange(3), np.arange(3), indexing='ij')\n",
    "coords = np.stack((x.ravel(), y.ravel(), z.ravel()), axis=1)\n",
    "\n",
    "# Random intensity values between 50 and 200\n",
    "intensity = np.random.uniform(50, 200, size=(coords.shape[0], 1))\n",
    "\n",
    "# Combine coords + intensity\n",
    "X = np.hstack((coords, intensity))\n",
    "\n",
    "# Apply standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Print first 5 for illustration\n",
    "print(f\"Original Data (X):\\n, {X[:5]}  and  {X.shape}\")\n",
    "print(\"\\nScaled Data (X_scaled):\\n\", X_scaled[:5])\n",
    "kmeans_k =3\n",
    "kmeans = KMeans(n_clusters=kmeans_k,init = 'k-means++',random_state=42).fit(X_scaled)\n",
    "# kmeans = KMeans(n_clusters=kmeans_k, init = 'k-means++', random_state=42).fit(X)\n",
    "kmeans_labels = kmeans.labels_   \n",
    "final_labels = -np.ones(len(X), dtype=int)  # Prepares an array to hold your final clustering labels. -1 means unassigned/outlier (just like DBSCAN does).Example: If X has 1000 points → final_labels = [-1, -1, ..., -1] (length 1000)\n",
    "\n",
    "label_offset = 0\n",
    "\n",
    "for cluster_id in np.unique(kmeans_labels):\n",
    "    print(f\"i have compl k-means now in dbsacn, cluster_id: {cluster_id}\")\n",
    "    indices = np.where(kmeans_labels == cluster_id)[0]  #  indices = np.where(kmeans_labels == cluster_id) --> indices returns tuple of array like -> (array([1, 4]),) to extract use [0] first array(np.where(kmeans_labels == cluster_id))[0] and get result like this # array([1, 4])\n",
    "    \n",
    "    X_sub = X_scaled[indices] # here extracting the coordinate and intensity value according to the cluster_id. [X_scaled size is: (N, 4)]\n",
    "    # X_sub = X[indices]\n",
    "\n",
    "    db = DBSCAN(eps= 0.8, min_samples=5).fit(X_sub)  # db scan here in each loop for each cluster further.\n",
    "    db_labels = db.labels_\n",
    "    db_labels[db_labels != -1] += label_offset\n",
    "    final_labels[indices] = db_labels\n",
    "    label_offset += db_labels.max() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create a moon-shaped dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.5, random_state=42)\n",
    "\n",
    "# Normalize\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Plot original data\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c='gray', edgecolor='k')\n",
    "plt.title(\"Input Data (Normalized)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Apply DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=5)\n",
    "# labels = db.fit_predict(X_scaled)\n",
    "clusters = db.fit(X_scaled)\n",
    "db_labels = clusters.labels_\n",
    "l0 = []\n",
    "l1 = []\n",
    "for l in db_labels:\n",
    "    # print(f\"lables:{l}\")\n",
    "    if l == 0:\n",
    "        l0.append(int(l))\n",
    "    else:\n",
    "        l1.append(int(l))\n",
    "\n",
    "print(f\"l0 index:{l0} and \\n  l1 index:{l1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ulab =np.unique(db_labels)\n",
    "print(ulab)\n",
    "label_offset =0 \n",
    "db_labels[db_labels != -1] += label_offset\n",
    "print(db_labels)\n",
    "# final_labels[indices] = db_labels\n",
    "label_offset += db_labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Labels assigned:\", np.unique(labels))\n",
    "print(\"Noise points (label == -1):\", list(labels).count(-1))\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='Set1', edgecolor='k')\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dc = np.load(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\clustering_output\\voxel_coords.npy\")\n",
    "print(dc.shape)\n",
    "# from scipy.io import loadmat\n",
    "# fd = loadmat(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\hybrid_Kdbcluster\\cluster_labels.mat\")\n",
    "# print(fd['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data1 = np.load(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\raw_npyData\\Tomogramma_BuddingYeastCell.npy\")\n",
    "data2 = data1[:50,:50,:50]\n",
    "size1 = data2.shape\n",
    "print(data2.shape[0],data2.shape[1],data2.shape[2],\"\\n --\" )\n",
    "print(f\"size: {size1[0]}\\n {size1[1]}\\n {size1[2]}\")\n",
    "print(f\"data1 shape: {data1.shape} \\n data2 shape: {data2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "kmeans_labels = np.array([0, 1, 0, 2, 1, 2, 0])\n",
    "cluster_id = 0\n",
    "\n",
    "indices = np.where(kmeans_labels == cluster_id)\n",
    "print(indices)  # (array([1, 4]),)\n",
    "\n",
    "indices = np.where(kmeans_labels == cluster_id)[0]\n",
    "print(indices)  # array([1, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# Adjust parsing to handle np.float64(...) formatting using regular expressions\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Reload the file after execution reset\n",
    "# txt_path = 'results\\featureQuantileThres\\AllFeatures_Stats.txt'\n",
    "# import os\n",
    "\n",
    "txt_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\featureQuantileThres\\AllFeatures_Stats.txt\"\n",
    "txt_path = os.path.normpath(txt_path)\n",
    "print(f\"txt_path: {txt_path}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "with open(txt_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if ':' in line:\n",
    "            name, data_str = line.split(':', 1)\n",
    "            # Replace np.float64(...) with float values using regex\n",
    "            cleaned_data_str = re.sub(r'np\\.float64\\((.*?)\\)', r'\\1', data_str.strip())\n",
    "            data_dict = ast.literal_eval(cleaned_data_str)\n",
    "            data_dict = {k: float(v) for k, v in data_dict.items()}\n",
    "            data_dict['Filename'] = name.strip()\n",
    "            rows.append(data_dict)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Reorder columns to start with Filename\n",
    "cols = ['Filename'] + [col for col in df.columns if col != 'Filename']\n",
    "df = df[cols]\n",
    "\n",
    "# Save as CSV\n",
    "BASE_DIR = Path.cwd().parent\n",
    "csv_path = BASE_DIR/ \"results\"/ \"featureQuantileThres\"\n",
    "\n",
    "csv_file = \"AllFeatures_Stats_Converted.csv\" # Save CSV locally\n",
    "csv_path = os.path.join(csv_path,csv_file)\n",
    "\n",
    "# df.to_csv(\"AllFeatures_Stats_Converted.csv\", index=False)\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"CSV file saved as AllFeatures_Stats_Converted.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def create_mesh_from_mask(data, mask, title=\"Mesh\", transparency=0.3):\n",
    "    if not np.any(mask):\n",
    "        print(f\"⚠️ No points in {title}. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        verts, faces, _, _ = marching_cubes(data * mask, level=0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Mesh creation failed for {title}: {e}\")\n",
    "        return\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.4, 0.6, 1.0])\n",
    "    mesh = mesh.filter_smooth_simple(number_of_iterations=1)\n",
    "\n",
    "    app = o3d.visualization.gui.Application.instance\n",
    "    app.initialize()\n",
    "    window = app.create_window(title, 1024, 768)\n",
    "    scene = o3d.visualization.rendering.Open3DScene(window.renderer)\n",
    "\n",
    "    mat = o3d.visualization.rendering.MaterialRecord()\n",
    "    mat.shader = \"defaultLitTransparency\"\n",
    "    mat.base_color = [0.4, 0.6, 1.0, transparency]\n",
    "    mat.base_roughness = 0.5\n",
    "\n",
    "    scene.add_geometry(\"mesh\", mesh, mat)\n",
    "    bbox = mesh.get_axis_aligned_bounding_box()\n",
    "    \n",
    "    center = bbox.get_center()\n",
    "    eye = center + np.array([0, 0, -1])  # Convert to NumPy array\n",
    "    up = [0, -1, 0]\n",
    "\n",
    "    scene.scene.camera.look_at(center, eye, up)\n",
    "\n",
    "    # scene.scene.camera.look_at(\n",
    "    # bbox.get_center(),           # center\n",
    "    # bbox.get_center() + [0, 0, -1],  # eye position\n",
    "    # [0, -1, 0]                   # up vector\n",
    "    # )\n",
    "\n",
    "    # scene.setup_camera(60, bbox, bbox.get_center())\n",
    "\n",
    "    def on_layout(context):\n",
    "        r = window.content_rect\n",
    "        scene.scene.set_viewport(r)\n",
    "\n",
    "    window.set_on_layout(on_layout)\n",
    "    app.run()\n",
    "\n",
    "def threshold_and_visualize(npy_file_path, threshold_val=1.20148978654012):\n",
    "    print(f\"CAN SEE THE NPYPATH : {npy_file_path}\")\n",
    "    # data = np.load(str(npy_file_path))\n",
    "    data = np.load(npy_file_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D numpy array.\")\n",
    "\n",
    "    print(f\"📁 Loaded: {npy_file_path.name} with shape {data.shape}\")\n",
    "    \n",
    "    mask_lower = data <= threshold_val\n",
    "    mask_upper = data > threshold_val\n",
    "\n",
    "    # create_mesh_from_mask(data, mask_lower, title=\"Lower Threshold Mesh\", transparency=0.1)\n",
    "    create_mesh_from_mask(data, mask_upper, title=\"Upper Threshold Mesh\", transparency=0.2)\n",
    "\n",
    "\n",
    "# ====== Replace this with your actual .npy file path ======\n",
    "# import os\n",
    "\n",
    "# filepath = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\raw_npyData\"\n",
    "# npy_path = os.path.join(filepath,'tomo_Grafene_24h.npy')\n",
    "# # npy_path = Path(\"path/to/your/datafile.npy\")  # 🛠️ Replace this path\n",
    "# threshold_and_visualize(npy_path, threshold_val=1.44148978654012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Safe path definition\n",
    "filepath = Path(r\"E:/Projects/substructure_3d_data/Substructure_Different_DataTypes/data/raw_npyData\")\n",
    "npy_path = filepath / \"tomo_Grafene_24h.npy\"\n",
    "\n",
    "threshold_and_visualize(npy_path, threshold_val=1.201978654012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "thres = 5\n",
    "array = np.random.randint(0,10,size=(5,5,5))\n",
    "print(array)\n",
    "data = array > thres\n",
    "print(f\"\\n data --> {thres} --> \\n {data} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "thres = 0.5\n",
    "array = np.random.rand(2,2,2)\n",
    "print(array)\n",
    "data = array > thres\n",
    "print(f\"\\n data --> {thres} --> \\n {data} \\n\")\n",
    "array = array\n",
    "print(f\"arraysize:{array.shape} \\n and \\n {array} \\n \" )\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from scipy.io import loadmat  # For .mat support\n",
    "# volume = np.load('data')\n",
    "verts,faces,_, _ = marching_cubes(data,level=0)\n",
    "mesh = o3d.geometry.TriangleMesh()\n",
    "mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "mesh.compute_vertex_normals()\n",
    "mesh.paint_uniform_color([0.6, 0.2, 1.0])\n",
    "\n",
    "# Visualize\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "# Simulated 3D data (a sphere)\n",
    "# x, y, z = np.indices((100, 100, 100))\n",
    "# sphere = (x - 50)**2 + (y - 50)**2 + (z - 50)**2\n",
    "# volume = np.exp(-sphere / 500)  # Smooth decay\n",
    "data = np.zeros((2, 3, 3))\n",
    "data[1, 1, 1] = 1.0 \n",
    "# data[1, 1, 2] = 1.5 \n",
    "# data[1, 2, 2] = 1.8\n",
    "\n",
    "print(data)\n",
    "# Threshold\n",
    "# threshold = 0.5\n",
    "# binary = volume > threshold\n",
    "\n",
    "# Mesh\n",
    "# verts, faces, _, _ = marching_cubes(volume * binary, level=0)\n",
    "verts, faces, _, _ = marching_cubes(data, level=0)\n",
    "mesh = o3d.geometry.TriangleMesh()\n",
    "mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "mesh.compute_vertex_normals()\n",
    "mesh.paint_uniform_color([0.6, 0.2, 1.0])\n",
    "\n",
    "# Visualize\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### this below code is working fine even if large data size is there. make a function or class of it , and use it when required.\n",
    "- #### but in line at this position just increase the size here: \n",
    "- ### if simplify:\n",
    "   - ##### voxel_size = max(volume.shape) / 64  # can vary value from 64-128-256-512 etc to smooth the visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# this is workin fine even if large data size is there. make a function or class of it , and use it when required.\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be 3D\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, simplify=True):\n",
    "    # Automatically find threshold using Otsu’s method\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold used: {threshold:.4f}\")\n",
    "\n",
    "    # Marching cubes\n",
    "    print(\"[INFO] Extracting mesh using marching cubes...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Original mesh: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / 64  # Tweakable\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Simplified mesh: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    mesh.paint_uniform_color([0.6, 0.7, 1.0])\n",
    "    return mesh\n",
    "\n",
    "def visualize_mesh(mesh):\n",
    "#     o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "    o3d.visualization.draw_geometries([mesh], mesh_show_back_face=False)\n",
    "\n",
    "# ========== 🧪 Example ========== #\n",
    "# npy_path = \"path_to_your_large_3d_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, simplify=True)\n",
    "# visualize_mesh(mesh)\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "resultData = PROJECT_PATH/\"results\"\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "input_npy = npy_path  # <-- Replace with your actual file path\n",
    "# input_npy = os.listdir(input_npy)\n",
    "for filename  in os.listdir(input_npy):\n",
    "    if filename.endswith('.npy'):\n",
    "\n",
    "        print(f\"filename in the path : {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "    #     input_npy = input_npy/\"tomo_Grafene_24h.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "    #     volume = load_data(input_npy)\n",
    "        volume = load_data(str(npyfilePath))\n",
    "        mesh = create_mesh_from_volume(volume, simplify=True)\n",
    "        visualize_mesh(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be a 3D numpy array\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, grid_factor=128, simplify=True, color_mode='gradient'):\n",
    "    \"\"\"\n",
    "        Enhancements:\n",
    "        Control grid resolution using grid_factor (affects voxel_size for simplification).\n",
    "        Color grading based on vertex Z-values (color_mode='gradient') or keep uniform (color_mode='uniform').\n",
    "        Clear toggles for both via function parameters.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Compute Otsu threshold from non-zero values\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold: {threshold:.4f}\")\n",
    "\n",
    "    # Generate mesh using marching cubes\n",
    "    print(\"[INFO] Extracting mesh...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Mesh before simplification: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Simplify mesh\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / grid_factor\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Mesh after simplification: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    # Apply color grading\n",
    "    if color_mode == 'gradient':\n",
    "        z_vals = np.asarray(mesh.vertices)[:, 2]\n",
    "        z_min, z_max = z_vals.min(), z_vals.max()\n",
    "        norm_z = (z_vals - z_min) / (z_max - z_min + 1e-8)\n",
    "        colors = np.stack([norm_z, 0.6 * np.ones_like(norm_z), 1.0 - norm_z], axis=1)\n",
    "        mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        mesh.paint_uniform_color([0.6, 0.7, 1.0])  # Default blueish\n",
    "\n",
    "    return mesh\n",
    "\n",
    "def visualize_mesh(mesh):\n",
    "    o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "\n",
    "# ========== 🧪 Example Usage ========== #\n",
    "# npy_path = \"path_to_your_large_3d_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=32, simplify=True, color_mode='gradient')\n",
    "# visualize_mesh(mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "resultData = PROJECT_PATH/\"results\"\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "input_npy = npy_path  # <-- Replace with your actual file path\n",
    "# input_npy = os.listdir(input_npy)\n",
    "count  = 0 # to stop at some no.\n",
    "for filename  in os.listdir(input_npy):\n",
    "    count +=1\n",
    "    if filename.endswith('.npy'):\n",
    "\n",
    "        print(f\"filename in the path : {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "    #     input_npy = input_npy/\"tomo_Grafene_24h.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "    #     volume = load_data(input_npy)\n",
    "        volume = load_data(str(npyfilePath))\n",
    "        mesh = create_mesh_from_volume(volume, grid_factor=256, simplify=True, color_mode='gradient')\n",
    "        visualize_mesh(mesh)\n",
    "        if count == 1:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# improvement using d solution , \n",
    "\n",
    "# import numpy as np\n",
    "# import open3d as o3d\n",
    "# from skimage.measure import marching_cubes\n",
    "# from skimage.filters import threshold_otsu\n",
    "\n",
    "# def load_data(npy_path):\n",
    "#     data = np.load(npy_path)\n",
    "#     assert data.ndim == 3, \"Data must be a 3D numpy array\"\n",
    "#     return data\n",
    "\n",
    "# def create_mesh_from_volume(volume, grid_factor=32, simplify=True, color_mode='gradient'):\n",
    "#     # Compute Otsu threshold from non-zero values\n",
    "#     flat = volume[volume > 0].flatten()\n",
    "#     threshold = threshold_otsu(flat)\n",
    "#     print(f\"[INFO] Otsu Threshold: {threshold:.4f}\")\n",
    "\n",
    "#     # Generate mesh using marching cubes\n",
    "#     print(\"[INFO] Extracting mesh...\")\n",
    "#     verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "#     print(f\"[INFO] Mesh before simplification: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "#     mesh = o3d.geometry.TriangleMesh()\n",
    "#     mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "#     mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "#     mesh.compute_vertex_normals()\n",
    "\n",
    "#     # Simplify mesh\n",
    "#     if simplify:\n",
    "#         voxel_size = max(volume.shape) / grid_factor\n",
    "#         mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "#         print(f\"[INFO] Mesh after simplification: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "#     # Apply color grading\n",
    "#     if color_mode == 'gradient':\n",
    "#         z_vals = np.asarray(mesh.vertices)[:, 2]\n",
    "#         z_min, z_max = z_vals.min(), z_vals.max()\n",
    "#         norm_z = (z_vals - z_min) / (z_max - z_min + 1e-8)\n",
    "#         # Enhanced color gradient (red to green to blue)\n",
    "#         colors = np.zeros((len(norm_z), 3))\n",
    "#         colors[:, 0] = 1.0 - norm_z  # Red decreases with Z\n",
    "#         colors[:, 1] = norm_z        # Green increases with Z\n",
    "#         colors[:, 2] = norm_z        # Blue increases with Z\n",
    "#         mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "#     else:\n",
    "#         mesh.paint_uniform_color([0.6, 0.7, 1.0])  # Default blueish\n",
    "\n",
    "#     return mesh\n",
    "\n",
    "# def visualize_mesh(mesh, transparency=0.5):\n",
    "#     vis = o3d.visualization.Visualizer()\n",
    "#     vis.create_window()\n",
    "#     vis.add_geometry(mesh)\n",
    "\n",
    "#     render_opt = vis.get_render_option()\n",
    "#     render_opt.background_color = np.asarray([1.0, 1.0, 1.0])\n",
    "#     render_opt.mesh_show_back_face = True\n",
    "#     render_opt.mesh_show_wireframe = False\n",
    "#     render_opt.light_on = True\n",
    "#     render_opt.mesh_show_transparency = True\n",
    "\n",
    "#     vis.run()\n",
    "#     vis.destroy_window()\n",
    "\n",
    "\n",
    "# # delete it -------------------\n",
    "# # def visualize_mesh(mesh, transparency=0.5):\n",
    "# #     # # Create visualizer with material properties\n",
    "# #     # vis = o3d.visualization.Visualizer()\n",
    "# #     # vis.create_window()\n",
    "    \n",
    "# #     # # Add mesh with material properties\n",
    "# #     # mat = o3d.visualization.rendering.MaterialRecord()\n",
    "# #     # mat.shader = \"defaultLit\"\n",
    "    \n",
    "# #     # # Modern Open3D versions use base_alpha instead of transparency\n",
    "# #     #     # Handle version compatibility\n",
    "# #     # if hasattr(mat, 'transparency'):\n",
    "# #     #     mat.transparency = transparency\n",
    "# #     # else:\n",
    "# #     #     mat.base_alpha = 1.0 - transparency\n",
    "# #     # # mat.base_alpha = 1.0 - transparency  # Alpha is inverse of transparency\n",
    "    \n",
    "# #     # # Additional material properties for better visualization\n",
    "# #     # mat.base_roughness = 0.4\n",
    "# #     # mat.base_metallic = 0.0\n",
    "# #     # mat.base_color = [1.0, 1.0, 1.0, 1.0]  # RGBA\n",
    "    \n",
    "# #     # vis.add_geometry(mesh, material=mat)\n",
    "    \n",
    "# #     # Configure render options\n",
    "# #     render_opt = vis.get_render_option()\n",
    "# #     render_opt.mesh_show_back_face = True\n",
    "# #     render_opt.mesh_show_wireframe = False\n",
    "# #     render_opt.light_on = True\n",
    "# #     render_opt.background_color = np.asarray([1.0, 1.0, 1.0])\n",
    "# #     render_opt.mesh_show_transparency = True  # Critical for transparency\n",
    "    \n",
    "# #     # Run visualization\n",
    "# #     vis.run()\n",
    "# #     vis.destroy_window()\n",
    "\n",
    "# # delete it -------------------\n",
    "\n",
    "# # ========== Example Usage ========== #\n",
    "# # npy_path = \"path_to_your_data.npy\"\n",
    "# # volume = load_data(npy_path)\n",
    "# # mesh = create_mesh_from_volume(volume, grid_factor=64, simplify=True, color_mode='gradient')\n",
    "# # visualize_mesh(mesh, transparency=0.6)\n",
    "\n",
    "# # ========== Example Usage ========== #\n",
    "# # npy_path = \"path_to_your_data.npy\"\n",
    "# # volume = load_data(npy_path)\n",
    "# # Create mesh with 60% transparency\n",
    "# # mesh = create_mesh_from_volume(volume, grid_factor=64, simplify=True, \n",
    "# # color_mode='gradient', transparency=0.6)\n",
    "# # visualize_mesh(mesh)\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "def visualize_mesh_modern(mesh, base_alpha=0.6, color=[1.0, 0.7, 0.3]):\n",
    "    \"\"\"\n",
    "    Visualize mesh using modern Open3D GUI renderer with transparency.\n",
    "    \n",
    "    Args:\n",
    "        mesh (o3d.geometry.TriangleMesh): The mesh to visualize.\n",
    "        base_alpha (float): Transparency level (0.0 = fully transparent, 1.0 = opaque).\n",
    "        color (list): RGBA color list (only RGB is used here for simplicity).\n",
    "    \"\"\"\n",
    "    from open3d.visualization import rendering\n",
    "\n",
    "    # Setup material\n",
    "    mat = rendering.MaterialRecord()\n",
    "    mat.shader = \"defaultLit\"\n",
    "    mat.base_color = [color[0], color[1], color[2], base_alpha]\n",
    "    mat.base_roughness = 0.4\n",
    "    mat.base_metallic = 0.0\n",
    "    mat.base_reflectance = 0.5\n",
    "    mat.transparency = 1.0 - base_alpha\n",
    "    mat.point_size = 3.0\n",
    "\n",
    "    # Launch modern GUI renderer\n",
    "    o3d.visualization.draw([{\n",
    "        \"name\": \"TransparentMesh\",\n",
    "        \"geometry\": mesh,\n",
    "        \"material\": mat\n",
    "    }])\n",
    "\n",
    "\n",
    "\n",
    "volume = np.load(\"path_to_your_3d_volume.npy\")\n",
    "mesh = create_mesh_from_volume(volume)  # Your function that generates mesh\n",
    "visualize_mesh_modern(mesh, base_alpha=0.4, color=[0.2, 0.7, 1.0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# datafile = npy_path/\"tomo_Grafene_24h.npy\"\n",
    "datafile = npy_path/\"AML2_cell11.npy\"\n",
    "volume = load_data(str(datafile))\n",
    "# Create mesh first (without transparency)\n",
    "mesh = create_mesh_from_volume(volume, grid_factor=256, color_mode='gradient')\n",
    "\n",
    "# Then visualize with transparency\n",
    "transparency=0.6\n",
    "base_alpha = 1.0 - transparency \n",
    "visualize_mesh(mesh)  # 60% transparent\n",
    "# visualize_mesh(mesh, transparency=base_alpha)  # 60% transparent\n",
    "\n",
    "# mesh, alpha = create_mesh_from_volume(volume, grid_factor=64, simplify=True, color_mode='gradient', transparency=0.2)\n",
    "# visualize_mesh_with_transparency(mesh, transparency=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import open3d as o3d;\n",
    "print(f\"this is version of o3d: {o3d.__version__}\")\n",
    "# print(o3d.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be a 3D numpy array\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, grid_factor=128, simplify=True, color_mode='gradient'):\n",
    "    \"\"\"\n",
    "        Enhancements:\n",
    "        Control grid resolution using grid_factor (affects voxel_size for simplification).\n",
    "        Color grading based on vertex Z-values (color_mode='gradient') or keep uniform (color_mode='uniform').\n",
    "        Clear toggles for both via function parameters.\n",
    "    \"\"\"\n",
    "    # Compute Otsu threshold from non-zero values\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold: {threshold:.4f}\")\n",
    "\n",
    "    # Generate mesh using marching cubes\n",
    "    print(\"[INFO] Extracting mesh...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Mesh before simplification: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Simplify mesh\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / grid_factor\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Mesh after simplification: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    # Apply color grading\n",
    "    if color_mode == 'gradient':\n",
    "        z_vals = np.asarray(mesh.vertices)[:, 2]\n",
    "        z_min, z_max = z_vals.min(), z_vals.max()\n",
    "        norm_z = (z_vals - z_min) / (z_max - z_min + 1e-8)\n",
    "        colors = np.stack([norm_z, 0.6 * np.ones_like(norm_z), 1.0 - norm_z], axis=1)\n",
    "        mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        mesh.paint_uniform_color([0.6, 0.7, 1.0])  # Default blueish\n",
    "\n",
    "    return mesh\n",
    "\n",
    "# def visualize_mesh(mesh):\n",
    "#     o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "\n",
    "# ========== 🧪 Example Usage ========== #\n",
    "# npy_path = \"path_to_your_large_3d_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=32, simplify=True, color_mode='gradient')\n",
    "# visualize_mesh(mesh)\n",
    "\n",
    "def visualize_mesh(mesh, transparency=0.5):\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Convert to TriangleMeshModel for transparency\n",
    "    mesh.material = o3d.visualization.rendering.MaterialRecord()\n",
    "    mesh.material.shader = \"defaultLitTransparency\"\n",
    "    mesh.material.base_color = [1.0, 0.6, 1.0, transparency]  # RGBA\n",
    "    vis.get_render_option().background_color = np.array([0, 0, 0])  # black background\n",
    "\n",
    "    vis = o3d.visualization.O3DVisualizer(\"Transparent Mesh Viewer\", 1024, 768)\n",
    "    vis.add_geometry(\"Mesh\", mesh, mesh.material)\n",
    "    vis.reset_camera_to_default()\n",
    "    vis.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# npy_path = \"path/to/your/3ddata.npy\"\n",
    "from pathlib import Path\n",
    "import os\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "datafile = npy_path/\"tomo_Grafene_24h.npy\"\n",
    "volume = load_data(str(datafile))\n",
    "mesh = create_mesh_from_volume(volume, grid_factor=64, simplify=True, color_mode='gradient')\n",
    "visualize_mesh(mesh, transparency=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# data = range(11)\n",
    "# data = list(data)\n",
    "# print(data)\n",
    "# quartileData= np.quantile(data,0.50)\n",
    "# print(quartileData)\n",
    "from pathlib import Path\n",
    "import os\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "resultData = PROJECT_PATH/\"results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.filters import threshold_otsu\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "def visualize_ostu(extract_data,npy_path):\n",
    "    # Extract coordinates of foreground for plotting\n",
    "    foreground_mask = extract_data\n",
    "    coords = np.argwhere(foreground_mask)\n",
    "    coords = coords[np.random.choice(len(coords), size=min(len(coords), 50000), replace=False)]\n",
    "\n",
    "    # Plot\n",
    "    fig = px.scatter_3d(\n",
    "        x=coords[:, 0], y=coords[:, 1], z=coords[:, 2],\n",
    "        opacity=0.008,\n",
    "        title=f\"Foreground Voxel Visualization ({os.path.basename(npy_path)})\",\n",
    "        labels={'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, resultDataPath, output_dir=\"otsu_results\"):\n",
    "    output_dir = os.path.join(resultDataPath,output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    extractedFilename = os.path.basename(npy_path) # return the filename as string.\n",
    "    # Load the 3D data\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    # Apply Otsu threshold\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    # Create masks\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    # Save masks\n",
    "    np.save(os.path.join(output_dir, f\"fgnd_mask{extractedFilename}.npy\"), foreground_mask)\n",
    "    np.save(os.path.join(output_dir, f\"bgnd_mask{extractedFilename}.npy\"), background_mask)\n",
    "\n",
    "    # # Extract coordinates of foreground for plotting\n",
    "    # coords = np.argwhere(foreground_mask)\n",
    "    # coords = coords[np.random.choice(len(coords), size=min(len(coords), 50000), replace=False)]\n",
    "\n",
    "    # # Plot\n",
    "    # fig = px.scatter_3d(\n",
    "    #     x=coords[:, 0], y=coords[:, 1], z=coords[:, 2],\n",
    "    #     opacity=0.5,\n",
    "    #     title=f\"Foreground Voxel Visualization ({os.path.basename(npy_path)})\",\n",
    "    #     labels={'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
    "    # )\n",
    "    # fig.show()\n",
    "\n",
    "    return threshold, foreground_mask, background_mask\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    # PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "    print(PROJECT_PATH)\n",
    "    resultData = PROJECT_PATH/\"results\"\n",
    "    npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "    # npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "    input_npy = npy_path  # <-- Replace with your actual file path\n",
    "    # input_npy = os.listdir(input_npy)\n",
    "    for filename  in os.listdir(input_npy):\n",
    "        print(f\"filename in the path : {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "\n",
    "        res = apply_otsu_segmentation(npyfilePath,resultData,output_dir=\"otsu_results\")\n",
    "        visualize_ostu(res[1],npyfilePath)\n",
    "        visualize_ostu(res[2],npyfilePath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Re-import necessary packages after kernel reset\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.filters import threshold_otsu\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, resultDataPath, output_dir=\"otsu_results\"):\n",
    "    output_dir = os.path.join(resultDataPath, output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    extractedFilename = os.path.basename(npy_path)\n",
    "    data = np.load(npy_path)\n",
    "\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    np.save(os.path.join(output_dir, f\"{extractedFilename}_fg_mask.npy\"), foreground_mask)\n",
    "    np.save(os.path.join(output_dir, f\"{extractedFilename}_bg_mask.npy\"), background_mask)\n",
    "\n",
    "    return threshold, foreground_mask, background_mask\n",
    "\n",
    "\n",
    "def visualize_foreground_background(fg_mask, bg_mask, title=\"3D Otsu Segmentation\", use_mesh=False, downsample=True, max_points=50000):\n",
    "    fg_coords = np.argwhere(fg_mask)\n",
    "    bg_coords = np.argwhere(bg_mask)\n",
    "\n",
    "    if downsample:\n",
    "        if len(fg_coords) > max_points:\n",
    "            fg_coords = fg_coords[np.random.choice(len(fg_coords), max_points, replace=False)]\n",
    "        if len(bg_coords) > max_points:\n",
    "            bg_coords = bg_coords[np.random.choice(len(bg_coords), max_points, replace=False)]\n",
    "\n",
    "    if not use_mesh:\n",
    "        fig = px.scatter_3d(\n",
    "            x=fg_coords[:, 0], y=fg_coords[:, 1], z=fg_coords[:, 2],\n",
    "            color=fg_coords[:, 2],\n",
    "            opacity=0.08,\n",
    "            color_continuous_scale=\"Blues\",\n",
    "            title=f\"{title} - Foreground\",\n",
    "            labels={'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
    "        )\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=bg_coords[:, 0], y=bg_coords[:, 1], z=bg_coords[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=1, opacity=0.01, color='gray'),\n",
    "            name='Background'\n",
    "        ))\n",
    "    else:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Mesh3d(\n",
    "            x=fg_coords[:, 0], y=fg_coords[:, 1], z=fg_coords[:, 2],\n",
    "            alphahull=5,\n",
    "            opacity=0.15,\n",
    "            color='lightblue',\n",
    "            name='Foreground (Mesh)'\n",
    "        ))\n",
    "        # fig.add_trace(go.Mesh3d(\n",
    "        #     x=bg_coords[:, 0], y=bg_coords[:, 1], z=bg_coords[:, 2],\n",
    "        #     alphahull=10,\n",
    "        #     opacity=0.02,\n",
    "        #     color='gray',\n",
    "        #     name='Background (Mesh)'\n",
    "        # ))\n",
    "        fig.update_layout(title=title, scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(margin=dict(l=0, r=0, t=40, b=0))\n",
    "    fig.show()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     from pathlib import Path\n",
    "#     import os\n",
    "\n",
    "#     PROJECT_PATH = Path.cwd().parent\n",
    "#     # PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "#     print(PROJECT_PATH)\n",
    "#     resultData = PROJECT_PATH/\"results\"\n",
    "#     npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "#     # npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "#     input_npy = npy_path  # <-- Replace with your actual file path\n",
    "#     # input_npy = os.listdir(input_npy)\n",
    "#     for filename  in os.listdir(input_npy):\n",
    "#         print(f\"filename in the path : {filename}\")\n",
    "#         # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "#         npyfilePath = input_npy/filename\n",
    "#         # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "\n",
    "#         res = apply_otsu_segmentation(npyfilePath,resultData,output_dir=\"otsu_results\")\n",
    "#         visualize_ostu(res[1],npyfilePath)\n",
    "#         visualize_ostu(res[2],npyfilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "resultData = PROJECT_PATH/\"results\"\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "input_npy = npy_path  # <-- Replace with your actual file path\n",
    "# input_npy = os.listdir(input_npy)\n",
    "count = 0\n",
    "# filelistnName = np.random.choice(os.listdir(input_npy),5)\n",
    "# print(f\"randomly selected 5 files in --> {filelistnName}\")\n",
    "for filename in os.listdir(input_npy):\n",
    "# for filename in filelistnName:\n",
    "    if filename in ['tomo_Grafene_24h.npy','tomo_grafene_48h.npy']:\n",
    "        print(f\"filename in the path : and processing with it ---->  {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "    \n",
    "        # res = apply_otsu_segmentation(npyfilePath,resultData,output_dir=\"otsu_results\")\n",
    "        # visualize_ostu(res[1],npyfilePath)\n",
    "        # visualize_ostu(res[2],npyfilePath)\n",
    "        # apply_otsu_segmentation(npy_path, resultDataPath, output_dir=\"otsu_results\"):\n",
    "        threshold, fg_mask, bg_mask = apply_otsu_segmentation(npyfilePath, resultData,output_dir=\"otsu_results\")\n",
    "        visualize_foreground_background(fg_mask, bg_mask, title=f\"{filename[:-4]}\", use_mesh= True, downsample=True, max_points=60000)\n",
    "        # visualize_foreground_background(fg_mask, bg_mask, use_mesh=True, downsample=True)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        print(f\"couting the file processed  --> {count}\")\n",
    "    \n",
    "        if count == 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# x = range(5)\n",
    "# print(list(x))\n",
    "# for val in list(x):\n",
    "#     if val in [3,2]:\n",
    "#         print(f\"val is-->2,3 : {val}\")\n",
    "#     else:\n",
    "#         print(val)\n",
    "    \n",
    "# # if val in \n",
    "# # y = np.random.choice(x,3)\n",
    "# print(f\"{x}, y--> {y} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from skimage.filters import threshold_otsu\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation_with_cupy(npy_path, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    flat_data = cp.asarray(data[data > 0].flatten())\n",
    "    threshold = float(threshold_otsu(cp.asnumpy(flat_data)))\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    base_name = Path(npy_path).stem\n",
    "    np.save(output_dir / f\"{base_name}_fg_mask.npy\", foreground_mask)\n",
    "    np.save(output_dir / f\"{base_name}_bg_mask.npy\", background_mask)\n",
    "\n",
    "    return threshold, foreground_mask, background_mask\n",
    "\n",
    "\n",
    "def visualize_with_open3d(fg_mask, bg_mask, title=\"3D Visualization\", downsample=True, max_points=50000):\n",
    "    fg_coords = np.argwhere(fg_mask)\n",
    "    bg_coords = np.argwhere(bg_mask)\n",
    "\n",
    "    if downsample:\n",
    "        if len(fg_coords) > max_points:\n",
    "            fg_coords = fg_coords[np.random.choice(len(fg_coords), max_points, replace=False)]\n",
    "        if len(bg_coords) > max_points:\n",
    "            bg_coords = bg_coords[np.random.choice(len(bg_coords), max_points, replace=False)]\n",
    "\n",
    "    fg_pcd = o3d.geometry.PointCloud()\n",
    "    fg_pcd.points = o3d.utility.Vector3dVector(fg_coords)\n",
    "    fg_colors = np.tile([0.3, 0.5, 1.0], (fg_coords.shape[0], 1))\n",
    "    fg_pcd.colors = o3d.utility.Vector3dVector(fg_colors)\n",
    "\n",
    "    bg_pcd = o3d.geometry.PointCloud()\n",
    "    bg_pcd.points = o3d.utility.Vector3dVector(bg_coords)\n",
    "    bg_colors = np.tile([0.6, 0.6, 0.6], (bg_coords.shape[0], 1))\n",
    "    bg_pcd.colors = o3d.utility.Vector3dVector(bg_colors)\n",
    "\n",
    "    o3d.visualization.draw_geometries([fg_pcd, bg_pcd], window_name=title)\n",
    "\n",
    "\n",
    "def batch_otsu_segmentation(input_dir, output_dir, use_open3d=True, downsample=True, max_points=50000):\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    npy_files = list(input_dir.glob(\"*.npy\"))\n",
    "\n",
    "    print(f\"🔍 Found {len(npy_files)} .npy files in: {input_dir}\")\n",
    "\n",
    "    for file in npy_files:\n",
    "        print(f\"\\n🚀 Processing: {file.name}\")\n",
    "        try:\n",
    "            threshold, fg_mask, bg_mask = apply_otsu_segmentation_with_cupy(file, output_dir)\n",
    "            if use_open3d:\n",
    "                visualize_with_open3d(fg_mask, bg_mask, title=file.stem, downsample=downsample, max_points=max_points)\n",
    "            print(f\"✅ Finished {file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed {file.name}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_npy_path = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    result_path = PROJECT_PATH / \"results\" / \"otsu_gpu\"\n",
    "\n",
    "    batch_otsu_segmentation(\n",
    "        input_dir=input_npy_path,\n",
    "        output_dir=result_path,\n",
    "        use_open3d=True,\n",
    "        downsample=True,\n",
    "        max_points=50000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"script has been modified to include the following enhancements:\n",
    "\n",
    "User Options:\n",
    "\n",
    "Choose to visualize foreground, background, or both.\n",
    "\n",
    "Enable or disable saving of .obj files.\n",
    "\n",
    "Color Grading:\n",
    "\n",
    "Color intensity is based on voxel values using matplotlib color maps.\n",
    "\n",
    "Interactive Visualization:\n",
    "\n",
    "Foreground and background are visualized using Open3D with transparency and downsampling.\n",
    "\n",
    "Robust Saving:\n",
    "\n",
    "Saves .obj files for foreground/background point clouds if enabled.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from skimage.filters import threshold_otsu\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation_with_cupy(npy_path, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    flat_data = cp.asarray(data[data > 0].flatten())\n",
    "    threshold = float(threshold_otsu(cp.asnumpy(flat_data)))\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    base_name = Path(npy_path).stem\n",
    "    np.save(output_dir / f\"{base_name}_fg_mask.npy\", foreground_mask)\n",
    "    np.save(output_dir / f\"{base_name}_bg_mask.npy\", background_mask)\n",
    "\n",
    "    return threshold, data, foreground_mask, background_mask\n",
    "\n",
    "\n",
    "def visualize_with_open3d(fg_mask, bg_mask, data, title=\"3D Visualization\", show_fg=True, show_bg=False,\n",
    "                           downsample=True, max_points=50000, save_obj=False, obj_output_dir=None):\n",
    "    geometries = []\n",
    "\n",
    "    if show_fg:\n",
    "        fg_coords = np.argwhere(fg_mask)\n",
    "        if downsample and len(fg_coords) > max_points:\n",
    "            fg_coords = fg_coords[np.random.choice(len(fg_coords), max_points, replace=False)]\n",
    "        fg_values = data[tuple(fg_coords.T)]\n",
    "        fg_colors = plt.get_cmap(\"Blues\")((fg_values - fg_values.min()) / (np.ptp(fg_values) + 1e-6))[:, :3]\n",
    "        fg_pcd = o3d.geometry.PointCloud()\n",
    "        fg_pcd.points = o3d.utility.Vector3dVector(fg_coords)\n",
    "        fg_pcd.colors = o3d.utility.Vector3dVector(fg_colors)\n",
    "        geometries.append(fg_pcd)\n",
    "        if save_obj and obj_output_dir:\n",
    "            o3d.io.write_point_cloud(str(Path(obj_output_dir) / f\"{title}_foreground.obj\"), fg_pcd)\n",
    "\n",
    "    if show_bg:\n",
    "        bg_coords = np.argwhere(bg_mask)\n",
    "        if downsample and len(bg_coords) > max_points:\n",
    "            bg_coords = bg_coords[np.random.choice(len(bg_coords), max_points, replace=False)]\n",
    "        bg_values = data[tuple(bg_coords.T)]\n",
    "        bg_colors = plt.get_cmap(\"Greys\")((bg_values - bg_values.min()) / (np.ptp(bg_values) + 1e-6))[:, :3]\n",
    "        bg_pcd = o3d.geometry.PointCloud()\n",
    "        bg_pcd.points = o3d.utility.Vector3dVector(bg_coords)\n",
    "        bg_pcd.colors = o3d.utility.Vector3dVector(bg_colors)\n",
    "        geometries.append(bg_pcd)\n",
    "        if save_obj and obj_output_dir:\n",
    "            o3d.io.write_point_cloud(str(Path(obj_output_dir) / f\"{title}_background.obj\"), bg_pcd)\n",
    "\n",
    "    if geometries:\n",
    "        o3d.visualization.draw_geometries(geometries, window_name=title)\n",
    "\n",
    "\n",
    "def batch_otsu_segmentation(input_dir, output_dir, use_open3d=True, downsample=True, max_points=50000,\n",
    "                             show_fg=True, show_bg=False, save_obj=False):\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    npy_files = list(input_dir.glob(\"*.npy\"))\n",
    "\n",
    "    print(f\"🔍 Found {len(npy_files)} .npy files in: {input_dir}\")\n",
    "\n",
    "    for file in npy_files:\n",
    "        print(f\"\\n🚀 Processing: {file.name}\")\n",
    "        try:\n",
    "            threshold, data, fg_mask, bg_mask = apply_otsu_segmentation_with_cupy(file, output_dir)\n",
    "            if use_open3d:\n",
    "                visualize_with_open3d(\n",
    "                    fg_mask, bg_mask, data,\n",
    "                    title=file.stem,\n",
    "                    show_fg=show_fg,\n",
    "                    show_bg=show_bg,\n",
    "                    downsample=downsample,\n",
    "                    max_points=max_points,\n",
    "                    save_obj=save_obj,\n",
    "                    obj_output_dir=output_dir\n",
    "                )\n",
    "            print(f\"✅ Finished {file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed {file.name}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_npy_path = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    result_path = PROJECT_PATH / \"results\" / \"otsu_gpu\"\n",
    "\n",
    "    batch_otsu_segmentation(\n",
    "        input_dir=input_npy_path,\n",
    "        output_dir=result_path,\n",
    "        use_open3d=True,\n",
    "        downsample=True,\n",
    "        max_points=80000,\n",
    "        show_fg=True,\n",
    "        show_bg=False,\n",
    "        save_obj=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This works fine for me I have written all the code in one file in Dataplotter.py file in a class qith the same name as function name.\n",
    "# from pathlib import Path involked using the test.py  workin fine . \n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "# import imageio\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "import os\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# smoothed = gaussian_filter(data * fg_mask, sigma=1.0)\n",
    "# verts, faces, _, _ = marching_cubes(smoothed, level=0)\n",
    "\n",
    "# mesh = o3d.geometry.TriangleMesh()\n",
    "# mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "# mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "\n",
    "# mesh.compute_vertex_normals()\n",
    "# mesh = mesh.filter_smooth_laplacian(number_of_iterations=5)\n",
    "# mesh.paint_uniform_color([0.8, 0.9, 1.0])\n",
    "\n",
    "\n",
    "\n",
    "def visualize_and_export_3d_mesh(fg_mask, data, smoothing = None, title= None,\n",
    "                                 save_obj_path=None,\n",
    "                                 save_png_path=None,\n",
    "                                 save_gif_path=None,\n",
    "                                 rotate_and_capture=False,\n",
    "                                 gif_frames=36):\n",
    "\n",
    "    if not np.any(fg_mask):\n",
    "        print(\" Foreground mask is empty. Skipping visualization.\")\n",
    "        return\n",
    "    \n",
    "    if smoothing is None:\n",
    "        verts, faces, _, _ = marching_cubes(data * fg_mask, level=0)\n",
    "\n",
    "    else:\n",
    "        # Apply Gaussian smoothing      \n",
    "        # smoothed_data = gaussian_filter(data * fg_mask, sigma=1.0)  #  smoothning = 1.0\n",
    "        smoothed_data = gaussian_filter(data * fg_mask, sigma=smoothing)\n",
    "        verts, faces, _, _ = marching_cubes(smoothed_data * fg_mask, level=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.7, 0.9, 1.0])  # Light blue\n",
    "\n",
    "    fg_points = np.argwhere(fg_mask)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(fg_points)\n",
    "    pcd.paint_uniform_color([1.0, 0.7, 0.3])  # Light orange\n",
    "\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name=title, width=1024, height=768, visible=True)\n",
    "    vis.add_geometry(mesh)\n",
    "    vis.add_geometry(pcd)\n",
    "\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([1, 1, 1])\n",
    "    opt.mesh_show_back_face = True\n",
    "    opt.light_on = True\n",
    "    opt.point_size = 2.5\n",
    "\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    ctr = vis.get_view_control()\n",
    "\n",
    "    # Save PNG snapshot\n",
    "    if save_png_path:\n",
    "        vis.capture_screen_image(save_png_path)\n",
    "        print(f\" PNG saved: {save_png_path}\")\n",
    "\n",
    "    # Auto-rotate and save as GIF\n",
    "    if rotate_and_capture and save_gif_path:\n",
    "        images = []\n",
    "        tmp_paths =[]\n",
    "        for i in range(gif_frames):\n",
    "            ctr.rotate(10.0, 0.0)  # Rotate around y-axis\n",
    "            vis.poll_events()\n",
    "            vis.update_renderer()\n",
    "            tmp_path = f\"_tmp_frame_{i:03d}.png\"\n",
    "            vis.capture_screen_image(tmp_path)\n",
    "            images.append(imageio.imread(tmp_path))\n",
    "            tmp_paths.append(tmp_path) # save for deletion\n",
    "\n",
    "        imageio.mimsave(save_gif_path, images, duration=0.1)\n",
    "        print(f\"GIF saved: {save_gif_path}\")\n",
    "\n",
    "        # Clean up temporary frames\n",
    "        for p in tmp_paths:\n",
    "            if os.path.exists(p):\n",
    "                os.remove(p)\n",
    "\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "    # if save_obj_path:\n",
    "    #     o3d.io.write_triangle_mesh(save_obj_path, mesh)\n",
    "    #     print(f\"✅ Mesh saved to: {save_obj_path}\")\n",
    "\n",
    "\n",
    "# To use this code run this: ----------->\n",
    "\n",
    "# visualize_and_export_3d_mesh(\n",
    "#     fg_mask=my_mask,\n",
    "#     data=my_data,\n",
    "#     title=\"Substructure View\",\n",
    "#     save_obj_path=\"output.obj\",\n",
    "#     save_png_path=\"snapshot.png\",\n",
    "#     save_gif_path=\"rotation.gif\",\n",
    "#     rotate_and_capture=True\n",
    "# )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "import os\n",
    "from listspecificfiles import readlistFiles\n",
    "from preprocessAll import DataPreprocessor\n",
    "from plot3dint import plot3dinteractive\n",
    "# from plot_dataModule import DataPlotter\n",
    "\n",
    "filepath1 = r\"data\\raw_npyData\"\n",
    "\n",
    "fpath = readlistFiles(filepath1,'.npy').file_with_Path()\n",
    "\n",
    "\n",
    "\n",
    "Background_removed_path  = RES_DIR / \"backgroud_removedchecked\"\n",
    "os.makedirs(Background_removed_path,exist_ok=True)\n",
    "\n",
    "for file in fpath:\n",
    "    filename = os.path.basename(file)\n",
    "    if filename.endswith('.npy') and filename !='tomo_grafene_48h.npy':\n",
    "        data = np.load(file)\n",
    "        save_png_path = os.path.join(Background_removed_path,f\"{filename[:-4]}bgOnly.png\")\n",
    "        save_gif_path = os.path.join(Background_removed_path,f\"{filename[:-4]}bgOnly.gif\")\n",
    "\n",
    "        \n",
    "        Masked_data, maskedValues_coordsOnly, filtered_Data_WithoutZero, UnMasked_coords, mask = DataPreprocessor.DataMasker(data, maskValue=1.334, masked_WithZero=True, masked_ANDRemoved=False)\n",
    "\n",
    "        # plot3dinteractive(Masked_data,keyvalue=filename,sample_fraction=0.05)\n",
    "\n",
    "        visualize_and_export_3d_mesh(~mask,data,smoothing=None,title=filename[:-4],save_obj_path=None,save_png_path=save_png_path, save_gif_path=save_gif_path,rotate_and_capture=True)\n",
    "        \n",
    "        # DataPlotter.visualize_and_export_3d_mesh(mask,data,smoothing=None,title=filename[:-4],save_obj_path=None,save_png_path=save_png_path, save_gif_path=save_gif_path,rotate_and_capture=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import marching_cubes\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Input must be a 3D array\")\n",
    "\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    fg_mask = data > threshold\n",
    "    bg_mask = ~fg_mask\n",
    "\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_fg_mask.npy\"), fg_mask)\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_bg_mask.npy\"), bg_mask)\n",
    "\n",
    "    return threshold, fg_mask, bg_mask, data\n",
    "\n",
    "\n",
    "################# this is old working but with a dark foreground that disable to see the hidden substructure , because outer surface is darker in color, below is improve dcode.\n",
    "\n",
    "# def visualize_3d_marching_cubes(fg_mask, data, title, save_obj_path=None):\n",
    "#     if not np.any(fg_mask):\n",
    "#         print(\" Foreground mask is empty. Skipping visualization.\")\n",
    "#         return\n",
    "\n",
    "#     verts, faces, _, _ = marching_cubes(data * fg_mask, level=0)\n",
    "#     mesh = o3d.geometry.TriangleMesh()\n",
    "#     mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "#     mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "#     mesh.compute_vertex_normals()\n",
    "#     mesh.paint_uniform_color([0.2, 0.6, 1.0])\n",
    "\n",
    "#     fg_points = np.argwhere(fg_mask)\n",
    "#     pcd = o3d.geometry.PointCloud()\n",
    "#     pcd.points = o3d.utility.Vector3dVector(fg_points)\n",
    "#     pcd.paint_uniform_color([1.0, 0.5, 0.0])\n",
    "\n",
    "#     o3d.visualization.draw_geometries([pcd, mesh], window_name=title)\n",
    "    # if save_obj_path:\n",
    "    #     o3d.io.write_triangle_mesh(save_obj_path, mesh)\n",
    "\n",
    "##########  updated code for visualization : #############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualize_3d_marching_cubes(fg_mask, data, title, save_obj_path=None):\n",
    "    if not np.any(fg_mask):\n",
    "        print(\" Foreground mask is empty. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    # Marching cubes on masked data\n",
    "    verts, faces, _, _ = marching_cubes(data * fg_mask, level=0)\n",
    "\n",
    "    # Create Mesh\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.7, 0.9, 1.0])  # light sky-blue\n",
    "\n",
    "    # Create Point Cloud (for inner features)\n",
    "    fg_points = np.argwhere(fg_mask)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(fg_points)\n",
    "    pcd.paint_uniform_color([1.0, 0.7, 0.3])  # light orange\n",
    "\n",
    "    # Visualize with enhanced settings\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window(window_name=title, width=1024, height=768)\n",
    "    vis.add_geometry(mesh)\n",
    "    vis.add_geometry(pcd)\n",
    "\n",
    "    opt = vis.get_render_option()\n",
    "    opt.background_color = np.asarray([1, 1, 1])  # white background\n",
    "    opt.mesh_show_back_face = True\n",
    "    opt.light_on = True\n",
    "    # opt.point_size = 2.5\n",
    "    # opt.line_width = 10.0\n",
    "    opt.point_size = 1.5\n",
    "    opt.line_width = 4.0\n",
    "    opt.mesh_color_option = o3d.visualization.MeshColorOption.Color\n",
    "    opt.point_color_option = o3d.visualization.PointColorOption.Color\n",
    "\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "    # # Optional Save\n",
    "    # if save_obj_path:\n",
    "    #     o3d.io.write_triangle_mesh(save_obj_path, mesh)\n",
    "    #     print(f\"✅ Mesh saved to: {save_obj_path}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_dir = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    output_dir = PROJECT_PATH / \"results\" / \"otsu_results\"\n",
    "    image_plotpath = PROJECT_PATH / \"results\" / \"otsu_plot\"\n",
    "    os.makedirs(image_plotpath, exist_ok = True)\n",
    "\n",
    "    print(f\"Found {len(os.listdir(input_dir))} .npy files in: {input_dir}\\n\")\n",
    "\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".npy\"):\n",
    "            print(f\"\\n🚀 Processing: {file}\")\n",
    "            npy_path = input_dir / file\n",
    "            save_png_path= os.path.join(image_plotpath, f\"{file[:-4]}.png\")\n",
    "            save_gif_path= os.path.join(image_plotpath, f\"{file[:-4]}.gif\")\n",
    "\n",
    "            try:\n",
    "                threshold, fg_mask, bg_mask, data = apply_otsu_segmentation(npy_path, output_dir)\n",
    "                print(f\"Otsu Threshold: {threshold:.7f}\")\n",
    "                # visualize_3d_marching_cubes(fg_mask, data, title=file, save_obj_path=output_dir / f\"{file[:-4]}_mesh.obj\")\n",
    "                # visualize_3d_marching_cubes(bg_mask, data, title=file[:-4], save_obj_path=None)\n",
    "                visualize_and_export_3d_mesh(\n",
    "                    fg_mask=fg_mask,\n",
    "                    data= data,\n",
    "                    smoothning=1.0, # Adjust this value as needed\n",
    "                    title=file[:-4],\n",
    "                    # save_obj_path=\"output.obj\",\n",
    "                    save_obj_path= None,\n",
    "                    save_png_path= save_png_path,\n",
    "                    save_gif_path= save_gif_path,\n",
    "                    rotate_and_capture=True\n",
    "                )\n",
    "\n",
    "                print(f\" <------  Finished {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\" <--- Failed {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gc  #  garbage colloctor\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from preprocessAll import DataPreprocessor\n",
    "# from plot_dataModule import visualize_and_export_3d_mesh\n",
    "from plot_dataModule import DataPlotter\n",
    "\n",
    "# from gui_utils import show_progress_gui\n",
    "\n",
    "from gui_utils import ProgressGUI\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "PROJECT_PATH = Path(__file__).resolve().parent.parent.parent\n",
    "\n",
    "input_dir = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "output_dir = PROJECT_PATH / \"results\" / \"otsu_results\"\n",
    "image_plotpath = PROJECT_PATH / \"results\" / \"otsu_plot\"\n",
    "\n",
    "os.makedirs(image_plotpath, exist_ok=True)\n",
    "\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith(\".npy\")]\n",
    "print(f\"\\n Found {len(files)} .npy files {files} in: {input_dir}\")\n",
    "\n",
    "# window, update_progress = show_progress_gui(len(files))\n",
    "gui = ProgressGUI(len(files))\n",
    "\n",
    "# for file in files:\n",
    "for idx, file in enumerate(files):\n",
    "    # Update the progress bar and label\n",
    "    # update_progress(idx + 1, file)\n",
    "    # update_progress(idx + 1, file)\n",
    "    if gui.is_cancelled():\n",
    "        break\n",
    "\n",
    "    gui.update(idx + 1, file)   \n",
    "\n",
    "    npy_path = input_dir / file\n",
    "    save_png_path = image_plotpath / f\"{file[:-4]}fg.png\"\n",
    "    save_gif_path = image_plotpath / f\"{file[:-4]}fg.gif\"\n",
    "    data = np.load(str(npy_path))\n",
    "    try:\n",
    "        print(f\"\\n Processing: {file}\")\n",
    "\n",
    "        res = DataPreprocessor(data).apply_otsu_segmentation(save_masks_to=None)\n",
    "\n",
    "        threshold, fg_mask, bg_mask  = res['threshold'], res['fg_mask'], res['bg_mask']\n",
    "        print(f\"Otsu Threshold: {threshold:.7f}\")\n",
    "\n",
    "        DataPlotter.visualize_and_export_3d_mesh(\n",
    "            fg_mask=fg_mask,\n",
    "            data=data,\n",
    "            smoothing=None,\n",
    "            title=file[:-4],\n",
    "            save_obj_path=None,\n",
    "            save_png_path=None,  #save_png_path\n",
    "            save_gif_path= None, #save_gif_path,\n",
    "            rotate_and_capture=True,\n",
    "            gif_frames=36           \n",
    "        )\n",
    "\n",
    "        print(f\" Finished: {file}\")\n",
    "       \n",
    "        # Save the threshold value to a text file\n",
    "        threshold_file = output_dir / f\"{file[:-4]}_ostu_threshold.txt\"  \n",
    "        with open(threshold_file, 'w') as f:\n",
    "            f.write(f\"Otsu Threshold: {threshold:.7f}\\n\")\n",
    "            f.write(f\"Foreground Mask Shape: {fg_mask.shape}\\n\")\n",
    "            f.write(f\"Background Mask Shape: {bg_mask.shape}\\n\")\n",
    "\n",
    "        # Clear memory manually\n",
    "        # del data, res, fg_mask, bg_mask\n",
    "        # gc.collect()      \n",
    "        del data, res, fg_mask, bg_mask\n",
    "        # Optional: Force garbage collection    \n",
    "        gc.collect()    \n",
    "\n",
    "     \n",
    "        time.sleep(2)  # Optional: Add a delay to observe the output    \n",
    "        # input(\" Press Enter to process the next file...\")\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        print(f\" Failed {file}: {e}\")\n",
    "        \n",
    "gui.close()\n",
    "\n",
    "print(f\"\\n Finished processing all files in: {input_dir} and \\n saved results to: {output_dir} \\n\")\n",
    "# Close the progress window\n",
    "# window.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib as Path\n",
    "\n",
    "def boxplot1(scores,file):\n",
    "        \n",
    "    # scores = [56, 67, 70, 73, 73, 75, 78, 85, 90, 91, 99]\n",
    "    plt.boxplot(scores)\n",
    "    x = np.linspace(0,1,len(scores))\n",
    "    q1 = np.quantile(scores,0.25)\n",
    "    q2= np.quantile(scores,0.50)\n",
    "    q3= np.quantile(scores,0.75)\n",
    "    # q1 = q1*np.ones_like(x) \n",
    "    # q2 = q2*np.ones_like(x) \n",
    "    # q3 = q3*np.ones_like(x) \n",
    "    print(f\"all qunatile \\n q1:{q1} \\n q2:{q2} \\n q3:{q3}\")\n",
    "\n",
    "    # plt.plot(x,q1,x,q2,x,q3)\n",
    "    plt.title(f\"{file[:-4]}\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "input_dir = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "output_dir = PROJECT_PATH / \"results\" / \"otsu_results\"\n",
    "image_plotpath = PROJECT_PATH / \"results\" / \"otsu_plot\"\n",
    "os.makedirs(image_plotpath, exist_ok = True)\n",
    "\n",
    "print(f\"Found {len(os.listdir(input_dir))} .npy files in: {input_dir}\\n\")\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".npy\"):\n",
    "        print(f\"\\n🚀 Processing: {file}\")\n",
    "        fulldatapath = input_dir/file\n",
    "        data = np.load(fulldatapath)\n",
    "        data1 = data.flatten()\n",
    "        boxplot1(data1,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('_mpl-gallery')\n",
    "\n",
    "# make data:\n",
    "np.random.seed(10)\n",
    "D = np.random.normal((3, 5, 4), (1.25, 1.00, 1.25), (100, 3))\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "VP = ax.boxplot(D, positions=[2, 4, 6], widths=1.5, patch_artist=True,\n",
    "                showmeans=False, showfliers=False,\n",
    "                medianprops={\"color\": \"white\", \"linewidth\": 0.5},\n",
    "                boxprops={\"facecolor\": \"C0\", \"edgecolor\": \"white\",\n",
    "                          \"linewidth\": 0.5},\n",
    "                whiskerprops={\"color\": \"C0\", \"linewidth\": 1.5},\n",
    "                capprops={\"color\": \"C0\", \"linewidth\": 1.5})\n",
    "\n",
    "ax.set(xlim=(0, 8), xticks=np.arange(1, 8),\n",
    "       ylim=(0, 8), yticks=np.arange(1, 8))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This script copies specific files from a source directory to a destination directory based on their suffixes.  \n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# # Source and destination directories\n",
    "# src_dir = Path(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\sepeareted_background_mainData\")\n",
    "# dst_dir = Path(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\processed\\bgremovedData\")\n",
    "\n",
    "# # Ensure destination exists\n",
    "# dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # File suffixes to match\n",
    "# suffixes = ('masked_data.npy', 'bgmaskvalues.npy')\n",
    "\n",
    "# # Walk through the source directory\n",
    "# for root, _, files in os.walk(src_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith(suffixes):\n",
    "#             src_file = Path(root) / file\n",
    "#             dst_file = dst_dir / file\n",
    "#             shutil.copy2(src_file, dst_file)\n",
    "#             print(f\"Copied: {src_file} -> {dst_file}\")\n",
    "\n",
    "# print(\"Copy operation completed.\")\n",
    "\n",
    "# this script cut files with masked_data.npy and bgmaskvalues.npy from the source directory and paste them to the destination directory.\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Source directory\n",
    "src_dir = Path(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\processed\\bgremovedData\")\n",
    "# Destination directory: create 'main_fgdata' in the parent of src_dir\n",
    "dst_dir = src_dir.parent / \"main_fgdata\"\n",
    "dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "suffix = \"masked_data.npy\"\n",
    "\n",
    "for root, _, files in os.walk(src_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(suffix):\n",
    "            src_file = Path(root) / file\n",
    "            dst_file = dst_dir / file\n",
    "            shutil.move(str(src_file), str(dst_file))\n",
    "            print(f\"Moved: {src_file} \\n -> {dst_file}\")\n",
    "\n",
    "print(\"Cut operation completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
