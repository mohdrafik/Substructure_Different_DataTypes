{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2af479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using module path: c:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\n",
      "üîÑ Module path added to sys.path: c:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\n"
     ]
    }
   ],
   "source": [
    "%run ../src/path_manager.py  # This runs the whole file once, so if AddPath() is called at the bottom of path_manager.py, \n",
    "# it will add your module path automatically.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# Normalized_data = Path.cwd().parent/\"data\"/\"normalized_npyData\"\n",
    "# import os \n",
    "\n",
    "# print(Normalized_data)\n",
    "\n",
    "# # lisspecificfiles.py\n",
    "# def readlistFiles(filepath,keyword):\n",
    "    \n",
    "#     Files = os.listdir(filepath)\n",
    "#     print(Files)\n",
    "#     for File in Files:\n",
    "#         if File.endswith(keyword):\n",
    "#             print(File)\n",
    "        \n",
    "# keyword = 'normalized.npy'\n",
    "# # readlistFiles(Normalized_data,keyword ='normalized.npy')\n",
    "# readlistFiles(Normalized_data,keyword ='Copy.txt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# datapath =r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\normalized_npyData\" \n",
    "# datapath = os.path.normpath(datapath)\n",
    "# print(datapath)\n",
    "# # datapath = Normalized_data\n",
    "# k1 = 'Copy.txt'\n",
    "# # k1 = 'NORMALIZED.NPY'\n",
    "# # k1 = k1.lower()\n",
    "# d1 = listmatchedFiles(datapath,k1)\n",
    "# # d1.matchedFiles()\n",
    "# print(d1.matched_Files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200b2d6",
   "metadata": {},
   "source": [
    "## convert the .mat data into the .npy data using the main key value from the main (.mat)  data files and saved in \"outputdatapath\".\n",
    "\n",
    "# Data/  --> Description \n",
    "- #### raw_npyData/ is directory  --> where file saved as .npy file after converting from .mat file.\n",
    "- #### normalized_matData/ is directory  --> where matNormalized data files saved as ..normalized.mat file after normalizing .mat file.\n",
    "- #### normalized_npyData/ is directory  --> where npyNormalized data files saved as -..normalized.npy file after normalizing .npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the auxfunction \n",
    "import sys\n",
    "import os\n",
    "# # Define the module path\n",
    "# module_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "# if not os.path.exists(module_path):\n",
    "#     module_path = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "\n",
    "# # Add the module path to sys.path if it's not already there\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "    \n",
    "import createmat2npy as mnpy   #  this module load the .mat file,extract data according to the key and convert them into .npy file.\n",
    "datapath = r'C:\\Users\\mrafik\\OneDrive - C.N.R. STIIMA\\tomogram all data\\all_tomogram_data'\n",
    "outputdatapath = r'E:/Projects/substructure_3d_data/Substructure_Different_DataTypes/data/raw_npyData/'\n",
    "outputdatapath = os.path.normpath(outputdatapath) \n",
    "if not os.path.exists(datapath and outputdatapath):\n",
    "    datapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\SharedContents\\OneDrive - C.N.R. STIIMA\\tomogram all data\\all_tomogram_data\"\n",
    "    outputdatapath = r'C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw_npyData'\n",
    "\n",
    "mnpy.mat2npy(datapath,outputdatapath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27129e6c-ce48-4fe6-8aae-adcbf52a8fee",
   "metadata": {},
   "source": [
    "## A function is defined to load and normalize 3D Numpy data: defined in the file createmat2npy.py file with name: load_and_normalize_npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152098e-4d5d-49f0-b4bd-e8c65715c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python Script for Loading & Normalization \n",
    "# sys.path.append(module_path)\n",
    "# import createmat2npy as mnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033a197-a0f2-4ff5-9e6e-cd617c009055",
   "metadata": {},
   "source": [
    "### all different normalized npy array data is stored in the normalized data and Dictionary saved as MATLAB .mat with name as 'all_normalizeddata.mat'\n",
    "\n",
    "- And seperatley saved the each npy file as  \"..._normalized.mat\" format also.(3d matrix format data) and also in               \"...__normalized.npy file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing .npy files (update this with your folder path)\n",
    "import scipy.io as sio # type: ignore\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np # type: ignore\n",
    "import createmat2npy as mnp\n",
    "import os\n",
    "\n",
    "data_folder = outputdatapath # raw_npyData/ \n",
    "\n",
    "BASE_DIR = Path.cwd().parent # where all code is there.\n",
    "\n",
    "Normalized_npyDataDir = BASE_DIR/ \"data\" / \"normalized_npyData\"\n",
    "if not os.path.exists(Normalized_npyDataDir):\n",
    "    os.makedirs(Normalized_npyDataDir)\n",
    "    print(f\"normalized npy file directory is created: {Normalized_npyDataDir}\")\n",
    "\n",
    "Normalized_matDataDir = BASE_DIR/ \"data\" / \"normalized_matData\"\n",
    "if not os.path.exists(Normalized_matDataDir):\n",
    "    os.makedirs(Normalized_matDataDir)\n",
    "    print(f\"normalized npy file directory is created: {Normalized_matDataDir}\")\n",
    "\n",
    "\n",
    "npy_files = glob.glob(os.path.join(data_folder, \"*.npy\"))  # List all .npy files\n",
    "# Dictionary to store the normalized datasets\n",
    "\n",
    "normalized_data = {}\n",
    "data_ranges = {}\n",
    "\n",
    "# Load and normalize each dataset\n",
    "for file in npy_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    base_name = os.path.splitext(file_name)[0]  # Remove .npy extension\n",
    "\n",
    "    data, min_val, max_val = mnp.load_and_normalize_npy(file)  # data --> normalized data return by above function.\n",
    "    normalized_data[file_name] = data  # Store in dictionary\n",
    "    data_ranges[file_name] = (min_val, max_val)  # Store original data range\n",
    "    print(f\"Loaded and normalized {file_name} - Min: {min_val}, Max: {max_val}\")\n",
    "\n",
    "# <--------------   Save normalized data as .mat (MATLAB format) -----------> \n",
    "    mat_save_path = os.path.join(Normalized_matDataDir, f\"{base_name}_normalized.mat\")\n",
    "    sio.savemat(mat_save_path, {base_name: data})\n",
    "    print(f\" Saved file :{base_name}_normalized.mat\") #mat_save_path}\")\n",
    "    \n",
    " # <--------------  Save normalized data as .npy numpy array.-----------> \n",
    "    save_path = os.path.join(Normalized_npyDataDir,  f\"{base_name}_normalized.npy\")  # Keep same filename\n",
    "    np.save(save_path, data)\n",
    "    print(f\"Saved as npy file: {base_name}_normalized.npy | Min_val: {min_val:.4f} | Max_val: {max_val:.4f}\")\n",
    "\n",
    "data_dict_npy = normalized_data  # All different normalized npy array data is stored in the normalized data \n",
    "AllCominedData = BASE_DIR/ \"data\" / \"combined_Data\"  \n",
    "if not os.path.exists(AllCominedData):\n",
    "    os.makedirs(AllCominedData)\n",
    "    print(f\"all normalized mat file data combined and saved in this directory : {AllCominedData}\") \n",
    "#  <--------------  Save all normalized data in one dictionary and saved as.mat file --------------> \n",
    "mat_path = os.path.join(AllCominedData, \"all_normalizeddata.mat\")\n",
    "sio.savemat(mat_path, data_dict_npy)\n",
    "print(f\"Dictionary saved as MATLAB .mat: {mat_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e895fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  <----------- To importthe modules from src/modules/ write these lines  ----------->\n",
    "# import os\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# from path_manager_JupyterCopy import AddPath\n",
    "# AddPath()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa868b2a",
   "metadata": {},
   "source": [
    "###  below The code plot the histogram of normalize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ee0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# module_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "# if not os.path.exists(module_path):\n",
    "#     print(f\" i am looking for the Gaetano sys path\")\n",
    "#     module_path = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "    \n",
    "# # Add the module path to sys.path if it's nat already there\n",
    "# print(f\"module path: {module_path}\")\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "\n",
    "# # BASE_DIR = Path.cwd().parent\n",
    "# # SRC_DIR = Path.cwd().parent/\"src\"\n",
    "# # if str(SRC_DIR) not in sys.path:\n",
    "# #     sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# # this is just for adding src/ path here so that I can call another script \n",
    "# #  src/path_manager.py which will add module path in my system path to implement here in jupyter. for calling differents modules.\n",
    "# # from addpathsrc_addpathscript import addpath \n",
    "# # addpath()\n",
    "# from path_manager_JupyterCopy import AddPath\n",
    "# AddPath()\n",
    "\n",
    "\n",
    "# from histogramplot import plot_normalizedata_hist\n",
    "# # from plot3dint import plot3dinteractive\n",
    "\n",
    "# for keyval in normalized_data:\n",
    "#     datakey = keyval\n",
    "#     print(f\"\\n data key :{datakey}\")\n",
    "#     dataval = normalized_data[datakey]\n",
    "#     voldata=dataval\n",
    "#     keyvalue = datakey\n",
    "#     # plot3dinteractive(voldata,keyvalue)\n",
    "#     plot_normalizedata_hist(dataval,datakey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a6eb1",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction & Quantile-Based Thresholding\n",
    "- Now that your 3D datasets are normalized, we will proceed with Feature Extraction & Quantile-Based Thresholding to identify meaningful substructures.\n",
    "###  Why This Step is Important?\n",
    "- Feature Extraction helps in understanding the distribution of voxel intensities.\n",
    "- Quantile-Based Thresholding helps to filter noise and identify significant regions in the dataset.\n",
    "\n",
    "## - What I Will Do?\n",
    "\n",
    "-  Step 1: Extract statistical features (mean, variance, quantiles)\n",
    "-  Step 2: Apply Quantile-Based Thresholding (0.95, 0.99 quantiles)\n",
    "-  Step 3: Visualize the thresholded regions in 3D slices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = Path.cwd().parent\n",
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # type: ignore\n",
    "\n",
    "from feature_thresholding import FeatureQuantileThresholding \n",
    "from listspecificfiles import readlistFiles  \n",
    "# path to data directory make it system independent always! don't hardcore it.\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "data_dir = PROJECT_DIR/\"data\"/\"normalized_npyData\"\n",
    "save_dir = PROJECT_DIR/\"results\"/\"featureQuantileThres\"\n",
    "\n",
    "# data_dir = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\normalized_npyData\"\n",
    "# save_dir = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\featureQuantileThres\"dynamic\n",
    "\n",
    "cwd = Path.cwd().parent.parent\n",
    "BASE_DIR = cwd/ \"results\"\n",
    "\n",
    "fq = FeatureQuantileThresholding(data_dir, save_dir, BASE_DIR=BASE_DIR)\n",
    "fq.process(visualize=True, save_features=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977feef",
   "metadata": {},
   "source": [
    "## filtered applied:  anisotropic_diffusion_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d024d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from listspecificfiles import readlistFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import argparse\n",
    "\n",
    "def anisotropic_diffusion_3d(volume, niter=10, kappa=30, gamma=0.1, step=(1.,1.,1.), option=1):\n",
    "    \"\"\"\n",
    "    Apply 3D anisotropic diffusion (Perona-Malik filtering) to a volume.\n",
    "    \"\"\"\n",
    "    # vol = volume.astype(np.float64)\n",
    "    vol = np.array(volume, dtype=np.float64)  # Use float64 for high precision\n",
    "\n",
    "\n",
    "    vol_out = vol.copy()\n",
    "    # Initialize gradient difference arrays\n",
    "    deltaD = np.zeros_like(vol_out)\n",
    "    deltaS = np.zeros_like(vol_out)\n",
    "    deltaE = np.zeros_like(vol_out)\n",
    "    # Initialize flux arrays\n",
    "    NS = np.zeros_like(vol_out)\n",
    "    EW = np.zeros_like(vol_out)\n",
    "    UD = np.zeros_like(vol_out)\n",
    "    for _ in range(niter):\n",
    "        # Compute differences along each axis\n",
    "        deltaD[:-1, :, :] = np.diff(vol_out, axis=0);   deltaD[-1, :, :] = 0\n",
    "        deltaS[:, :-1, :] = np.diff(vol_out, axis=1);   deltaS[:, -1, :] = 0\n",
    "        deltaE[:, :, :-1] = np.diff(vol_out, axis=2);   deltaE[:, :, -1] = 0\n",
    "        # Compute conductance (edge stopping) coefficients\n",
    "        if option == 1:\n",
    "            cD = np.exp(-(deltaD / kappa)**2) / step[0]\n",
    "            cS = np.exp(-(deltaS / kappa)**2) / step[1]\n",
    "            cE = np.exp(-(deltaE / kappa)**2) / step[2]\n",
    "        else:  # option == 2\n",
    "            cD = 1.0 / (1.0 + (deltaD / kappa)**2) / step[0]\n",
    "            cS = 1.0 / (1.0 + (deltaS / kappa)**2) / step[1]\n",
    "            cE = 1.0 / (1.0 + (deltaE / kappa)**2) / step[2]\n",
    "        # Flux in each direction\n",
    "        D = cD * deltaD\n",
    "        S = cS * deltaS\n",
    "        E = cE * deltaE\n",
    "        # Net flux (divergence)\n",
    "        UD[:] = D;   UD[1:, :, :] -= D[:-1, :, :]\n",
    "        NS[:] = S;   NS[:, 1:, :] -= S[:, :-1, :]\n",
    "        EW[:] = E;   EW[:, :, 1:] -= E[:, :, :-1]\n",
    "        # Update volume\n",
    "        vol_out += gamma * (UD + NS + EW)\n",
    "    return vol_out\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"3D tomographic data processing with anisotropic diffusion and quantile thresholding\")\n",
    "#     parser.add_argument(\"input_file\", help=\"Path to input 3D .npy file\")\n",
    "#     parser.add_argument(\"--iterations\", \"-n\", type=int, default=10, help=\"Number of diffusion iterations\")\n",
    "#     parser.add_argument(\"--kappa\", \"-k\", type=float, default=30.0, help=\"Conduction coefficient for anisotropic diffusion\")\n",
    "#     parser.add_argument(\"--gamma\", \"-g\", type=float, default=0.1, help=\"Diffusion speed (time step)\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # Load data\n",
    "#     data = np.load(args.input_file)\n",
    "#     print(f\"Loaded volume of shape {data.shape}, dtype {data.dtype}\")\n",
    "#     print(f\"Mean={data.mean():.4f}, Std={data.std():.4f}, Min={data.min():.4f}, Max={data.max():.4f}\")\n",
    "#     q95 = np.quantile(data, 0.95);  q99 = np.quantile(data, 0.99)\n",
    "#     print(f\"95th percentile={q95:.4f}, 99th percentile={q99:.4f}\")\n",
    "\n",
    "#     # Anisotropic diffusion filtering\n",
    "#     print(f\"Applying anisotropic diffusion: niter={args.iterations}, kappa={args.kappa}, gamma={args.gamma}\")\n",
    "#     filtered = anisotropic_diffusion_3d(data, niter=args.iterations, kappa=args.kappa, gamma=args.gamma, option=1)\n",
    "#     # Compute thresholds on filtered data\n",
    "#     thr95 = np.quantile(filtered, 0.95)\n",
    "#     thr99 = np.quantile(filtered, 0.99)\n",
    "#     print(f\"Thresholding at 95% = {thr95:.4f}, 99% = {thr99:.4f}\")\n",
    "\n",
    "#     # Prepare volume for visualization (clip values below 95th percentile)\n",
    "#     vol_display = filtered.copy()\n",
    "#     vol_display[vol_display < thr95] = thr95\n",
    "\n",
    "#     # Create Plotly volume plot\n",
    "#     fig = go.Figure(data=go.Volume(\n",
    "#         x=np.arange(filtered.shape[2]),\n",
    "#         y=np.arange(filtered.shape[1]),\n",
    "#         z=np.arange(filtered.shape[0]),\n",
    "#         value=vol_display,\n",
    "#         isomin=thr95,\n",
    "#         isomax=float(filtered.max()),\n",
    "#         opacity=0.1,\n",
    "#         surface_count=3,\n",
    "#         colorscale=\"Viridis\",\n",
    "#         caps=dict(x_show=False, y_show=False, z_show=False)\n",
    "#     ))\n",
    "#     fig.update_layout(scene=dict(aspectmode='data'),\n",
    "#                       title=\"3D Volume rendering (thresholded at 95th percentile)\")\n",
    "#     # Show figure (in a script, this will open a browser window)\n",
    "#     fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def anisotropic_diffusion_3d(volume, niter=10, kappa=30, gamma=0.1, step=(1., 1., 1.), option=1):\n",
    "\n",
    "#     vol = np.array(volume, dtype=np.float64)  # Use float64 for high precision\n",
    "    \n",
    "#     vol_out = vol.copy()\n",
    "\n",
    "#     deltaD = np.zeros_like(vol_out)\n",
    "#     deltaS = np.zeros_like(vol_out)\n",
    "#     deltaE = np.zeros_like(vol_out)\n",
    "\n",
    "#     NS = np.zeros_like(vol_out)\n",
    "#     EW = np.zeros_like(vol_out)\n",
    "#     UD = np.zeros_like(vol_out)\n",
    "\n",
    "#     for _ in range(niter):\n",
    "#         deltaD[:-1, :, :] = np.diff(vol_out, axis=0);   deltaD[-1, :, :] = 0\n",
    "#         deltaS[:, :-1, :] = np.diff(vol_out, axis=1);   deltaS[:, -1, :] = 0\n",
    "#         deltaE[:, :, :-1] = np.diff(vol_out, axis=2);   deltaE[:, :, -1] = 0\n",
    "\n",
    "#         if option == 1:\n",
    "#             cD = np.exp(-(deltaD / kappa) ** 2) / step[0]\n",
    "#             cS = np.exp(-(deltaS / kappa) ** 2) / step[1]\n",
    "#             cE = np.exp(-(deltaE / kappa) ** 2) / step[2]\n",
    "\n",
    "#         else:\n",
    "\n",
    "#             cD = 1.0 / (1.0 + (deltaD / kappa) ** 2) / step[0]\n",
    "#             cS = 1.0 / (1.0 + (deltaS / kappa) ** 2) / step[1]\n",
    "#             cE = 1.0 / (1.0 + (deltaE / kappa) ** 2) / step[2]\n",
    "\n",
    "#         D = cD * deltaD\n",
    "#         S = cS * deltaS\n",
    "#         E = cE * deltaE\n",
    "\n",
    "#         UD[:] = D;  UD[1:, :, :] -= D[:-1, :, :]\n",
    "#         NS[:] = S;  NS[:, 1:, :] -= S[:, :-1, :]\n",
    "#         EW[:] = E;  EW[:, :, 1:] -= E[:, :, :-1]\n",
    "\n",
    "#         vol_out += gamma * (UD + NS + EW)\n",
    "\n",
    "#     return vol_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbe78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot 3D volume with improved visibility\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_anisotropic_diffusion_3d(display_data, opacity, surface_count, isomin=None):\n",
    "\n",
    "    if isomin is None:\n",
    "        isomin = np.quantile(display_data, 0.90)  # Default threshold if not specified\n",
    "\n",
    "    fig = go.Figure(data=go.Volume(\n",
    "        x=np.arange(display_data.shape[2]),\n",
    "        y=np.arange(display_data.shape[1]),\n",
    "        z=np.arange(display_data.shape[0]),\n",
    "        value=display_data,\n",
    "        isomin=isomin,\n",
    "        isomax=display_data.max(),\n",
    "        opacity=opacity,\n",
    "        surface_count=surface_count,\n",
    "        colorscale=\"Inferno\",  # Use reversed colormap for better contrast\n",
    "        caps=dict(x_show=False, y_show=False, z_show=False)\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(aspectmode='data'),\n",
    "        title=\"Refined 3D Volume Rendering (Post-Filtering)\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34030461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414621e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing block\n",
    "import os\n",
    "import numpy as np\n",
    "from listspecificfiles import readlistFiles  # Custom module to list files\n",
    "\n",
    "# Set the directory and file pattern\n",
    "\n",
    "keyword = '.npy'\n",
    "dataset_names = readlistFiles(data_dir, keyword).matched_Files\n",
    "print(dataset_names)\n",
    "# Process each file (here we only process the first one as a test)\n",
    "count = 0\n",
    "for file_name in dataset_names:\n",
    "    count += 1\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    print(f\"Processing: {file_path}\")\n",
    "\n",
    "    # Load normalized .npy volume\n",
    "    data = np.load(file_path)\n",
    "\n",
    "    # Step 1: Apply anisotropic diffusion filtering\n",
    "    filtered = anisotropic_diffusion_3d(data, niter=30, kappa=5, gamma=0.1, option=1)\n",
    "\n",
    "    # Step 2: Clip for visualization contrast\n",
    "    clip_value = np.quantile(filtered, 0.90)\n",
    "    filtered_clipped = np.clip(filtered, clip_value, filtered.max())\n",
    "\n",
    "    # Step 3: Plot 3D volume\n",
    "    opacity = 0.2\n",
    "    surface_count = 5\n",
    "    plot_anisotropic_diffusion_3d(filtered_clipped, opacity, surface_count)\n",
    "\n",
    "    if count == 1:\n",
    "        break  # Remove this line if you want to process all files\n",
    "\n",
    "\n",
    "# data_dir = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\raw_npyData\"\n",
    "# keyword = '.npy'\n",
    "# dataset_names = readlistFiles(data_dir, keyword).matched_Files\n",
    "\n",
    "# count = 0\n",
    "# for file_name in dataset_names:\n",
    "# count += 1\n",
    "# file_path = os.path.join(data_dir, file_name)\n",
    "# print(f\"Processing: {file_path}\")\n",
    "# data = np.load(file_path)\n",
    "\n",
    "# # Step 1: Apply anisotropic diffusion filtering\n",
    "# filtered = anisotropic_diffusion_3d(data, niter=30, kappa=5, gamma=0.1, option=1)\n",
    "\n",
    "# # Step 2: Clip for visualization\n",
    "# clip_value = np.quantile(filtered, 0.90)\n",
    "# filtered_clipped = np.clip(filtered, clip_value, filtered.max())\n",
    "\n",
    "# # Step 3: Plot 3D volume\n",
    "# opacity = 0.2\n",
    "# surface_count = 5\n",
    "# plot_anisotropic_diffusion_3d(filtered_clipped, opacity, surface_count)\n",
    "\n",
    "# if count == 1:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc919b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtered Data Stats:\")\n",
    "print(f\"Min: {filtered.min()}, Max: {filtered.max()}\")\n",
    "print(f\"Q50: {np.quantile(filtered, 0.50)}\")\n",
    "print(f\"Q75: {np.quantile(filtered, 0.75)}\")\n",
    "print(f\"Q90: {np.quantile(filtered, 0.90)}\")\n",
    "print(f\"Q95: {np.quantile(filtered, 0.95)}\")\n",
    "print(f\"Q99: {np.quantile(filtered, 0.99)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, 5, 8, 10, 12, 13, 15, 18, 100])  # 100 is an upper outlier, 1 is a lower one\n",
    "\n",
    "\n",
    "lower_bound = np.percentile(data, 5)   # 5th percentile\n",
    "upper_bound = np.percentile(data, 95)  # 95th percentile\n",
    "\n",
    "# Filter the data to keep only central 90%\n",
    "filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "\n",
    "print(f\"Original Data:, {data} now lower: {lower_bound} and upper :{upper_bound} \\n\")\n",
    "print(\"Filtered Data (5%‚Äì95%):\", filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# parent_dir = Path.cwd().parent\n",
    "# child_dir = Path.cwd().parent/ \"src\"  # here we moved to the child directory.\n",
    "# Base_dir = Path.cwd()\n",
    "# print(f\"parent_dir: {parent_dir} \\ncurrent directory: {Base_dir} \\nchild directory:{child_dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d8c39-21c0-4fdb-b61a-3ba49be176eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# import gc  # Garbage collector to free memory\n",
    "\n",
    "# def plot3dinteractive(voldata, keyvalue, output_dir, sample_fraction=0.005):\n",
    "#     \"\"\"Plots large 3D NumPy arrays interactively and saves as HTML.\n",
    "    \n",
    "#     - `voldata`: Input 3D NumPy array.\n",
    "#     - `keyvalue`: Filename for saving.\n",
    "#     - `output_dir`: Directory to save HTML plots.\n",
    "#     - `sample_fraction`: Fraction of points to randomly plot.\n",
    "#     \"\"\"\n",
    "#     array_3d = voldata\n",
    "#     x1, y1, z1 = array_3d.shape\n",
    "#     print(f\"Shape of {keyvalue}: {x1, y1, z1}\")\n",
    "\n",
    "#     # Create a 3D meshgrid\n",
    "#     x, y, z = np.meshgrid(np.arange(x1), np.arange(y1), np.arange(z1))\n",
    "\n",
    "#     # Mask non-zero values\n",
    "#     mask = array_3d > 0\n",
    "#     x_vals = x[mask].flatten()\n",
    "#     y_vals = y[mask].flatten()\n",
    "#     z_vals = z[mask].flatten()\n",
    "#     values = array_3d[mask].flatten()\n",
    "\n",
    "#     # **Randomly sample points** to reduce memory usage\n",
    "#     num_points = len(values)\n",
    "#     sample_size = int(num_points * sample_fraction)\n",
    "\n",
    "#     if sample_size > 0:\n",
    "#         indices = np.random.choice(num_points, sample_size, replace=False)\n",
    "#         x_vals = x_vals[indices]\n",
    "#         y_vals = y_vals[indices]\n",
    "#         z_vals = z_vals[indices]\n",
    "#         values = values[indices]\n",
    "#     else:\n",
    "#         print(f\"‚ö† Warning: Not enough non-zero points for {keyvalue}. Skipping...\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Plotting {sample_size} points out of {num_points} ({sample_fraction * 100}% sampled)\")\n",
    "\n",
    "#     # **Enhanced Color Grading**\n",
    "#     colorscale = [\n",
    "#         [0.0, \"white\"],    # Outer structure (light color)\n",
    "#         [0.2, \"lightblue\"],\n",
    "#         [0.4, \"deepskyblue\"],\n",
    "#         [0.6, \"dodgerblue\"],\n",
    "#         [0.8, \"blue\"],      # Middle layers\n",
    "#         [1.0, \"darkblue\"]   # Deep inner structure (dark color)\n",
    "#     ]\n",
    "\n",
    "#     # Create a 3D scatter plot\n",
    "#     fig = go.Figure(data=go.Scatter3d(\n",
    "#         x=x_vals,\n",
    "#         y=y_vals,\n",
    "#         z=z_vals,\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=2,\n",
    "#             color=values,\n",
    "#             colorscale=colorscale,\n",
    "#             opacity=0.5\n",
    "#         )\n",
    "#     ))\n",
    "\n",
    "#     # Set axis labels and layout\n",
    "#     fig.update_layout(\n",
    "#         title=f\"3D Structure: {keyvalue}\",\n",
    "#         scene=dict(\n",
    "#             xaxis_title='X',\n",
    "#             yaxis_title='Y',\n",
    "#             zaxis_title='Z',\n",
    "#             bgcolor=\"black\"  # Dark background for better contrast\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # **Save plot as an interactive HTML file**\n",
    "#     save_path = os.path.join(output_dir, f\"{keyvalue}.html\")\n",
    "#     fig.write_html(save_path)\n",
    "#     print(f\"‚úÖ Saved: {save_path}\")\n",
    "\n",
    "#     # **Clear memory**\n",
    "#     del fig, x_vals, y_vals, z_vals, values\n",
    "#     gc.collect()  # Garbage collection to free memory\n",
    "\n",
    "# # **Directory paths**\n",
    "# data_dir = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\intermdata1\"\n",
    "# output_dir = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\"\n",
    "\n",
    "# # **Ensure output directory exists**\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # **Process all .npy files one by one**\n",
    "# npy_files = [f for f in os.listdir(data_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "# for filename in npy_files:\n",
    "#     file_path = os.path.join(data_dir, filename)\n",
    "#     voldata = np.load(file_path)  # Load .npy file\n",
    "#     plot3dinteractive(voldata, filename, output_dir, sample_fraction=0.05)  # Save & clear memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3890f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ae51bc",
   "metadata": {},
   "source": [
    "# db scan implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58092b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using module path: c:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\n",
      "üîÑ Module path added to sys.path: c:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\n",
      "üîç Found 8 .npy files in folder: c:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw_npyData\n",
      "\n",
      "üöÄ Processing file: AML2_cell11.npy\n",
      "‚úÖ Finished AML2_cell11.npy - Clusters: 1\n",
      "\n",
      "üöÄ Processing file: AML3_cell16.npy\n",
      "‚úÖ Finished AML3_cell16.npy - Clusters: 1\n",
      "\n",
      "üöÄ Processing file: Tomogramma_BuddingYeastCell.npy\n",
      "‚úÖ Finished Tomogramma_BuddingYeastCell.npy - Clusters: 1\n",
      "\n",
      "üöÄ Processing file: Tomogramma_Cell1.npy\n",
      "‚úÖ Finished Tomogramma_Cell1.npy - Clusters: 1\n",
      "\n",
      "üöÄ Processing file: Tomogramma_Cell2.npy\n",
      "‚úÖ Finished Tomogramma_Cell2.npy - Clusters: 1\n",
      "\n",
      "üöÄ Processing file: Tomogramma_Cell3.npy\n",
      "‚úÖ Finished Tomogramma_Cell3.npy - Clusters: 1\n",
      "\n",
      "üöÄ Processing file: tomo_Grafene_24h.npy\n",
      "‚úÖ Finished tomo_Grafene_24h.npy - Clusters: 1\n",
      "\n",
      "üöÄ Processing file: tomo_grafene_48h.npy\n",
      "‚úÖ Finished tomo_grafene_48h.npy - Clusters: 1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "import os\n",
    "from path_manager_JupyterCopy import AddPath\n",
    "AddPath()\n",
    "\n",
    "from cluster3Dprocess import Cluster3DProcessor\n",
    "\n",
    "\n",
    "def batch_process_npy_folder(input_dir,Baseoutput_dir, method='dbscan', save_outputs=True,\n",
    "                              downsample=True, max_points=100000,\n",
    "                              save_subvolumes=True, visualize_interactive=True):\n",
    "    \n",
    "    npy_files = glob.glob(os.path.join(input_dir, '*.npy'))\n",
    "\n",
    "    print(f\"üîç Found {len(npy_files)} .npy files in folder: {input_dir}\")\n",
    "\n",
    "    for npy_file in npy_files:\n",
    "        print(f\"\\nüöÄ Processing file: {os.path.basename(npy_file)}\")\n",
    "        try:\n",
    "            processor = Cluster3DProcessor(\n",
    "                npy_file=npy_file,\n",
    "                Baseoutput_dir = Baseoutput_dir,\n",
    "                method=method,\n",
    "                save_outputs=save_outputs,\n",
    "                downsample=downsample,\n",
    "                max_points=max_points,\n",
    "                save_subvolumes=save_subvolumes,\n",
    "                visualize_interactive=visualize_interactive\n",
    "            )\n",
    "            coords, labels = processor.run()\n",
    "            print(f\"‚úÖ Finished {os.path.basename(npy_file)} - Clusters: {len(set(labels)) - (1 if -1 in labels else 0)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {os.path.basename(npy_file)}: {e}\")\n",
    "            \n",
    "\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "input_dir = PROJECT_DIR/\"data\"/\"raw_npyData\"\n",
    "Baseoutput_dir = PROJECT_DIR/\"results\"\n",
    "# Example usage:\n",
    "batch_process_npy_folder(\n",
    "    input_dir= input_dir,\n",
    "    Baseoutput_dir= Baseoutput_dir,\n",
    "    method='dbscan',\n",
    "    save_outputs=True,\n",
    "    downsample=True,\n",
    "    max_points=100000,\n",
    "    save_subvolumes=True,\n",
    "    visualize_interactive=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d1e48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
