{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/path_manager.py  # This runs the whole file once, so if AddPath() is called at the bottom of path_manager.py, \n",
    "# it will add your module path automatically.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c37fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# Normalized_data = Path.cwd().parent/\"data\"/\"normalized_npyData\"\n",
    "# import os \n",
    "\n",
    "# print(Normalized_data)\n",
    "\n",
    "# # lisspecificfiles.py\n",
    "# def readlistFiles(filepath,keyword):\n",
    "    \n",
    "#     Files = os.listdir(filepath)\n",
    "#     print(Files)\n",
    "#     for File in Files:\n",
    "#         if File.endswith(keyword):\n",
    "#             print(File)\n",
    "        \n",
    "# keyword = 'normalized.npy'\n",
    "# # readlistFiles(Normalized_data,keyword ='normalized.npy')\n",
    "# readlistFiles(Normalized_data,keyword ='Copy.txt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# datapath =r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\normalized_npyData\" \n",
    "# datapath = os.path.normpath(datapath)\n",
    "# print(datapath)\n",
    "# # datapath = Normalized_data\n",
    "# k1 = 'Copy.txt'\n",
    "# # k1 = 'NORMALIZED.NPY'\n",
    "# # k1 = k1.lower()\n",
    "# d1 = listmatchedFiles(datapath,k1)\n",
    "# # d1.matchedFiles()\n",
    "# print(d1.matched_Files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200b2d6",
   "metadata": {},
   "source": [
    "## convert the .mat data into the .npy data using the main key value from the main (.mat)  data files and saved in \"outputdatapath\".\n",
    "\n",
    "# Data/  --> Description \n",
    "- #### raw_npyData/ is directory  --> where file saved as .npy file after converting from .mat file.\n",
    "- #### normalized_matData/ is directory  --> where matNormalized data files saved as ..normalized.mat file after normalizing .mat file.\n",
    "- #### normalized_npyData/ is directory  --> where npyNormalized data files saved as -..normalized.npy file after normalizing .npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the auxfunction \n",
    "import sys\n",
    "import os\n",
    "# # Define the module path\n",
    "# module_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "# if not os.path.exists(module_path):\n",
    "#     module_path = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "\n",
    "# # Add the module path to sys.path if it's not already there\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)\n",
    "    \n",
    "import createmat2npy as mnpy   #  this module load the .mat file,extract data according to the key and convert them into .npy file.\n",
    "datapath = r'C:\\Users\\mrafik\\OneDrive - C.N.R. STIIMA\\tomogram all data\\all_tomogram_data'\n",
    "outputdatapath = r'E:/Projects/substructure_3d_data/Substructure_Different_DataTypes/data/raw_npyData/'\n",
    "outputdatapath = os.path.normpath(outputdatapath) \n",
    "if not os.path.exists(datapath and outputdatapath):\n",
    "    datapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\SharedContents\\OneDrive - C.N.R. STIIMA\\tomogram all data\\all_tomogram_data\"\n",
    "    outputdatapath = r'C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw_npyData'\n",
    "\n",
    "mnpy.mat2npy(datapath,outputdatapath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27129e6c-ce48-4fe6-8aae-adcbf52a8fee",
   "metadata": {},
   "source": [
    "## A function is defined to load and normalize 3D Numpy data: defined in the file createmat2npy.py file with name: load_and_normalize_npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152098e-4d5d-49f0-b4bd-e8c65715c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python Script for Loading & Normalization \n",
    "# sys.path.append(module_path)\n",
    "# import createmat2npy as mnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033a197-a0f2-4ff5-9e6e-cd617c009055",
   "metadata": {},
   "source": [
    "### all different normalized npy array data is stored in the normalized data and Dictionary saved as MATLAB .mat with name as 'all_normalizeddata.mat'\n",
    "\n",
    "- And seperatley saved the each npy file as  \"..._normalized.mat\" format also.(3d matrix format data) and also in               \"...__normalized.npy file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing .npy files (update this with your folder path)\n",
    "import scipy.io as sio # type: ignore\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import numpy as np # type: ignore\n",
    "import createmat2npy as mnp\n",
    "import os\n",
    "\n",
    "data_folder = outputdatapath # raw_npyData/ \n",
    "\n",
    "BASE_DIR = Path.cwd().parent # where all code is there.\n",
    "\n",
    "Normalized_npyDataDir = BASE_DIR/ \"data\" / \"normalized_npyData\"\n",
    "if not os.path.exists(Normalized_npyDataDir):\n",
    "    os.makedirs(Normalized_npyDataDir)\n",
    "    print(f\"normalized npy file directory is created: {Normalized_npyDataDir}\")\n",
    "\n",
    "Normalized_matDataDir = BASE_DIR/ \"data\" / \"normalized_matData\"\n",
    "if not os.path.exists(Normalized_matDataDir):\n",
    "    os.makedirs(Normalized_matDataDir)\n",
    "    print(f\"normalized npy file directory is created: {Normalized_matDataDir}\")\n",
    "\n",
    "\n",
    "npy_files = glob.glob(os.path.join(data_folder, \"*.npy\"))  # List all .npy files\n",
    "# Dictionary to store the normalized datasets\n",
    "\n",
    "normalized_data = {}\n",
    "data_ranges = {}\n",
    "\n",
    "# Load and normalize each dataset\n",
    "for file in npy_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    base_name = os.path.splitext(file_name)[0]  # Remove .npy extension\n",
    "\n",
    "    data, min_val, max_val = mnp.load_and_normalize_npy(file)  # data --> normalized data return by above function.\n",
    "    normalized_data[file_name] = data  # Store in dictionary\n",
    "    data_ranges[file_name] = (min_val, max_val)  # Store original data range\n",
    "    print(f\"Loaded and normalized {file_name} - Min: {min_val}, Max: {max_val}\")\n",
    "\n",
    "# <--------------   Save normalized data as .mat (MATLAB format) -----------> \n",
    "    mat_save_path = os.path.join(Normalized_matDataDir, f\"{base_name}_normalized.mat\")\n",
    "    sio.savemat(mat_save_path, {base_name: data})\n",
    "    print(f\" Saved file :{base_name}_normalized.mat\") #mat_save_path}\")\n",
    "    \n",
    " # <--------------  Save normalized data as .npy numpy array.-----------> \n",
    "    save_path = os.path.join(Normalized_npyDataDir,  f\"{base_name}_normalized.npy\")  # Keep same filename\n",
    "    np.save(save_path, data)\n",
    "    print(f\"Saved as npy file: {base_name}_normalized.npy | Min_val: {min_val:.4f} | Max_val: {max_val:.4f}\")\n",
    "\n",
    "data_dict_npy = normalized_data  # All different normalized npy array data is stored in the normalized data \n",
    "AllCominedData = BASE_DIR/ \"data\" / \"combined_Data\"  \n",
    "if not os.path.exists(AllCominedData):\n",
    "    os.makedirs(AllCominedData)\n",
    "    print(f\"all normalized mat file data combined and saved in this directory : {AllCominedData}\") \n",
    "#  <--------------  Save all normalized data in one dictionary and saved as.mat file --------------> \n",
    "mat_path = os.path.join(AllCominedData, \"all_normalizeddata.mat\")\n",
    "sio.savemat(mat_path, data_dict_npy)\n",
    "print(f\"Dictionary saved as MATLAB .mat: {mat_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e895fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  <----------- To importthe modules from src/modules/ write these lines  ----------->\n",
    "# import os\n",
    "# import sys\n",
    "# from pathlib import Path\n",
    "# from path_manager_JupyterCopy import AddPath\n",
    "# AddPath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <----------------- Below The code plot the histogram of RAW data ------------------------>\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# BASE_DIR = Path.cwd().parent\n",
    "# SRC_DIR = Path.cwd().parent/\"src\"\n",
    "# if str(SRC_DIR) not in sys.path:\n",
    "#     sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# this is just for adding src/ path here so that I can call another script \n",
    "#  src/path_manager.py which will add module path in my system path to implement here in jupyter. for calling differents modules.\n",
    "# from addpathsrc_addpathscript import addpath \n",
    "# addpath()\n",
    "from path_manager_JupyterCopy import AddPath\n",
    "AddPath()\n",
    "\n",
    "\n",
    "from histogramplot import plot_normalizedata_hist\n",
    "# from plot3dint import plot3dinteractive\n",
    "\n",
    "for keyval in normalized_data:\n",
    "    datakey = keyval\n",
    "    print(f\"\\n data key :{datakey}\")\n",
    "    dataval = normalized_data[datakey]\n",
    "    voldata = dataval\n",
    "    keyvalue = datakey\n",
    "    # plot3dinteractive(voldata,keyvalue)\n",
    "    plot_normalizedata_hist(dataval,datakey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa868b2a",
   "metadata": {},
   "source": [
    "###  below The code plot the histogram of normalize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ee0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "module_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "if not os.path.exists(module_path):\n",
    "    print(f\" i am looking for the Gaetano sys path\")\n",
    "    module_path = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "    \n",
    "# Add the module path to sys.path if it's nat already there\n",
    "print(f\"module path: {module_path}\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# BASE_DIR = Path.cwd().parent\n",
    "# SRC_DIR = Path.cwd().parent/\"src\"\n",
    "# if str(SRC_DIR) not in sys.path:\n",
    "#     sys.path.append(str(SRC_DIR))\n",
    "\n",
    "# this is just for adding src/ path here so that I can call another script \n",
    "#  src/path_manager.py which will add module path in my system path to implement here in jupyter. for calling differents modules.\n",
    "# from addpathsrc_addpathscript import addpath \n",
    "# addpath()\n",
    "from path_manager_JupyterCopy import AddPath\n",
    "AddPath()\n",
    "\n",
    "\n",
    "from histogramplot import plot_normalizedata_hist\n",
    "# from plot3dint import plot3dinteractive\n",
    "\n",
    "for keyval in normalized_data:\n",
    "    datakey = keyval\n",
    "    print(f\"\\n data key :{datakey}\")\n",
    "    dataval = normalized_data[datakey]\n",
    "    voldata=dataval\n",
    "    keyvalue = datakey\n",
    "    # plot3dinteractive(voldata,keyvalue)\n",
    "    plot_normalizedata_hist(dataval,datakey,output_path = None )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a6eb1",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction & Quantile-Based Thresholding\n",
    "- Now that your 3D datasets are normalized, we will proceed with Feature Extraction & Quantile-Based Thresholding to identify meaningful substructures.\n",
    "###  Why This Step is Important?\n",
    "- Feature Extraction helps in understanding the distribution of voxel intensities.\n",
    "- Quantile-Based Thresholding helps to filter noise and identify significant regions in the dataset.\n",
    "\n",
    "## - What I Will Do?\n",
    "\n",
    "-  Step 1: Extract statistical features (mean, variance, quantiles)\n",
    "-  Step 2: Apply Quantile-Based Thresholding (0.95, 0.99 quantiles)\n",
    "-  Step 3: Visualize the thresholded regions in 3D slices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = Path.cwd().parent\n",
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa07d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # type: ignore\n",
    "\n",
    "from feature_thresholding import FeatureQuantileThresholding \n",
    "from listspecificfiles import readlistFiles  \n",
    "# path to data directory make it system independent always! don't hardcore it.\n",
    "PROJECT_DIR = Path.cwd().parent\n",
    "data_dir = PROJECT_DIR/\"data\"/\"normalized_npyData\"\n",
    "save_dir = PROJECT_DIR/\"results\"/\"featureQuantileThres\"\n",
    "\n",
    "# data_dir = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\normalized_npyData\"\n",
    "# save_dir = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\featureQuantileThres\"dynamic\n",
    "\n",
    "cwd = Path.cwd().parent.parent\n",
    "BASE_DIR = cwd/ \"results\"\n",
    "\n",
    "fq = FeatureQuantileThresholding(data_dir, save_dir, BASE_DIR=BASE_DIR)\n",
    "fq.process(visualize=True, save_features=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977feef",
   "metadata": {},
   "source": [
    "## filtered applied:  anisotropic_diffusion_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d024d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from listspecificfiles import readlistFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0b4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import argparse\n",
    "\n",
    "def anisotropic_diffusion_3d(volume, niter=10, kappa=30, gamma=0.1, step=(1.,1.,1.), option=1):\n",
    "    \"\"\"\n",
    "    Apply 3D anisotropic diffusion (Perona-Malik filtering) to a volume.\n",
    "    \"\"\"\n",
    "    # vol = volume.astype(np.float64)\n",
    "    vol = np.array(volume, dtype=np.float64)  # Use float64 for high precision\n",
    "\n",
    "\n",
    "    vol_out = vol.copy()\n",
    "    # Initialize gradient difference arrays\n",
    "    deltaD = np.zeros_like(vol_out)\n",
    "    deltaS = np.zeros_like(vol_out)\n",
    "    deltaE = np.zeros_like(vol_out)\n",
    "    # Initialize flux arrays\n",
    "    NS = np.zeros_like(vol_out)\n",
    "    EW = np.zeros_like(vol_out)\n",
    "    UD = np.zeros_like(vol_out)\n",
    "    for _ in range(niter):\n",
    "        # Compute differences along each axis\n",
    "        deltaD[:-1, :, :] = np.diff(vol_out, axis=0);   deltaD[-1, :, :] = 0\n",
    "        deltaS[:, :-1, :] = np.diff(vol_out, axis=1);   deltaS[:, -1, :] = 0\n",
    "        deltaE[:, :, :-1] = np.diff(vol_out, axis=2);   deltaE[:, :, -1] = 0\n",
    "        # Compute conductance (edge stopping) coefficients\n",
    "        if option == 1:\n",
    "            cD = np.exp(-(deltaD / kappa)**2) / step[0]\n",
    "            cS = np.exp(-(deltaS / kappa)**2) / step[1]\n",
    "            cE = np.exp(-(deltaE / kappa)**2) / step[2]\n",
    "        else:  # option == 2\n",
    "            cD = 1.0 / (1.0 + (deltaD / kappa)**2) / step[0]\n",
    "            cS = 1.0 / (1.0 + (deltaS / kappa)**2) / step[1]\n",
    "            cE = 1.0 / (1.0 + (deltaE / kappa)**2) / step[2]\n",
    "        # Flux in each direction\n",
    "        D = cD * deltaD\n",
    "        S = cS * deltaS\n",
    "        E = cE * deltaE\n",
    "        # Net flux (divergence)\n",
    "        UD[:] = D;   UD[1:, :, :] -= D[:-1, :, :]\n",
    "        NS[:] = S;   NS[:, 1:, :] -= S[:, :-1, :]\n",
    "        EW[:] = E;   EW[:, :, 1:] -= E[:, :, :-1]\n",
    "        # Update volume\n",
    "        vol_out += gamma * (UD + NS + EW)\n",
    "    return vol_out\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description=\"3D tomographic data processing with anisotropic diffusion and quantile thresholding\")\n",
    "#     parser.add_argument(\"input_file\", help=\"Path to input 3D .npy file\")\n",
    "#     parser.add_argument(\"--iterations\", \"-n\", type=int, default=10, help=\"Number of diffusion iterations\")\n",
    "#     parser.add_argument(\"--kappa\", \"-k\", type=float, default=30.0, help=\"Conduction coefficient for anisotropic diffusion\")\n",
    "#     parser.add_argument(\"--gamma\", \"-g\", type=float, default=0.1, help=\"Diffusion speed (time step)\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # Load data\n",
    "#     data = np.load(args.input_file)\n",
    "#     print(f\"Loaded volume of shape {data.shape}, dtype {data.dtype}\")\n",
    "#     print(f\"Mean={data.mean():.4f}, Std={data.std():.4f}, Min={data.min():.4f}, Max={data.max():.4f}\")\n",
    "#     q95 = np.quantile(data, 0.95);  q99 = np.quantile(data, 0.99)\n",
    "#     print(f\"95th percentile={q95:.4f}, 99th percentile={q99:.4f}\")\n",
    "\n",
    "#     # Anisotropic diffusion filtering\n",
    "#     print(f\"Applying anisotropic diffusion: niter={args.iterations}, kappa={args.kappa}, gamma={args.gamma}\")\n",
    "#     filtered = anisotropic_diffusion_3d(data, niter=args.iterations, kappa=args.kappa, gamma=args.gamma, option=1)\n",
    "#     # Compute thresholds on filtered data\n",
    "#     thr95 = np.quantile(filtered, 0.95)\n",
    "#     thr99 = np.quantile(filtered, 0.99)\n",
    "#     print(f\"Thresholding at 95% = {thr95:.4f}, 99% = {thr99:.4f}\")\n",
    "\n",
    "#     # Prepare volume for visualization (clip values below 95th percentile)\n",
    "#     vol_display = filtered.copy()\n",
    "#     vol_display[vol_display < thr95] = thr95\n",
    "\n",
    "#     # Create Plotly volume plot\n",
    "#     fig = go.Figure(data=go.Volume(\n",
    "#         x=np.arange(filtered.shape[2]),\n",
    "#         y=np.arange(filtered.shape[1]),\n",
    "#         z=np.arange(filtered.shape[0]),\n",
    "#         value=vol_display,\n",
    "#         isomin=thr95,\n",
    "#         isomax=float(filtered.max()),\n",
    "#         opacity=0.1,\n",
    "#         surface_count=3,\n",
    "#         colorscale=\"Viridis\",\n",
    "#         caps=dict(x_show=False, y_show=False, z_show=False)\n",
    "#     ))\n",
    "#     fig.update_layout(scene=dict(aspectmode='data'),\n",
    "#                       title=\"3D Volume rendering (thresholded at 95th percentile)\")\n",
    "#     # Show figure (in a script, this will open a browser window)\n",
    "#     fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f9ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcbe78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot 3D volume with improved visibility\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_anisotropic_diffusion_3d(display_data, opacity, surface_count, isomin=None):\n",
    "\n",
    "    if isomin is None:\n",
    "        isomin = np.quantile(display_data, 0.90)  # Default threshold if not specified\n",
    "\n",
    "    fig = go.Figure(data=go.Volume(\n",
    "        x=np.arange(display_data.shape[2]),\n",
    "        y=np.arange(display_data.shape[1]),\n",
    "        z=np.arange(display_data.shape[0]),\n",
    "        value=display_data,\n",
    "        isomin=isomin,\n",
    "        isomax=display_data.max(),\n",
    "        opacity=opacity,\n",
    "        surface_count=surface_count,\n",
    "        colorscale=\"Inferno\",  # Use reversed colormap for better contrast\n",
    "        caps=dict(x_show=False, y_show=False, z_show=False)\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(aspectmode='data'),\n",
    "        title=\"Refined 3D Volume Rendering (Post-Filtering)\"\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34030461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414621e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing block\n",
    "import os\n",
    "import numpy as np\n",
    "from listspecificfiles import readlistFiles  # Custom module to list files\n",
    "\n",
    "# Set the directory and file pattern\n",
    "\n",
    "keyword = '.npy'\n",
    "dataset_names = readlistFiles(data_dir, keyword).matched_Files\n",
    "print(dataset_names)\n",
    "# Process each file (here we only process the first one as a test)\n",
    "count = 0\n",
    "for file_name in dataset_names:\n",
    "    count += 1\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    print(f\"Processing: {file_path}\")\n",
    "\n",
    "    # Load normalized .npy volume\n",
    "    data = np.load(file_path)\n",
    "\n",
    "    # Step 1: Apply anisotropic diffusion filtering\n",
    "    filtered = anisotropic_diffusion_3d(data, niter=30, kappa=5, gamma=0.1, option=1)\n",
    "\n",
    "    # Step 2: Clip for visualization contrast\n",
    "    clip_value = np.quantile(filtered, 0.90)\n",
    "    filtered_clipped = np.clip(filtered, clip_value, filtered.max())\n",
    "\n",
    "    # Step 3: Plot 3D volume\n",
    "    opacity = 0.2\n",
    "    surface_count = 5\n",
    "    plot_anisotropic_diffusion_3d(filtered_clipped, opacity, surface_count)\n",
    "\n",
    "    if count == 1:\n",
    "        break  # Remove this line if you want to process all files\n",
    "\n",
    "\n",
    "# data_dir = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\raw_npyData\"\n",
    "# keyword = '.npy'\n",
    "# dataset_names = readlistFiles(data_dir, keyword).matched_Files\n",
    "\n",
    "# count = 0\n",
    "# for file_name in dataset_names:\n",
    "# count += 1\n",
    "# file_path = os.path.join(data_dir, file_name)\n",
    "# print(f\"Processing: {file_path}\")\n",
    "# data = np.load(file_path)\n",
    "\n",
    "# # Step 1: Apply anisotropic diffusion filtering\n",
    "# filtered = anisotropic_diffusion_3d(data, niter=30, kappa=5, gamma=0.1, option=1)\n",
    "\n",
    "# # Step 2: Clip for visualization\n",
    "# clip_value = np.quantile(filtered, 0.90)\n",
    "# filtered_clipped = np.clip(filtered, clip_value, filtered.max())\n",
    "\n",
    "# # Step 3: Plot 3D volume\n",
    "# opacity = 0.2\n",
    "# surface_count = 5\n",
    "# plot_anisotropic_diffusion_3d(filtered_clipped, opacity, surface_count)\n",
    "\n",
    "# if count == 1:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc919b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtered Data Stats:\")\n",
    "print(f\"Min: {filtered.min()}, Max: {filtered.max()}\")\n",
    "print(f\"Q50: {np.quantile(filtered, 0.50)}\")\n",
    "print(f\"Q75: {np.quantile(filtered, 0.75)}\")\n",
    "print(f\"Q90: {np.quantile(filtered, 0.90)}\")\n",
    "print(f\"Q95: {np.quantile(filtered, 0.95)}\")\n",
    "print(f\"Q99: {np.quantile(filtered, 0.99)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928cdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, 5, 8, 10, 12, 13, 15, 18, 100])  # 100 is an upper outlier, 1 is a lower one\n",
    "\n",
    "\n",
    "lower_bound = np.percentile(data, 5)   # 5th percentile\n",
    "upper_bound = np.percentile(data, 95)  # 95th percentile\n",
    "\n",
    "# Filter the data to keep only central 90%\n",
    "filtered_data = data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "\n",
    "print(f\"Original Data:, {data} now lower: {lower_bound} and upper :{upper_bound} \\n\")\n",
    "print(\"Filtered Data (5%–95%):\", filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# parent_dir = Path.cwd().parent\n",
    "# child_dir = Path.cwd().parent/ \"src\"  # here we moved to the child directory.\n",
    "# Base_dir = Path.cwd()\n",
    "# print(f\"parent_dir: {parent_dir} \\ncurrent directory: {Base_dir} \\nchild directory:{child_dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d8c39-21c0-4fdb-b61a-3ba49be176eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# import gc  # Garbage collector to free memory\n",
    "\n",
    "# def plot3dinteractive(voldata, keyvalue, output_dir, sample_fraction=0.005):\n",
    "#     \"\"\"Plots large 3D NumPy arrays interactively and saves as HTML.\n",
    "    \n",
    "#     - `voldata`: Input 3D NumPy array.\n",
    "#     - `keyvalue`: Filename for saving.\n",
    "#     - `output_dir`: Directory to save HTML plots.\n",
    "#     - `sample_fraction`: Fraction of points to randomly plot.\n",
    "#     \"\"\"\n",
    "#     array_3d = voldata\n",
    "#     x1, y1, z1 = array_3d.shape\n",
    "#     print(f\"Shape of {keyvalue}: {x1, y1, z1}\")\n",
    "\n",
    "#     # Create a 3D meshgrid\n",
    "#     x, y, z = np.meshgrid(np.arange(x1), np.arange(y1), np.arange(z1))\n",
    "\n",
    "#     # Mask non-zero values\n",
    "#     mask = array_3d > 0\n",
    "#     x_vals = x[mask].flatten()\n",
    "#     y_vals = y[mask].flatten()\n",
    "#     z_vals = z[mask].flatten()\n",
    "#     values = array_3d[mask].flatten()\n",
    "\n",
    "#     # **Randomly sample points** to reduce memory usage\n",
    "#     num_points = len(values)\n",
    "#     sample_size = int(num_points * sample_fraction)\n",
    "\n",
    "#     if sample_size > 0:\n",
    "#         indices = np.random.choice(num_points, sample_size, replace=False)\n",
    "#         x_vals = x_vals[indices]\n",
    "#         y_vals = y_vals[indices]\n",
    "#         z_vals = z_vals[indices]\n",
    "#         values = values[indices]\n",
    "#     else:\n",
    "#         print(f\"⚠ Warning: Not enough non-zero points for {keyvalue}. Skipping...\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Plotting {sample_size} points out of {num_points} ({sample_fraction * 100}% sampled)\")\n",
    "\n",
    "#     # **Enhanced Color Grading**\n",
    "#     colorscale = [\n",
    "#         [0.0, \"white\"],    # Outer structure (light color)\n",
    "#         [0.2, \"lightblue\"],\n",
    "#         [0.4, \"deepskyblue\"],\n",
    "#         [0.6, \"dodgerblue\"],\n",
    "#         [0.8, \"blue\"],      # Middle layers\n",
    "#         [1.0, \"darkblue\"]   # Deep inner structure (dark color)\n",
    "#     ]\n",
    "\n",
    "#     # Create a 3D scatter plot\n",
    "#     fig = go.Figure(data=go.Scatter3d(\n",
    "#         x=x_vals,\n",
    "#         y=y_vals,\n",
    "#         z=z_vals,\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=2,\n",
    "#             color=values,\n",
    "#             colorscale=colorscale,\n",
    "#             opacity=0.5\n",
    "#         )\n",
    "#     ))\n",
    "\n",
    "#     # Set axis labels and layout\n",
    "#     fig.update_layout(\n",
    "#         title=f\"3D Structure: {keyvalue}\",\n",
    "#         scene=dict(\n",
    "#             xaxis_title='X',\n",
    "#             yaxis_title='Y',\n",
    "#             zaxis_title='Z',\n",
    "#             bgcolor=\"black\"  # Dark background for better contrast\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # **Save plot as an interactive HTML file**\n",
    "#     save_path = os.path.join(output_dir, f\"{keyvalue}.html\")\n",
    "#     fig.write_html(save_path)\n",
    "#     print(f\"✅ Saved: {save_path}\")\n",
    "\n",
    "#     # **Clear memory**\n",
    "#     del fig, x_vals, y_vals, z_vals, values\n",
    "#     gc.collect()  # Garbage collection to free memory\n",
    "\n",
    "# # **Directory paths**\n",
    "# data_dir = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\intermdata1\"\n",
    "# output_dir = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\"\n",
    "\n",
    "# # **Ensure output directory exists**\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # **Process all .npy files one by one**\n",
    "# npy_files = [f for f in os.listdir(data_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "# for filename in npy_files:\n",
    "#     file_path = os.path.join(data_dir, filename)\n",
    "#     voldata = np.load(file_path)  # Load .npy file\n",
    "#     plot3dinteractive(voldata, filename, output_dir, sample_fraction=0.05)  # Save & clear memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3890f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8ae51bc",
   "metadata": {},
   "source": [
    "# db scan implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58092b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "# from path_manager_JupyterCopy import AddPath\n",
    "# AddPath()\n",
    "\n",
    "# from cluster3Dprocess import Cluster3DProcessor\n",
    "\n",
    "\n",
    "# def batch_process_npy_folder(input_dir,Baseoutput_dir, method='dbscan', save_outputs=True,\n",
    "#                               downsample=True, max_points=100000,\n",
    "#                               save_subvolumes=True, visualize_interactive=True):\n",
    "    \n",
    "#     npy_files = glob.glob(os.path.join(input_dir, '*.npy'))\n",
    "\n",
    "#     print(f\"🔍 Found {len(npy_files)} .npy files in folder: {input_dir}\")\n",
    "\n",
    "#     for npy_file in npy_files:\n",
    "#         print(f\"\\n🚀 Processing file: {os.path.basename(npy_file)}\")\n",
    "#         try:\n",
    "#             processor = Cluster3DProcessor(\n",
    "#                 npy_file=npy_file,\n",
    "#                 Baseoutput_dir = Baseoutput_dir,\n",
    "#                 method=method,\n",
    "#                 save_outputs=save_outputs,\n",
    "#                 downsample=downsample,\n",
    "#                 max_points=max_points,\n",
    "#                 save_subvolumes=save_subvolumes,\n",
    "#                 visualize_interactive=visualize_interactive\n",
    "#             )\n",
    "#             coords, labels = processor.run()\n",
    "#             print(f\"✅ Finished {os.path.basename(npy_file)} - Clusters: {len(set(labels)) - (1 if -1 in labels else 0)}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error processing {os.path.basename(npy_file)}: {e}\")\n",
    "            \n",
    "\n",
    "# PROJECT_DIR = Path.cwd().parent\n",
    "# input_dir = PROJECT_DIR/\"data\"/\"raw_npyData\"\n",
    "# Baseoutput_dir = PROJECT_DIR/\"results\"\n",
    "# # Example usage:\n",
    "# batch_process_npy_folder(\n",
    "#     input_dir= input_dir,\n",
    "#     Baseoutput_dir= Baseoutput_dir,\n",
    "#     method='dbscan',\n",
    "#     save_outputs=True,\n",
    "#     downsample=True,\n",
    "#     max_points=100000,\n",
    "#     save_subvolumes=True,\n",
    "#     visualize_interactive=True\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d1e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Cluster Count via Histogram Peak Detection\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks,convolve\n",
    "from scipy.signal.windows import gaussian\n",
    "\n",
    "# import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from scipy.signal import find_peaks, gaussian, convolve\n",
    "\n",
    "def estimate_clusters_by_histogram(volume, bins=512, smooth_sigma=3, plot=True):\n",
    "    \"\"\"\n",
    "    Estimate the number of clusters based on histogram peak detection.\n",
    "    Also shows histogram with smoothed curve and detected peaks.\n",
    "    \"\"\"\n",
    "    # Flatten the volume to 1D\n",
    "    data = volume.ravel().astype(np.float64)\n",
    "\n",
    "    # Compute histogram\n",
    "    hist_counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
    "\n",
    "    # Apply Gaussian smoothing\n",
    "    if smooth_sigma is not None and smooth_sigma > 0:\n",
    "        kernel_size = int(smooth_sigma * 6) | 1  # make odd\n",
    "        kernel = gaussian(kernel_size, smooth_sigma)\n",
    "        kernel /= kernel.sum()\n",
    "        hist_smooth = convolve(hist_counts, kernel, mode='same')\n",
    "    else:\n",
    "        hist_smooth = hist_counts\n",
    "\n",
    "    # Find peaks in smoothed histogram\n",
    "    peaks, properties = find_peaks(hist_smooth, prominence=np.max(hist_smooth) * 0.01, distance=5)\n",
    "    optimal_k = len(peaks)\n",
    "\n",
    "    # Plot histogram and peaks\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(bin_centers, hist_counts, label='Original Histogram', alpha=0.3, color='gray')\n",
    "        plt.plot(bin_centers, hist_smooth, label='Smoothed Histogram', color='blue')\n",
    "        plt.plot(bin_centers[peaks], hist_smooth[peaks], 'rx', markersize=8, label='Detected Peaks')\n",
    "        for peak in peaks:\n",
    "            plt.axvline(bin_centers[peak], color='red', linestyle='--', alpha=0.4)\n",
    "        plt.title(\"Intensity Histogram with Peak Detection\")\n",
    "        plt.xlabel(\"Intensity\")\n",
    "        plt.ylabel(\"Voxel Count\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return optimal_k, peaks, hist_smooth, bin_centers\n",
    "\n",
    "\n",
    "# def estimate_clusters_by_histogram(volume, bins=512, smooth_sigma=3):\n",
    "#     \"\"\"\n",
    "#     Estimate optimal number of clusters by finding peaks in the intensity histogram.\n",
    "#     Returns (optimal_k, peak_indices, histogram_counts, bin_centers).\n",
    "#     \"\"\"\n",
    "#     # Flatten the volume intensities\n",
    "#     data = volume.ravel().astype(np.float64)\n",
    "#     # Compute histogram over intensity range\n",
    "#     hist_counts, bin_edges = np.histogram(data, bins=bins)\n",
    "#     bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
    "    \n",
    "#     # Smooth the histogram using a Gaussian kernel for stability\n",
    "#     if smooth_sigma is not None and smooth_sigma > 0:\n",
    "#         # Create Gaussian kernel\n",
    "#         kernel_size = int(smooth_sigma * 6)  # cover +-3 sigma\n",
    "#         if kernel_size % 2 == 0: kernel_size += 1\n",
    "#         kernel = gaussian(kernel_size, smooth_sigma)\n",
    "#         kernel /= kernel.sum()\n",
    "#         hist_smooth = convolve(hist_counts, kernel, mode='same')\n",
    "#     else:\n",
    "#         hist_smooth = hist_counts\n",
    "    \n",
    "#     # Find peaks: require a minimum prominence to avoid noise peaks\n",
    "#     peaks, properties = find_peaks(hist_smooth, prominence=np.max(hist_smooth)*0.01, distance=5)\n",
    "#     optimal_k = len(peaks)\n",
    "#     return optimal_k, peaks, hist_smooth, bin_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement for the raw data \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "Mainpath = Path.cwd().parent\n",
    "dataPath = Mainpath/\"data\"/\"raw_npyData\"\n",
    "print(os.listdir(dataPath))\n",
    "filepath= os.path.join(dataPath,'Tomogramma_BuddingYeastCell.npy')\n",
    "data = np.load(filepath)\n",
    "data.shape\n",
    "volume = data\n",
    "param = estimate_clusters_by_histogram(volume, bins=512, smooth_sigma=3, plot=True)\n",
    "# optimal_k, peaks, hist_smooth, bin_centers = estimate_clusters_by_histogram(volume, bins=512, smooth_sigma=3)\n",
    "# print(f\"parameters: {print(val) for val in param}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a2856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal_k\n",
    "# peaks\n",
    "\n",
    "count = 0 \n",
    "for val in param:\n",
    "    count += 1\n",
    "    print(count)\n",
    "    if count != 3:\n",
    "        print(f\"inside the loop! : {count}\")\n",
    "        print(f\"values--> {val} and its'shape: --> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original vs. Anisotropic-Diffusion Filtering for Clustering\n",
    "# Optional: pip install medpy for anisotropic diffusion filtering\n",
    "from medpy.filter.smoothing import anisotropic_diffusion\n",
    "\n",
    "def anisotropic_diffusion_filter(volume, n_iter=10, kappa=50, gamma=0.1):\n",
    "    \"\"\"\n",
    "    Apply 3D anisotropic diffusion filtering (Perona-Malik) to reduce noise.\n",
    "    Scales volume to [0,1] internally for stability.\n",
    "    \"\"\"\n",
    "    vol = volume.astype(np.float64)\n",
    "    # Scale to 0-1 range for the filter as recommended&#8203;:contentReference[oaicite:5]{index=5}\n",
    "    vol_min, vol_max = vol.min(), vol.max()\n",
    "    if vol_max > vol_min:\n",
    "        vol_norm = (vol - vol_min) / (vol_max - vol_min)\n",
    "    else:\n",
    "        vol_norm = vol  # constant volume\n",
    "    # Apply anisotropic diffusion\n",
    "    filtered = anisotropic_diffusion(vol_norm, niter=n_iter, kappa=kappa, gamma=gamma, voxelspacing=None, option=1)\n",
    "    # Scale back to original intensity range\n",
    "    filtered = filtered * (vol_max - vol_min) + vol_min\n",
    "    return filtered\n",
    "\n",
    "def choose_volume_for_clustering(original_vol):\n",
    "    \"\"\"\n",
    "    Determine whether original or anisotropic-diffused volume is better for clustering.\n",
    "    Returns the volume to use (possibly filtered) and a flag indicating if filtering was used.\n",
    "    \"\"\"\n",
    "    # Perform anisotropic diffusion filtering\n",
    "    filt_vol = anisotropic_diffusion_filter(original_vol)\n",
    "    # Compute histogram peaks for both\n",
    "    k_orig, peaks_orig, hist_orig, _ = estimate_clusters_by_histogram(original_vol)\n",
    "    k_filt, peaks_filt, hist_filt, _ = estimate_clusters_by_histogram(filt_vol)\n",
    "    # Decide based on number of peaks and peak prominence\n",
    "    use_filtered = False\n",
    "    if k_filt > k_orig:\n",
    "        use_filtered = True\n",
    "    elif k_filt == k_orig:\n",
    "        # Compare average peak prominence if equal peaks\n",
    "        # (compute prominence for the first peak as example)\n",
    "        if len(peaks_filt) > 0 and len(peaks_orig) > 0:\n",
    "            prom_filt = np.mean(hist_filt[peaks_filt])\n",
    "            prom_orig = np.mean(hist_orig[peaks_orig])\n",
    "            if prom_filt > prom_orig * 1.1:  # filtered peaks are >10% higher on average\n",
    "                use_filtered = True\n",
    "    chosen_vol = filt_vol if use_filtered else original_vol\n",
    "    return chosen_vol, use_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ae0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "anisotropic_diffusion_filter(data,n_iter=10, kappa=50, gamma=0.1)\n",
    "param = choose_volume_for_clustering(volume)\n",
    "print(f\"parameters: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e78444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def intensity_kmeans(volume, n_clusters):\n",
    "    \"\"\"\n",
    "    Run K-means clustering on voxel intensities. Returns a label volume of the same shape.\n",
    "    \"\"\"\n",
    "    data = volume.ravel().reshape(-1, 1)  # shape (N,1)\n",
    "    # Configure KMeans (use MiniBatchKMeans for very large N to save memory)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10, tol=1e-4, verbose=0)\n",
    "    # Fit on all voxels (this may be memory intensive but required by specification)\n",
    "    kmeans.fit(data)\n",
    "    labels_flat = kmeans.labels_\n",
    "    # Reshape back to the original volume shape\n",
    "    label_volume = labels_flat.reshape(volume.shape)\n",
    "    return label_volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_kmeans(volume,n_clusters= optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "def connectivity_filtering(label_volume, structure=None, min_size=0):\n",
    "    \"\"\"\n",
    "    Ensure each cluster label is spatially connected. \n",
    "    Splits clusters into connected components and filters out small components.\n",
    "    Returns a new label volume with unique labels for each connected region.\n",
    "    \"\"\"\n",
    "    if structure is None:\n",
    "        # Define connectivity: 26-neighborhood in 3D (3x3x3 cube of ones)\n",
    "        structure = np.ones((3,3,3), dtype=np.int8)\n",
    "    volume_shape = label_volume.shape\n",
    "    # Output volume for new labels\n",
    "    filtered_labels = np.zeros(volume_shape, dtype=np.int32)\n",
    "    new_label = 1  # start labels from 1 (0 reserved for background if any)\n",
    "    # Loop over each original cluster label\n",
    "    orig_labels = np.unique(label_volume)\n",
    "    for lbl in orig_labels:\n",
    "        if lbl < 0: \n",
    "            continue  # skip invalid labels if any\n",
    "        mask = (label_volume == lbl)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        # Label connected components within this mask\n",
    "        comp_labels, num_comp = ndimage.label(mask, structure=structure)\n",
    "        # Assign each component a unique new label\n",
    "        for comp_idx in range(1, num_comp+1):\n",
    "            comp_mask = (comp_labels == comp_idx)\n",
    "            comp_size = comp_mask.sum()\n",
    "            if min_size and comp_size < min_size:\n",
    "                # Optionally skip tiny components (assign 0 to remove)\n",
    "                continue\n",
    "            filtered_labels[comp_mask] = new_label\n",
    "            new_label += 1\n",
    "    return filtered_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b830e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629d0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def visualize_clusters_3d(label_volume, max_clusters=None):\n",
    "    \"\"\"\n",
    "    Create a Plotly Figure with each cluster visualized as a 3D isosurface.\n",
    "    If max_clusters is set, limit to that many largest clusters for performance.\n",
    "    \"\"\"\n",
    "    zdim, ydim, xdim = label_volume.shape\n",
    "    # Prepare coordinate grids (voxel indices or physical coordinates if available)\n",
    "    Z, Y, X = np.mgrid[0:zdim, 0:ydim, 0:xdim]\n",
    "    fig = go.Figure()\n",
    "    # Choose clusters to display\n",
    "    labels = np.unique(label_volume)\n",
    "    labels = labels[labels != 0]  # exclude 0 if used as background\n",
    "    if max_clusters:\n",
    "        # Optionally select top N clusters by size\n",
    "        sizes = [(label_volume == lbl).sum() for lbl in labels]\n",
    "        # sort labels by size descending\n",
    "        labels = [lbl for _, lbl in sorted(zip(sizes, labels), reverse=True)]\n",
    "        labels = labels[:max_clusters]\n",
    "    # Define some colors for clusters (cycle if needed)\n",
    "    color_list = [\"red\", \"green\", \"blue\", \"yellow\", \"magenta\", \"cyan\", \"orange\", \"purple\"]\n",
    "    # Plot each cluster\n",
    "    for i, lbl in enumerate(labels):\n",
    "        mask = (label_volume == lbl)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        # Flatten coordinates and mask values for plotting\n",
    "        Xc = X[mask]\n",
    "        Yc = Y[mask]\n",
    "        Zc = Z[mask]\n",
    "        vals = mask[mask]  # this will be an array of True(1) values\n",
    "        # Assign a color (repeat if out of predefined colors)\n",
    "        color = color_list[i % len(color_list)]\n",
    "        # Create isosurface trace for this cluster\n",
    "        fig.add_trace(go.Isosurface(\n",
    "            x=Xc, y=Yc, z=Zc, value=vals.astype(np.int8),\n",
    "            isomin=1, isomax=1,  # show surface where mask=1\n",
    "            surface_count=1,     # single surface\n",
    "            opacity=0.6,\n",
    "            coloraxis=None,      # use a solid color from colorscale\n",
    "            colorscale=[[0, color], [1, color]],  # monochromatic colorscale for this cluster\n",
    "            showscale=False,\n",
    "            name=f\"Cluster {lbl}\"\n",
    "        ))\n",
    "    # Set figure layout for better appearance\n",
    "    fig.update_layout(scene=dict(\n",
    "            xaxis_title='X', yaxis_title='Y', zaxis_title='Z'),\n",
    "        title=\"3D Clusters Visualization\")\n",
    "    return fig\n",
    "\n",
    "# Example usage (assuming 'final_labels' is the output from connectivity_filtering):\n",
    "# fig = visualize_clusters_3d(final_labels, max_clusters=10)\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db710854",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_clusters_3d(final_labels, max_clusters=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a3bfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
