{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use Case:\n",
    "    1. Use It in Jupyter:\n",
    "                \n",
    "        from path_manager import addpath\n",
    "        paths = addpath()\n",
    "        # Use the returned dictionary if needed\n",
    "        print(\"Base dir:\", paths['BASE_DIR'])\n",
    "\n",
    "    2. Use in Python Script:\n",
    "        from path_manager import addpath\n",
    "        addpath()\n",
    "        # Then import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below code , To save the plot and see the data distribution values. x_data = x-axis is created from the data values by uniformly distributting the data from min(data) to max(data)\n",
    "### we can have quick look of data arrangement , but there is drawback in plot that y-values if they are same should be look piled up on each other will look apart by a certain x-axis difference. so we will plot it differently : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "# print(f\"garbage path : {os.getcwd()} and \\n {Path.home()}\")\n",
    "# cws = Path(__file__).resolve().parent   # use it everywhere for current working scripts path (cws)\n",
    "cws = Path.cwd()\n",
    "BASE_DIR = cws.parent             # \\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\\n",
    "DATA_PATH = BASE_DIR /\"data\"      # \\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\\n",
    "rawnypyData_Path = DATA_PATH/\"raw_npyData\"\n",
    "data_dir = str(rawnypyData_Path)\n",
    "print(f\"data_dir : {data_dir}\")\n",
    "\n",
    "\n",
    "save_dir = BASE_DIR/\"results\"/\"rawData_XYPlot\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create if missing\n",
    "\n",
    "plot_Type = int(input(\"enter value 0/1: for simple:0 Complex :1\"))\n",
    "t_start = time.time()\n",
    "\n",
    "if plot_Type == 0:\n",
    "\n",
    "    for file in os.listdir(data_dir):\n",
    "\n",
    "        if file.endswith('.npy'):\n",
    "            filename = os.path.join(data_dir, file)\n",
    "            print(f\"filename:{file}\")\n",
    "            data = np.load(filename)\n",
    "            data = data.flatten().reshape(-1, 1)\n",
    "\n",
    "            # created the x_axis with it's min and max values and uniformly distributed between them.\n",
    "            x_data = np.linspace(data.min(), data.max(), len(data))\n",
    "\n",
    "            # plot the data\n",
    "            # print(f\"data size: {data.shape}\")\n",
    "            plt.plot(x_data, data, '.', color='blue', markersize=0.4)\n",
    "            plt.title(f\"{file[:-4]} Linearly Spaced Vector \")\n",
    "            plt.xlabel('linearly spaced vector from data itself')\n",
    "            plt.ylabel('original Values')\n",
    "            plt.grid(True)\n",
    "            # plt.show() if  we put it here then it will show the plot  and the clear it before saving.\n",
    "\n",
    "            # save_dir = BASE_DIR/\"results\"/\"rawData_XYPlot\"\n",
    "            save_path = os.path.join(str(save_dir), f\"{file[:-4]}.png\")\n",
    "\n",
    "            if f\"{file[:-4]}.png\" not in os.listdir(save_dir):\n",
    "                print(f\"saving the file in save_path: {save_path}\")\n",
    "                plt.savefig(save_path, dpi=300)\n",
    "            else:\n",
    "                print(f\"file is already available in save_path: {save_path}\")\n",
    "            \n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "if plot_Type == 1:\n",
    "    countcomplex = 0\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.npy'):\n",
    "            filename = os.path.join(data_dir, file)\n",
    "            print(f\"processing  file, filename: {file}\")\n",
    "            countcomplex += 1\n",
    "\n",
    "            # if countcomplex > 5:\n",
    "            if file[0:10] =='Tomogramma' or file[0:3] == 'AML':\n",
    "                # print(\" i am not skipping when continue is off : only one file is allowed to be plotted\")\n",
    "                print(f\"skipping the file: {file} and countcomplex: {countcomplex}\")\n",
    "                continue\n",
    "\n",
    "            data = np.load(filename).flatten()\n",
    "\n",
    "            x_data = np.linspace(data.min(), data.max(), len(data))\n",
    "\n",
    "            # === 1. Find horizontal \"flat line\" band ===\n",
    "            # Detect most frequent value range using histogram\n",
    "            hist_vals, bin_edges = np.histogram(data, bins=10000)\n",
    "            dominant_bin_index = np.argmax(hist_vals)\n",
    "            bin_start = bin_edges[dominant_bin_index]\n",
    "            bin_end = bin_edges[dominant_bin_index + 1]\n",
    "\n",
    "            flat_band = data[(data >= bin_start) & (data < bin_end)]\n",
    "\n",
    "            if flat_band.size == 0:\n",
    "                print(\"No flat band detected.\")\n",
    "\n",
    "                continue\n",
    "\n",
    "            flat_min = np.min(flat_band)\n",
    "            flat_max = np.max(flat_band)\n",
    "            flat_mean = np.mean(flat_band)\n",
    "\n",
    "            # === 2. Plot with color-coded data ===\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            scatter = plt.scatter(x_data, data, c=data, cmap='viridis', s=0.4)\n",
    "\n",
    "            # === 3. Draw horizontal lines for flat values ===\n",
    "            flat_y_vals = [(\"Min\", flat_min, 'red'),\n",
    "                           (\"Mean\", flat_mean, 'blue'),\n",
    "                           (\"Max\", flat_max, 'black')]\n",
    "            # used_y = []\n",
    "\n",
    "            # for label, y_val, color in flat_y_vals:\n",
    "            #     plt.axhline(y=y_val, color=color, linestyle='--', linewidth=1)\n",
    "\n",
    "            #     # === 4. Auto offset to avoid text overlap ===\n",
    "            #     offset = 0\n",
    "            #     while any(abs((y_val + offset) - y) < 0.002 * (flat_max - flat_min) for y in used_y):\n",
    "            #         offset += 0.002 * (flat_max - flat_min)\n",
    "            #     y_text = y_val + offset\n",
    "            #     used_y.append(y_text)\n",
    "\n",
    "            #     # Text with position\n",
    "            #     plt.text(x_data[0], y_text, f'{label}: {y_val:.8f}',\n",
    "            #              color=color, fontsize=6, verticalalignment='bottom')\n",
    "            \n",
    "            used_y = []\n",
    "            x_text_position = x_data[0] - 0.0002*(x_data[-1] - x_data[0])  # shift right\n",
    "            \n",
    "\n",
    "            if file[-5:-4] =='h':\n",
    "                text_gap = 0.12* max(abs(flat_max - flat_min), 1e-1)  # vertical spacing\n",
    "            else:\n",
    "                text_gap = 0.05* max(abs(flat_max - flat_min), 1e-1)  # vertical spacing\n",
    "                \n",
    "\n",
    "            for i, (label, y_val, color) in enumerate(flat_y_vals):\n",
    "                y_text = y_val + i * text_gap  # stagger vertically (fixed gap)\n",
    "                plt.axhline(y=y_val, color=color, linestyle='--', linewidth=0.6)\n",
    "\n",
    "                plt.text(x_text_position, y_text, f'{label}: {y_val:.16f}',\n",
    "                        color=color, fontsize=8,\n",
    "                        verticalalignment='bottom', horizontalalignment='left',\n",
    "                        bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))\n",
    "\n",
    "\n",
    "            # === 5. Plot settings ===\n",
    "            plt.title(f\"{file[:-4]}\")\n",
    "            plt.xlabel('Linearly spaced vector from data')\n",
    "            plt.ylabel('RI Values')\n",
    "            plt.grid(True)\n",
    "            cbar = plt.colorbar(scatter)\n",
    "\n",
    "            cbar.set_label('Intensity')\n",
    "\n",
    "            # Save the figure\n",
    "            plt.tight_layout()\n",
    "            # plt.show() # if we put it here then it will show the plot  and the clear it before saving.\n",
    "\n",
    "            save_path = os.path.join(save_dir, f\"{file[:-4]}_marked.png\")\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "            print(f\"saving the file in save_path: {save_path}\")\n",
    "\n",
    "            # if f\"{file[:-4]}_marked.png\" not in os.listdir(save_dir):\n",
    "            #     print(f\"saving the file in save_path: {save_path}\")\n",
    "            #     plt.savefig(save_path, dpi=300)\n",
    "            # else:\n",
    "            #     print(f\"file is already available in save_path: {save_path}\")\n",
    "            # Show the plot\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "t_end = time.time()\n",
    "print(f\"Total Time --------------->: {t_end - t_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(0, 10, 10)\n",
    "# y1 = np.sin(x)\n",
    "# y2 =  y1 + 2\n",
    "# plt.plot(x, y1, label='sin(x)')\n",
    "# plt.plot(x, y2, label='sin(x) + 2')\n",
    "# plt.show()\n",
    "# file[-5:-4]\n",
    "filepath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw_npyData\\Tomogramma_Cell.npy\"\n",
    "import numpy as np\n",
    "import os\n",
    "filename = os.path.basename(filepath)\n",
    "print(f\"Extracted name is Tomogramam : {filename[0:10]} and length : {len(filename[0:10])}\")\n",
    "\n",
    "# filename = os.path.basename(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding resolution of data, so not to skip small varaition,\n",
    "# First just plot Raw Data after making flat, if it is equal and more than 2 dimensions -> data = data.flatten()\n",
    "# x_data = np.linspace(min(data),max(data),len(data)), data = data.flatten() ---> y_data  =  sorted_data = np.sort(data)\n",
    "\n",
    "# binsize = diff(choosen smallest values after sorting the data) : data = data.flatten().reshape(-1,1)\n",
    "# 201*201*201 = 8120601; 200*200*200 = 8000000;\n",
    "# :.2e\t1.23e-05\t2 decimal places\n",
    "# :.8e\t1.23456000e-11\t8 decimal places\n",
    "# # Print with scientific notation and 8 decimal places\n",
    "# print(f\"{value:.8e}\n",
    "\n",
    "from pathlib import Path\n",
    "from listspecificfiles import readlistFiles\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "data_path = r\"data\\raw_npyData\"  # relative path with r\"\"relativepath\"\n",
    "files = readlistFiles(data_path, '.npy')\n",
    "fpath = files.file_with_Path()  # file with path name.\n",
    "BASE_DIR = Path.cwd().parent\n",
    "print(f\"BASE_DIR:--------> {BASE_DIR}\")\n",
    "save_pathtTEXT = BASE_DIR/\"results\"\n",
    "\n",
    "################################ for simple plot of data ##############################################\n",
    "\n",
    "\n",
    "def simple_plot(data, file, savepathplot):\n",
    "    x_data = np.linspace(min(data), max(data), len(data))\n",
    "    # fig = plt.figure(figsize=(10,6))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_data, data, s=0.1)\n",
    "    plt.title(f\"{os.path.basename(file)[:-4]} sortred Data \")\n",
    "    plt.xlabel('Data: linearly spaced data itself')\n",
    "    plt.ylabel('Data:RI_value')\n",
    "    plt.grid(True)\n",
    "    save_dir = BASE_DIR/\"results\"/\"rawData_XYPlot\"/\"plotSortedData\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"{os.path.basename(file)[:-4]}\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "#####################################################################################################\n",
    "\n",
    "textFilesSave = os.path.join(save_pathtTEXT, 'details_Datafile.txt')\n",
    "with open(textFilesSave, 'w') as f:\n",
    "    f.close()\n",
    "# count = 0\n",
    "for file in fpath:\n",
    "    # count +=1\n",
    "    fileName = os.path.basename(file)\n",
    "    # print(f\"FullfilePath:{file} \\n fileName:{fileName}\")\n",
    "\n",
    "    data = np.load(file)\n",
    "    Data_shape = data.shape\n",
    "    data = data.flatten()\n",
    "    sorted_data = np.sort(data)\n",
    "    # simple_plot(sorted_data,file,BASE_DIR)\n",
    "\n",
    "    diff_1data = np.diff(sorted_data)\n",
    "    # diff_2data = np.diff(sorted_data)\n",
    "\n",
    "# ------------------------  found zeros in the diff1, Now filter zeros then go for first and second nonzero values --------------\n",
    "    # Filter out exact or near-zero differences to avoid duplicates\n",
    "    prcsn = 1e-9  # set precison\n",
    "    nonzero_diffs = diff_1data[np.abs(diff_1data) > prcsn]\n",
    "\n",
    "    # Use np.unique to get distinct difference values\n",
    "    unique_diffs = np.unique(nonzero_diffs)\n",
    "\n",
    "    # Extract min and second min, keeping 9 decimal precision\n",
    "    # for unique_val in range(unique_diffs):\n",
    "    min_val = unique_diffs[0]\n",
    "    second_min_val = unique_diffs[1] if len(unique_diffs) > 1 else None\n",
    "\n",
    "    # Print values with high precision\n",
    "    print(\n",
    "        f\"Minimum difference {fileName}  : {min_val:.12f} i.e. {min_val:.12e}\")\n",
    "    if second_min_val is not None:\n",
    "        print(\n",
    "            f\"Second minimum difference {fileName}: {second_min_val:.12f} i.e. {second_min_val:.12e}\")\n",
    "    else:\n",
    "        print(\"No second unique non-zero difference found.\")\n",
    "\n",
    "    # print(f\"RESOLUTION_DATA: {min(diff_1data):.8f} DATA SIZE: {sorted_data.shape} \\n lower value:{sorted_data[0:3]} and upper value:{sorted_data[-4:]}\")\n",
    "    # text_data = f\"{fileName}: Data_shape:{Data_shape}, DATA SIZE: {sorted_data.shape}, RESOLUTION_DATA: {min(diff_1data)},lower value:{sorted_data[0:3]}, upper value:{sorted_data[-3:]}\\n\"\n",
    "    # with open(textFilesSave,'a') as f:\n",
    "    #     f.write(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # np.unique_counts(nonzero_diffs)\n",
    "# a = np.array([1, 2, 4, 3, 2])\n",
    "# # values,count = np.unique(a, return_counts=True) # return_counts -returns the unique value counts.\n",
    "# # print(f\"a:{a.shape} \\n values:{values} \\n count:{count} \\n\")\n",
    "# print(f\"a:{a.shape} \\n\")\n",
    "# # a = np.array([1.3312498765,1.3312498761,1.3312498712,1.3312498763,1.3312498768,1.3312498765])\n",
    "# # a  = np.sort(a)\n",
    "# # a = list(a)\n",
    "# f,count,inv,count1 = np.unique(np.round(a,decimals= 11), return_counts=True,return_inverse=True,return_index = True)\n",
    "# # print(f\"a : {float(a)} and its size: {len(a)}\")\n",
    "# print(f\"a : {a} and its size: {len(a)}\")\n",
    "\n",
    "# # f,idx1,count = np.unique(a,return_index = True, return_counts=True)\n",
    "# f,count1,count,inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = 1.01e-9\n",
    "# print(f\"val:{x:.11f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# str1 = '1.000010200000'\n",
    "# tt1 = str1.rstrip('0')   # eliminate the trailing zeros.\n",
    "# tt1\n",
    "# # 'Hello world'.title()  # 'Hello World'\n",
    "# # \"they're bill's friends from the UK\".title() #\"They'Re Bill'S Friends From The Uk\"\n",
    "# # \"42\".zfill(5)  # padding of total 5 including the \"42\", so three zeros (000) will be added at starting. \n",
    "# str2 = 'janakak'\n",
    "# str2.replace('k','R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## - below code find the significant digits in my data and plot the histogram: \n",
    "##💡  it tells you how many digits are non-zero or meaningful in the float’s scientific representation.\n",
    "\n",
    "##💡 It’s a heuristic method — useful for checking how many digits \"matter\" in terms of numerical precision.\n",
    "\n",
    "##💡 np.unique(data): --> it always return the results in the order uniqValues(sorted array of unique values in increasing Order).\n",
    "\n",
    "##💡idx: (first index of each unique values in the original data).\n",
    "\n",
    "##💡inverse_idx:  \n",
    "\n",
    "##💡counts : returns counts of each values in unique array.\n",
    "\n",
    "<!-- Total_res = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    uniqValues,idx,inverse_idx,counts = Total_res  #  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def UniqueValueCount(data):\n",
    "    \"\"\"\n",
    "    give your data in numpy array format 1d: \n",
    "    uniqValues,idx,inverse_idx,counts reurn values in this order\n",
    "    get your results:, it will return Total_res then extract these: uniqValues,idx,inverse_idx,counts\n",
    "    extract like this : uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    \"\"\"\n",
    "    significant_digit_data = data\n",
    "    # uniqValues,idx,inverse_idx,counts = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    Total_res = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    print(f\"\\n unique values:{uniqValues},\\n  Unique counts: {counts} and \\n indexes:{idx}, \\n reverse_idx : {inverse_idx} and \\n orginal Array :{uniqValues[inverse_idx]}\")\n",
    "    return Total_res\n",
    "\n",
    "data = np.array([1,2,4,3,2])\n",
    "UniqueValueCount(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# DOES NOT WORKED ALWAYS GAVE 18/19 DECIMAL DIGITS PRECISION FOR ALL DATA --> becasue of formatting f\"{val:.18f}\" #####################\n",
    "\n",
    "# def significant_digits(val):\n",
    "#     \"\"\"Extract first significant digit (ignores sign and zeros).\"\"\"\n",
    "#     val = float(val)\n",
    "#     if val == 0:\n",
    "#         return 0\n",
    "#     my_strVal = str(val)  \n",
    "#     return int(my_strVal.lstrip('-0.')[0])\n",
    "\n",
    "# def decimal_places(val):\n",
    "#         s = f\"{val:.18f}\".rstrip('0')\n",
    "#         if '.' in s:\n",
    "#             return len(s.split('.')[1])\n",
    "#         return 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# useful for checking how many digits \"matter\" in terms of numerical precision.\n",
    "\n",
    "from  pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "\n",
    "from listspecificfiles import readlistFiles\n",
    "\n",
    "from plot_dataModule import DataPlotter  # import DataPlotter from plot_dataModule.py file, all the functions are defined realted to plot the histogram and save it in the directory and simple raw data plot.\n",
    "\n",
    "from data_saver_module import DataSaver # import DataSaver from data_saver_module.py file, all the functions are defined related to save the data in the directory.\n",
    "\n",
    "data_path = r\"data\\raw_npyData\"  # relative path with r\"\"relativepath\"\n",
    "files = readlistFiles(data_path,'.npy')\n",
    "fpath = files.file_with_Path()  # file with path name.\n",
    "\n",
    "# Directory to save histograms\n",
    "BASE_DIR = Path.cwd().parent\n",
    "save_dir = BASE_DIR/\"results\"/\"histogram_significantDigits\"\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "\n",
    "\n",
    "############################# this is the function to find number of decimal points values after decimal in right side of decimal (show precision) #####################\n",
    "import math\n",
    "from decimal import Decimal\n",
    "def actual_significant_digits_after_decimal(val):\n",
    "    \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\n",
    "        handle the inf and NaN values also in the data. return zeros in this case.\n",
    "    \"\"\"\n",
    "\n",
    "    if math.isinf(val) or math.isnan(val):\n",
    "        return 0  # Or return None to indicate 'not countable'\n",
    "    \n",
    "    if val == 0:\n",
    "        return 0\n",
    "    d = Decimal(str(val)).normalize()\n",
    "    if '.' not in str(d):\n",
    "        return 0\n",
    "    return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "####################################### This is for the Find the Unique values and used to find Maximum precision value in data. ###########################################################\n",
    "import numpy as np\n",
    "def UniqueValueCount(data):\n",
    "    \"\"\"\n",
    "    give your data in numpy array format 1d: \n",
    "    uniqValues,idx,inverse_idx,counts reurn values in this order\n",
    "    get your results:, it will return Total_res then extract these: uniqValues,idx,inverse_idx,counts\n",
    "    extract like this : uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    \"\"\"\n",
    "    significant_digit_data = data\n",
    "    # uniqValues,idx,inverse_idx,counts = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    Total_res = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    return Total_res\n",
    "    print(f\"\\n unique values:{uniqValues},\\n  Unique counts: {counts} and \\n indexes:{idx}, \\n reverse_idx : {inverse_idx} and \\n orginal Array :{uniqValues[inverse_idx]}\")\n",
    "\n",
    "# data = np.array([1,2,4,3,2])\n",
    "# UniqueValueCount(data)\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "# proceess all files in the directory and generate histogram for each file and save it in the directory. all the auxiliary functions are defined in the same Cell, are used in this code.\n",
    "save_dir1 = BASE_DIR/\"results\"/\"histogram_3splitSgnfctDigits\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "countfile = 0\n",
    "for FileWithPath in fpath:\n",
    "    countfile +=1\n",
    "    print(f\"FileWithPath: {FileWithPath} and countfile: {countfile}\")\n",
    "\n",
    "    # this was just to test the code, so not to process all files in the directory.\n",
    "    # if countfile <=4 or countfile >=6 :\n",
    "    #     print(f\"Ignored file .: {countfile}\")\n",
    "    #     # break\n",
    "    #     continue\n",
    "    \n",
    "    print(f\"Processed file .: {countfile}\")\n",
    "\n",
    "    filename = os.path.basename(FileWithPath)\n",
    "    print(f\"Processing filename: {filename}\")\n",
    "    \n",
    "    # Load and flatten data\n",
    "    data = np.load(FileWithPath)\n",
    "    data = data.flatten()\n",
    "    print(f\"Shape of data: {data.shape[0]}\")\n",
    "    \n",
    "    # Extract first significant digit\n",
    "    # significant_digit_data = [significant_digits(val) for val in data if val != 0]\n",
    "    significant_digit_data = [actual_significant_digits_after_decimal(val) for val in data if val != 0]\n",
    "\n",
    "    # Plot histogram\n",
    "    UniqueValues = UniqueValueCount(significant_digit_data)[0]\n",
    "    Counts_OFUniqueValues = UniqueValueCount(significant_digit_data)[3]\n",
    "    x_axis_maxValueHist =  max(UniqueValues)\n",
    "    \n",
    "\n",
    "    metadata_list = [\n",
    "        {\n",
    "            \"filename\": f\"{filename[:-4]}\",\n",
    "            \"unique_values\": UniqueValues.tolist(),  # Convert numpy array to list\n",
    "            \"counts\": Counts_OFUniqueValues.tolist(),  # Convert numpy array to list\n",
    "            \"significant_DecimalDigits\": int(x_axis_maxValueHist),  # Convert numpy.int64 to int\n",
    "            \"significant_digit_data\": significant_digit_data  # significant decimal digits for each data value in the data array (which are converted from 3d numpy to 1d array).\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "################################### this is to save the metadata in the json file. #######################################################################################\n",
    "    from data_saver_module import DataSaver\n",
    "    Data_sgnDecimalDgts_save_dir = BASE_DIR/\"results\"/\"Data_SignificantDecimalDigits\"\n",
    "    os.makedirs(Data_sgnDecimalDgts_save_dir, exist_ok=True)  # Create if missing\n",
    "    saver = DataSaver(save_dir=Data_sgnDecimalDgts_save_dir, include_fields=[], filename=f\"{filename[:-4]}.json\")\n",
    "    saver.save(metadata_list)\n",
    "\n",
    "\n",
    "\n",
    "    print(F\"HERE precision  digits in the data values: {x_axis_maxValueHist}\")\n",
    "\n",
    "    # <----------------  JUST CHOOSE ONE OF THE FOLLOWING HISTOGRAMS TO PLOT AND SAVE IT. JUST UNCOMMENT ANY ONE ----------- >\n",
    "\n",
    " ############### 1ST --  invoke the function to plot histogram with color coding and annotation\n",
    "\n",
    "    if countfile >=1:   # for ignoring the saving the histogram and its plotting.\n",
    "        continue\n",
    "\n",
    "\n",
    "    DataPlotter.plot_histogram_with_annotate_counts(\n",
    "        Counts_OFUniqueValues,\n",
    "        UniqueValues,\n",
    "        title = f\"{filename[:-4]}: Annotated Histogram\",\n",
    "        xlabel = f\"Significant Digit (1–{max(UniqueValues)})\",\n",
    "        ylabel = \"Normalized Frequency (Fixed Height)\",\n",
    "        saveplot = False,\n",
    "        filename = filename[:-4],\n",
    "        save_dir = save_dir,\n",
    "        dpi = 600  # Publication-ready resolution\n",
    "    )\n",
    "\n",
    "\n",
    "############### 2ND -- invoke the function to plot histogram Here HORIZONTAL SPLIT HISTOGRAM : with color coding and annotation\n",
    "    DataPlotter.plot_horizontal_split_histogram(\n",
    "    significant_digit_data=significant_digit_data,\n",
    "    title= f\"{filename[:-4]}: Annotated Histogram\",\n",
    "    xlabel=\"Frequency of Occurrence\",\n",
    "    ylabel = f\"Significant Digit (1–{max(UniqueValues)})\",\n",
    "    saveplot = False,\n",
    "    filename = filename[:-4],\n",
    "    save_dir= save_dir1\n",
    "    )\n",
    "\n",
    "print(f\"Total number of files processed: {countfile} and plot generated for each file if saveplot is True.\")\n",
    "\n",
    "\n",
    "################################ older version of histogram plot with color coding and annotation ###############\n",
    "    # plt.figure(figsize=(8, 5))\n",
    "    # plt.hist(significant_digit_data,bins='auto')\n",
    "    # plt.title(f\"Significant Digit Histogram: {filename}\")\n",
    "    # plt.xlabel(f\"Significant Digit (1-{max(UniqueValues)})\")\n",
    "    # plt.ylabel(\"Frequency\")\n",
    "    # plt.xticks(range(1, max(UniqueValues)+1))\n",
    "    # plt.grid(True)\n",
    "    # plt.show()\n",
    "    # Save the figure\n",
    "    # hist_path = os.path.join(save_dir, f\"{filename[:-4]}_histogram.png\")\n",
    "    # plt.savefig(hist_path)\n",
    "    # plt.close()\n",
    "\n",
    "    # Optional: Print most common digit\n",
    "    # most_common_digit = np.argmax(counts) + 1  # bins are 1-indexed\n",
    "    # print(f\"Most frequent significant digit: {most_common_digit}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries after reset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import label\n",
    "\n",
    "\n",
    "def Peak_Detect_BackgroundThreshold(flat_data, save_path=None):\n",
    "    \"\"\"\n",
    "    Analyze a flat data array, plot its histogram, and save the plot.\n",
    "    \n",
    "    Parameters:\n",
    "        flat_data (numpy.ndarray): 1D array of data values.\n",
    "        save_path (str): Path to save the histogram plot.\n",
    "    \"\"\"\n",
    "    # Compute histogram\n",
    "    # Choose binning method\n",
    "    # binning_method = input(\"Choose binning method (sturges, doanes, freedman-diaconis, auto): \").strip().lower()\n",
    "    if binning_method in ['sturges', 'doanes', 'fd']:\n",
    "        counts, bin_edges = np.histogram(flat_data, bins=binning_method)\n",
    "    else:\n",
    "        counts, bin_edges = np.histogram(flat_data, bins=5000)  # Default to 'auto' if input is invalid\n",
    "\n",
    "    peak_idx = np.argmax(counts)  # index of the peak\n",
    "\n",
    "    # Thresholding and labeling\n",
    "    threshold = 0.6 * counts[peak_idx]  # 60% of peak value\n",
    "    mask = counts >= threshold\n",
    "    labeled, num_features = label(mask)\n",
    "\n",
    "    peak_label = labeled[peak_idx]\n",
    "    range_idxs = np.where(labeled == peak_label)[0]\n",
    "    range_start = bin_edges[range_idxs[0]]\n",
    "    range_end = bin_edges[range_idxs[-1] + 1]\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(bin_edges[:-1], counts, width=np.diff(bin_edges), edgecolor='black', label='Histogram', align='edge')\n",
    "    plt.axhline(threshold, color='red', linestyle='--', label=f'Threshold ({threshold:.1f})')\n",
    "    plt.axvspan(range_start, range_end, color='orange', alpha=0.3, label=f'Connected Region ({range_start:.2f}-{range_end:.2f})')\n",
    "    plt.scatter(bin_edges[peak_idx], counts[peak_idx], color='blue', zorder=5, label='Peak')\n",
    "    plt.title(f'Connected Region Around Peak{filename[:-4]}')\n",
    "    plt.xlabel('RI Value')\n",
    "    plt.ylabel('Count')\n",
    "    plt.ylim(0, 900)  # Extend y-axis for better visibility\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Histogram saved to {save_path}\")\n",
    "    return range_start, range_end, threshold, peak_label\n",
    "\n",
    "\n",
    "\n",
    "# Re-import necessary packages after code execution state rese\n",
    "\n",
    "# Simulate example data with background noise and signal\n",
    "# np.random.seed(0)\n",
    "# background = np.random.normal(loc=1.334, scale=1e-12, size=100000)  # very fine, dominant background\n",
    "# signal = np.random.normal(loc=1.335, scale=1e-10, size=5000)        # weaker signal cluster\n",
    "# data = np.concatenate([background, signal])\n",
    "def peak_range(data, approx_bin_width = None):\n",
    "        # Use a more memory-efficient bin width strategy\n",
    "    if approx_bin_width:\n",
    "        approx_bin_width = approx_bin_width\n",
    "        print(f\"choosing the given approx_bin_width value: {approx_bin_width}\")\n",
    "\n",
    "    else:\n",
    "        approx_bin_width = 0.01  # e.g., good for typical float precision\n",
    "\n",
    "    bins = np.arange(np.min(data), np.max(data) + approx_bin_width, approx_bin_width)\n",
    "\n",
    "    # Recompute histogram with adjusted bin width\n",
    "    counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    peak_idx = np.argmax(counts)\n",
    "    peak_val = bin_centers[peak_idx]\n",
    "\n",
    "    # Iteratively find range around the peak\n",
    "    left = peak_idx\n",
    "    while left > 0 and counts[left] >= counts[left - 1] * 0.5:\n",
    "        left -= 1\n",
    "    right = peak_idx\n",
    "    while right < len(counts) - 1 and counts[right] >= counts[right + 1] * 0.5:\n",
    "        right += 1\n",
    "\n",
    "    peak_range = (bin_centers[left], bin_centers[right])\n",
    "    filtered_data = data[(data < peak_range[0]) | (data > peak_range[1])]\n",
    "\n",
    "    # Plotting updated histograms\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "    # Original histogram\n",
    "    axs[0].hist(data, bins=bins, color='gray', edgecolor='black')\n",
    "    axs[0].axvspan(peak_range[0], peak_range[1], color='red', alpha=0.3, label='Filtered Background')\n",
    "    axs[0].set_title(\"Original Histogram with Background\")\n",
    "    axs[0].set_xlabel(\"Value\")\n",
    "    axs[0].set_ylabel(\"Frequency\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Filtered histogram\n",
    "    axs[1].hist(filtered_data, bins=bins, color='blue', edgecolor='black')\n",
    "    axs[1].set_title(\"Histogram After Background Removal\")\n",
    "    axs[1].set_xlabel(\"Value\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define filtering function using decimal precision\n",
    "def filter_background_by_precision(data, decimal_threshold=4, width_scale=2):\n",
    "    bin_width = width_scale * 10**(-decimal_threshold)\n",
    "    bins = np.arange(min(data), max(data) + bin_width, bin_width)\n",
    "    \n",
    "    counts, edges = np.histogram(data, bins=bins)\n",
    "    mode_bin_index = np.argmax(counts)\n",
    "    \n",
    "    # Identify background bin range\n",
    "    bg_min = edges[mode_bin_index]\n",
    "    bg_max = edges[mode_bin_index + 1]\n",
    "    \n",
    "    # Filter out background values\n",
    "    foreground = data[(data < bg_min) | (data > bg_max)]\n",
    "    \n",
    "    return foreground, (bg_min, bg_max), bin_width, bins\n",
    "\n",
    "# Apply filtering\n",
    "# filtered_data, (bg_min, bg_max), bin_width, bins_used = filter_background_by_precision(data)\n",
    "\n",
    "# # Plot histogram: Before and After filtering\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
    "\n",
    "# # Original\n",
    "# axs[0].hist(data, bins=bins_used, color='gray', edgecolor='black')\n",
    "# axs[0].set_title(\"Original Data Histogram\")\n",
    "# axs[0].set_xlabel(\"RI Value\")\n",
    "# axs[0].set_ylabel(\"Frequency\")\n",
    "# axs[0].axvspan(bg_min, bg_max, color='red', alpha=0.3, label='Filtered Background')\n",
    "# axs[0].legend()\n",
    "\n",
    "# # Filtered\n",
    "# axs[1].hist(filtered_data, bins=bins_used, color='blue', edgecolor='black')\n",
    "# axs[1].set_title(\"Filtered Data Histogram\")\n",
    "# axs[1].set_xlabel(\"RI Value\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.ndimage import label\n",
    "from path_manager import addpath    \n",
    "paths = addpath()\n",
    "\n",
    "from listspecificfiles import readlistFiles\n",
    "data_path = r\"data\\raw_npyData\"  # relative path with r\"relativepath\"\n",
    "files = readlistFiles(data_path,'.npy')\n",
    "fpath = files.file_with_Path()  # file with path name.  \n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path.cwd().parent    \n",
    "print(f\"BASE_DIR:--------> {BASE_DIR}\")\n",
    "save_dir = BASE_DIR/\"results\"/\"peakDetect_thresBackground\"\n",
    "os.makedirs(save_dir,exist_ok=True)  # Create if missing    \n",
    "\n",
    "# binning_method = input(\"Choose binning method (sturges, doanes, freedman-diaconis, auto): \").strip().lower()\n",
    "\n",
    "countfile = 0\n",
    "for FileWithPath in fpath:\n",
    "    countfile +=1\n",
    "    \n",
    "    print(f\"FileWithPath: {FileWithPath} and countfile: {countfile}\")\n",
    "    filename = os.path.basename(FileWithPath)\n",
    "    if countfile >= 3 and countfile <= 5:\n",
    "        print(f\"ignored File : {filename}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing filename: {filename}\")\n",
    "    # Load and flatten data\n",
    "    data = np.load(FileWithPath)\n",
    "    data = data.flatten()\n",
    "    print(f\"Shape of data: {data.shape[0]}\")\n",
    "    \n",
    "    # Call the function to analyze the data and save the histogram\n",
    "    # save_path = os.path.join(save_dir, f\"{filename[:-4]}_peakBack.png\")\n",
    "    \n",
    "    # range_start, range_end, threshold, peak_label = Peak_Detect_BackgroundThreshold(data, save_path=save_path)\n",
    "    peak_range(data, approx_bin_width=0.0001)\n",
    "\n",
    "    # filtered_data, (bg_min, bg_max), bin_width, bins_used = filter_background_by_precision(data)\n",
    "       \n",
    "    # # Compute max frequencies separately for y-axis limit\n",
    "    # original_counts, _ = np.histogram(data, bins=bins_used)\n",
    "    # filtered_counts, _ = np.histogram(filtered_data, bins=bins_used)\n",
    "\n",
    "    # original_ymax = original_counts.max()\n",
    "    # filtered_ymax = filtered_counts.max()\n",
    "\n",
    "    # # Add some padding\n",
    "    # original_ymax *= 1.1\n",
    "    # filtered_ymax *= 1.1\n",
    "\n",
    "    # # Plot histogram: Before and After filtering\n",
    "    # fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # # Original\n",
    "    # axs[0].hist(data, bins=bins_used, color='gray', edgecolor='black')\n",
    "    # axs[0].set_title(\"Data Histogram\")\n",
    "    # axs[0].set_xlabel(\"RI Value\")\n",
    "    # axs[0].set_ylabel(\"Frequency\")\n",
    "    # axs[0].axvspan(bg_min, bg_max, color='red', alpha=0.3, label='Filtered Background')\n",
    "    # axs[0].set_ylim(0, original_ymax)\n",
    "\n",
    "    # # Filtered\n",
    "    # axs[1].hist(filtered_data, bins=bins_used, color='blue', edgecolor='black')\n",
    "    # axs[1].set_title(\"Filtered Histogram\")\n",
    "    # axs[1].set_xlabel(\"RI Value\")\n",
    "    # axs[1].set_ylim(0, filtered_ymax)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UniqueValueCount\n",
    "# Counts_OFUniqueValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"min val: {min(significant_digit_data)} and max val :{max(significant_digit_data)},len: {len(significant_digit_data)}\")\n",
    "# most_common_digit = np.argmax(significant_digit_data) + 1\n",
    "# print(most_common_digit)\n",
    "# significant_digit_data[most_common_digit -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested working fin eto count the number after the decimal(howmany decimal place number exist in a number.) --> give the precision plac eof the given number. \n",
    "#  AS I HAVE Seen in Matlab , it was 16 digits precision in data values. upper method is good named as actual_significant_digits_after_decimal, as shown below also.\n",
    "# from decimal import Decimal\n",
    "# def actual_significant_digits_after_decimal(val):\n",
    "#     \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\"\"\"\n",
    "#     if val == 0:\n",
    "#         return 0\n",
    "#     d = Decimal(str(val)).normalize()\n",
    "#     if '.' not in str(d):\n",
    "#         return 0\n",
    "#     return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "# c =0\n",
    "# cdeclist =[]\n",
    "# for val in data:\n",
    "#     c +=1\n",
    "    \n",
    "#     strval = str(val)\n",
    "#     # rawstrip = strval.split('.')\n",
    "#     cdec1 = strval.split('.')[1].rstrip('0')\n",
    "#     # cdec0 = strval.split('.')[0].lstrip('0')\n",
    "#     # print(f\"cdecl: {cdec1} and cdec0:{cdec0} and original val: {strval}     {rawstrip}\")\n",
    "#     print(cdec1)\n",
    "#     cdeclist.append(len(cdec1))\n",
    "#     # print(f\"{val} and {type(val)}\")\n",
    "    \n",
    "#     # if len(cdec) >= 14:\n",
    "#     #     print(f\"decimal value till 7 place:{cdec}\")\n",
    "#     if c == 10:\n",
    "#         print(f\"decimal value till 7 place:{c}\")\n",
    "#         break\n",
    "    \n",
    "# print(f\"{cdeclist} and {type(cdeclist[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "# val = 1.25e-5\n",
    "val = 1.25001200100\n",
    "d = Decimal(str(val)).normalize()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.0**0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =\"1.334000000000000\"\n",
    "len(x)\n",
    "for val in data:\n",
    "    print\n",
    "x.split('.')[1].rstrip('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "def actual_significant_digits_after_decimal(val):\n",
    "    \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    d = Decimal(str(val)).normalize()\n",
    "    if '.' not in str(d):\n",
    "        return 0\n",
    "    return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "dec_place = []\n",
    "for val in data:\n",
    "      decimal_placesinEcahVal = actual_significant_digits_after_decimal(val)\n",
    "      dec_place.append(decimal_placesinEcahVal)\n",
    "      print(f\"decimal values:{decimal_placesinEcahVal}\")\n",
    "\n",
    "# data.shape\n",
    "# c =0\n",
    "# for val in data:\n",
    "#     c +=1\n",
    "#     print(f\"{val} and {type(val)}\")\n",
    "    \n",
    "#     if c == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_array = np.array([0.0122502, 1.2302355, -1.02415, 0.00034001, 10.000123])\n",
    "from decimal import Decimal\n",
    "def actual_significant_digits_after_decimal(val):\n",
    "    \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    d = Decimal(str(val)).normalize()\n",
    "    if '.' not in str(d):\n",
    "        return 0\n",
    "    return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "dec_place = []\n",
    "for val in float_array:\n",
    "      decimal_placesinEcahVal = actual_significant_digits_after_decimal(val)\n",
    "      dec_place.append(decimal_placesinEcahVal)\n",
    "      print(f\"decimal values:{decimal_placesinEcahVal}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dec_place)\n",
    "plt.title('hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 123.782\n",
    "print(f\"val : {val:.9e}\")\n",
    "# res= significant_digits(val)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_decimal_precision_npy(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    print(folder_path)\n",
    "    result_rows = []\n",
    "\n",
    "    def decimal_places(val):\n",
    "        s = f\"{val:.18f}\".rstrip('0')\n",
    "        if '.' in s:\n",
    "            return len(s.split('.')[1])\n",
    "        return 0\n",
    "\n",
    "    def significant_digits(val):\n",
    "        s = f\"{val:.18e}\".split('e')[0].replace('.', '').lstrip('0')\n",
    "        return len(s)\n",
    "\n",
    "    for file in folder.glob(\"*.npy\"):\n",
    "        data = np.load(file)\n",
    "        flat_data = data.flatten()\n",
    "        nonzero_data = flat_data[flat_data != 0]\n",
    "\n",
    "        if len(nonzero_data) == 0:\n",
    "            continue\n",
    "\n",
    "        dec_places_list = [decimal_places(v) for v in nonzero_data]\n",
    "        sig_digits_list = [significant_digits(v) for v in nonzero_data]\n",
    "\n",
    "        max_dec_places = max(dec_places_list)\n",
    "        max_sig_digits = max(sig_digits_list)\n",
    "\n",
    "        # Get values with the most decimal places\n",
    "        max_dec_values = nonzero_data[np.array(dec_places_list) == max_dec_places]\n",
    "\n",
    "        # Group by decimal place count\n",
    "        unique_dec_counts, dec_counts = np.unique(dec_places_list, return_counts=True)\n",
    "\n",
    "        # Prepare summary row\n",
    "        row = {\n",
    "            'File': file.name,\n",
    "            'Max Decimal Places': max_dec_places,\n",
    "            'Max Significant Digits': max_sig_digits,\n",
    "            'Total Elements': len(flat_data),\n",
    "            'Nonzero Elements': len(nonzero_data),\n",
    "            'Values with Max Decimal Places': \"; \".join([f\"{v:.18f}\" for v in max_dec_values[:3]]) + (\" ...\" if len(max_dec_values) > 3 else \"\")\n",
    "        }\n",
    "\n",
    "        # Add grouped counts\n",
    "        for d, count in zip(unique_dec_counts, dec_counts):\n",
    "            row[f\"Decimals={d} Count\"] = count\n",
    "\n",
    "        result_rows.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(result_rows)\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv = Path(BASE_DIR/\"results\"/\"decimal_precision_report.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    return df, output_csv\n",
    "\n",
    "# Run the function on a sample folder (adjust path accordingly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_folder = data_path\n",
    "df_result, csv_path = analyze_decimal_precision_npy(sample_folder)\n",
    "\n",
    "# import ace_tools as tools \n",
    "# tools.display_dataframe_to_user(name=\"Decimal Precision Analysis\", dataframe=df_result)\n",
    "# csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(5)\n",
    "x = [0,1,2,3,4,5]\n",
    "print(x)\n",
    "ll1 = [1,1,3,4,4,6,5,4,4,0,0,0,55,65]\n",
    "for val in ll1:\n",
    "    x.append(val)\n",
    "un1 = np.unique(x)\n",
    "count = np.unique_counts(x)\n",
    "print(f\"x as original:{x} \\n un1:{un1} and count:{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Compute the non-zero differences in sorted data\n",
    "    Plot a histogram of spacing (difference values)\n",
    "    Optionally zoom in on small-scale structure (e.g., 99th percentile or top N smallest)\n",
    " \"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_diff_distribution(data_array, file_label=\"Data\", show_log=False, zoom_percentile=None):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of spacing (differences) in sorted data.\n",
    "\n",
    "    Parameters:\n",
    "        data_array (np.ndarray): Flattened array of values.\n",
    "        file_label (str): Label for title or saving (default \"Data\").\n",
    "        show_log (bool): If True, plot x-axis in log scale.\n",
    "        zoom_percentile (float): Zoom into differences below this percentile (e.g., 99.0).\n",
    "    \"\"\"\n",
    "    # Flatten and sort\n",
    "    sorted_data = np.sort(data_array.flatten())\n",
    "\n",
    "    # Compute differences\n",
    "    diffs = np.diff(sorted_data)\n",
    "\n",
    "    # Filter out near-zero diffs (due to float precision or repeats)\n",
    "    diffs = diffs[np.abs(diffs) > 1e-12]\n",
    "\n",
    "    # Optional zoom\n",
    "    if zoom_percentile:\n",
    "        cutoff = np.percentile(diffs, zoom_percentile)\n",
    "        diffs = diffs[diffs <= cutoff]\n",
    "        print(f\"Zooming into differences <= {cutoff:.9f} (percentile {zoom_percentile})\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(diffs, bins=100, color='skyblue', edgecolor='gray')\n",
    "    plt.title(f\"Histogram of Value Differences ({file_label})\")\n",
    "    plt.xlabel(\"Spacing Between Consecutive Values\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    if show_log:\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Log-scaled Spacing\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from listspecificfiles import readlistFiles\n",
    "fpaths = readlistFiles(data_path,'.npy').file_with_Path()\n",
    "for fpath in fpaths:\n",
    "    data_array = np.load(fpath)\n",
    "    plot_diff_distribution(data_array, file_label=\"Data\", show_log=False, zoom_percentile=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape\n",
    "from posixpath import basename\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# pathModule = BASE_DIR/\"src\"/\"modules\"\n",
    "# sys.path.append(str(pathModule))\n",
    "# print(f\"pathofmodules:{pathModule}\")\n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "\n",
    "from listspecificfiles import*\n",
    "Relative_data_path = r\"data\\raw_npyData\"\n",
    "\n",
    "# Relative_data_path = os.path.normpath()\n",
    "# files = readlistFiles(Relative_data_path,'.npy')\n",
    "# print(f\"full output: {readlistFiles(Relative_data_path,'.npy').file_with_Path()}\")\n",
    "fpath = readlistFiles(Relative_data_path,'.npy').file_with_Path() \n",
    "for file in fpath:\n",
    "    data = np.load(file)\n",
    "    print(file)\n",
    "    uniquedata = np.unique_counts(data)\n",
    "    print(f\"data shape : {data.shape} \\n unique Count: {uniquedata.counts} \\n unique_values: {uniquedata.values} and \\n now see the differencs: { data.shape[0] - uniquedata.counts.shape[0]} \")\n",
    "    # datadic = {f\"{basename[:-4]}_shape\":{data.shape}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_valuesInData = data[data == 1.33] ; print(f\"total values in this:{most_valuesInData.shape} and data : {data.shape} \");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hybrid_kmeans_dbscan.py\n",
    "\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import scipy.io as sio\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # THRESHOLD_VALUE = 1.334\n",
    "# def load_volume(filepath,THRESHOLD_VALUE):\n",
    "#     if filepath.endswith('.npy'):\n",
    "#         volume = np.load(filepath)\n",
    "#         volume[volume <= THRESHOLD_VALUE] = 0  # Threshold to remove background\n",
    "#     elif filepath.endswith('.mat'):\n",
    "#         mat = sio.loadmat(filepath)\n",
    "#         # Assuming your volume variable is named 'volume' in .mat\n",
    "#         volume = next(v for v in mat.values() if isinstance(v, np.ndarray) and v.ndim == 3)\n",
    "#         volume[volume <= THRESHOLD_VALUE] = 0  # Threshold to remove background\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported file format. Use .mat or .npy\")\n",
    "#     return volume\n",
    "\n",
    "# def extract_features(volume):\n",
    "#     coords = np.array(np.nonzero(volume)).T\n",
    "#     intensities = volume[volume > 0].flatten().reshape(-1, 1)\n",
    "#     return np.hstack((coords, intensities))\n",
    "\n",
    "# def run_kmeans(X_scaled, n_clusters=4):\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "#     return kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# def run_dbscan_per_cluster(X_scaled, kmeans_labels, eps=0.6, min_samples=5):\n",
    "#     final_labels = -np.ones(len(X_scaled), dtype=int)\n",
    "#     label_offset = 0\n",
    "#     for cluster_id in np.unique(kmeans_labels):\n",
    "#         indices = np.where(kmeans_labels == cluster_id)[0]\n",
    "#         db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#         sub_labels = db.fit_predict(X_scaled[indices])\n",
    "#         sub_labels[sub_labels != -1] += label_offset\n",
    "#         final_labels[indices] = sub_labels\n",
    "#         label_offset += sub_labels.max() + 1 if sub_labels.max() != -1 else 0\n",
    "#     return final_labels\n",
    "\n",
    "# def save_results(output_dir, labels, coords):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     np.save(os.path.join(output_dir, \"cluster_labels.npy\"), labels)\n",
    "#     sio.savemat(os.path.join(output_dir, \"cluster_labels.mat\"), {\"labels\": labels})\n",
    "#     np.save(os.path.join(output_dir, \"voxel_coords.npy\"), coords)\n",
    "\n",
    "\n",
    "# def plot_clusters(coords, labels, title=\"Cluster Visualization\"):\n",
    "#     fig = plt.figure(figsize=(10, 7))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "#     scatter = ax.scatter(coords[:, 0], coords[:, 1], coords[:, 2], c=labels, cmap='tab20', s=2)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar(scatter)\n",
    "#     plt.show()\n",
    "\n",
    "# # # Example use:\n",
    "# volume = load_volume(\"yourfile.mat\")\n",
    "# X = extract_features(volume)\n",
    "# X_scaled = StandardScaler().fit_transform(X)\n",
    "# kmeans_labels = run_kmeans(X_scaled, n_clusters=4)\n",
    "# final_labels = run_dbscan_per_cluster(X_scaled, kmeans_labels)\n",
    "# plot_clusters(X[:, :3], final_labels)\n",
    "# save_results(\"output_dir\", final_labels, X[:, :3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "# from basicstatics import basicstat\n",
    "from scipy.io import savemat\n",
    "\n",
    "from pathlib import Path\n",
    "from path_manager import AddPath\n",
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\")\n",
    "from createmat2npyViceVersa import npy2mat \n",
    "\n",
    "datapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\"\n",
    "outputdatapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\\convertedmatfiles\"\n",
    "\n",
    "npy2mat(datapath=datapath,outputdatapath=outputdatapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I've created a working .py script that:\n",
    "-  #  load either .mat or .npy files,\n",
    "-  # Applies K-Means followed by DBSCAN\n",
    "-  #  Visualizes clusters using matplotlib in 3D\n",
    "-  #  Saves cluster labels and voxel coordinates to both .npy and .mat formats\n",
    "-  # Created an output directory automatically for saved files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybrid_kmeans_dbscan import *\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "SRCFILES  = Path.cwd().parent\n",
    "DATAFILES  = SRCFILES/\"data\"/\"raw_npyData\"\n",
    "\n",
    "listfiles = os.listdir(str(DATAFILES))\n",
    "Datafile = random.choice(listfiles)\n",
    "\n",
    "print(f\" --------------->  check step by step : SRCFILES: {SRCFILES} --> DATAFILES: {DATAFILES} --> \\n listfiles: {listfiles} \\n --> Datafile random choice: {Datafile}\")\n",
    "# Datafile = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\\Tomogramma_BuddingYeastCell.mat\"\n",
    "FileCompletePath = os.path.join(DATAFILES,Datafile)\n",
    "THRESHOLD_VALUE = 1.334\n",
    "\n",
    "volume = load_volume(FileCompletePath,THRESHOLD_VALUE)  # or .npy\n",
    "\n",
    "X = extract_features(volume)\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "kmeans_labels = run_kmeans(X_scaled, n_clusters=4)\n",
    "final_labels = run_dbscan_per_cluster(X_scaled, kmeans_labels, eps=0.6, min_samples=20)\n",
    "\n",
    "plot_clusters(X[:, :3], final_labels, title=\"Final Clusters\")\n",
    "\n",
    "save_results(\"clustering_output\", final_labels, X[:, :3])\n",
    "\n",
    "# for invoking the open3d plot function ------>\n",
    "# SRCFILES  = Path.cwd()\n",
    "# RESFilesINsrc = SRCFILES/\"clustering_output/\"\n",
    "# print(RESFilesINsrc)\n",
    " \n",
    "# clusteredNPY_Path = os.path.join(str(RESFilesINsrc),'cluster_labels.npy')\n",
    "# clusteredNPY_Coords = os.path.join(str(RESFilesINsrc),'voxel_coords.npy')\n",
    "\n",
    "# VisualizeOpen3d(clusteredNPY_Path,clusteredNPY_Coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordCluster = np.load(r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\\voxel_coords.npy\")\n",
    "clusterLabels = np.load(r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\\cluster_labels.npy\")\n",
    "# clusterLabels.shape\n",
    "print(f\"coord: {coordCluster.shape} and \\n {coordCluster[1:5,:]} \\n  and \\n cluster labels: {clusterLabels.shape}\\n {clusterLabels[1:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRCFILES  = Path.cwd()\n",
    "# RESFilesINsrc = SRCFILES/\"clustering_output/\"\n",
    "# print(RESFilesINsrc)\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SRCFILES  = Path.cwd()\n",
    "RESFilesINsrc = SRCFILES/\"clustering_output/\"\n",
    "print(RESFilesINsrc)\n",
    " \n",
    "clusteredNPY_Path = os.path.join(str(RESFilesINsrc),'cluster_labels.npy')\n",
    "clusteredNPY_Coords = os.path.join(str(RESFilesINsrc),'voxel_coords.npy')\n",
    "\n",
    "print(f\" here to check the final path : {clusteredNPY_Path},\\n --{clusteredNPY_Coords} <----------------------\\n\" )\n",
    "\n",
    "\n",
    "label_path = clusteredNPY_Path\n",
    "coord_path = clusteredNPY_Coords \n",
    "\n",
    "from meshvisClustCordLabels import *\n",
    "visualize_and_save_clusters(label_path, coord_path, output_dir=\"o3d_clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here below the code which test only k-means with coordinates and intensity as features save all parameters in .mat format  \n",
    "### in second run : I just consider the intesnsity values for k-means clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Clustering with K-Means + DBSCAN for 3D Volume Data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- USER OPTIONS ---\n",
    "\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "SRCFILES  = Path.cwd().parent\n",
    "# SRCFILES  = Path(__file__).resolve().parent.parent\n",
    "\n",
    "DATAFILES  = SRCFILES/\"data\"/\"raw_npyData\"\n",
    "\n",
    "listfiles = os.listdir(str(DATAFILES))\n",
    "\n",
    "for filename in listfiles:\n",
    "    if filename.endswith('.npy'):\n",
    "        DATAFILE_NAME = filename\n",
    "# Datafile = random.choice(listfiles)\n",
    "# DATAFILE_NAME = \"AML2_cell11.npy\"  # data\\raw_npyData\\tomo_Grafene_24h.npy\n",
    "# DATAFILE_NAME = \"tomo_Grafene_24h.npy\"  #  data\\raw_npyData\\tomo_Grafene_24h.npy\n",
    "        DATAFILES = str(DATAFILES)\n",
    "        input_path = os.path.join(DATAFILES,DATAFILE_NAME)\n",
    "        print(f\"input path : {input_path}\")\n",
    "        \n",
    "\n",
    "# for file in listfiles:\n",
    "#     if file == DATAFILE_NAME:\n",
    "#         DATAFILES = str(DATAFILES)\n",
    "#         input_path = os.path.join(DATAFILES,DATAFILE_NAME)\n",
    "#         print(f\"input path : {input_path}\")\n",
    "        \n",
    "# print(f\" --------------->  check step by step : SRCFILES: {SRCFILES} --> DATAFILES: {DATAFILES} --> \\n listfiles: {listfiles} \\n --> Datafile random choice: {Datafile}\")\n",
    "# Datafile = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\\Tomogramma_BuddingYeastCell.mat\"\n",
    "\n",
    "        # Choose input type\n",
    "        input_type = \"npy\"   #\"mat\"   or \"npy\"\n",
    "        input_path = input_path  # or .npy\n",
    "        volume_key = \"volume\"  # for .mat file: key inside the .mat dict\n",
    "\n",
    "        # output_dir = \"cluster_output\"\n",
    "        output_dir = SRCFILES/\"results\"/\"hybrid_Kdbcluster\"\n",
    "        kmeans_k = 7\n",
    "\n",
    "        # --- LOAD VOLUME DATA ---\n",
    "        if input_type == \"mat\":\n",
    "            mat_data = sio.loadmat(input_path)\n",
    "            volume = mat_data[volume_key]\n",
    "        elif input_type == \"npy\":\n",
    "            volume = np.load(input_path)\n",
    "            # to reduce the size of data volume.\n",
    "            # x_row,y_row,z_row = volume.shape\n",
    "            # volume = volume[:x_row/2,:y_row/2,:z_row/2]\n",
    "            # volume = volume[::5, ::5, ::5]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input_type. Choose 'mat' or 'npy'.\")\n",
    "\n",
    "        # --- EXTRACT NONZERO VOXELS AS POINT CLOUD ---\n",
    "        coords = np.array(np.nonzero(volume)).T  \n",
    "        # it will returns the coordinate of each nonzero values in volume and formate will be like this [[]\n",
    "        # coords =\n",
    "        # [z1,x1,y1]\n",
    "        # [z2,x2,y2]\n",
    "        # ........\n",
    "        # [zn,xn,yn]]\n",
    "\n",
    "        # intensities = volume[volume > 0].reshape(-1, 1)\n",
    "        intensities = volume[volume != 0].reshape(-1, 1)  # beacuse 48 hour data has some negative values.\n",
    "        # X = np.hstack((coords, intensities))  # shape: (N, 4)\n",
    "        X = np.hstack((coords,intensities))  # shape: (N, 4)\n",
    "        # np.hstack() horizontally stacks arrays (i.e., along columns / axis=1), meaning it concatenates them side by side. a = [[1],[2],[3]] , b = [[10],[20],[30]]\n",
    "        # np.hstack(a,b) --> results will be [[1,10],[2,20],[3,30]]\n",
    "        X1 = X[:,3]\n",
    "        X2 = X1.reshape(-1, 1)\n",
    "        # --- SCALE FEATURES ---\n",
    "        X_scaled = StandardScaler().fit_transform(X2)   # Z-score scaling/normalization -> zero mean, unit variance\n",
    "\n",
    "        # --- APPLY K-MEANS ---\n",
    "        kmeans = KMeans(n_clusters=kmeans_k,init = 'k-means++', random_state=42).fit(X_scaled)\n",
    "        # kmeans = KMeans(n_clusters=kmeans_k, init = 'k-means++', random_state=42).fit(X)\n",
    "        kmeans_labels = kmeans.labels_   # kmeans.labels_ --> kmeans_labels is one row (1xN) of labels (0,1,.., n_clusters -1) as output → array of cluster assignments for each data point, storing the cluster labels for all samples in the kmeans_labels variable, so you can use them later for saving or analyzing clusters / future use.\n",
    "\n",
    "        # <------------ For saving the k-means cluster and corresponding coordinates results -------- >\n",
    "        kmeans_coords_with_labels = np.hstack((coords, kmeans_labels.reshape(-1, 1)))  # [x, y, z, kmeans_label]\n",
    "        # Save as .npy\n",
    "        kmeans_intResultDir = os.path.join(output_dir,f\"kmIntensity{DATAFILE_NAME[:-4]}\")\n",
    "        os.makedirs(kmeans_intResultDir,exist_ok=True)\n",
    "        np.save(os.path.join(kmeans_intResultDir, \"kmeans_coords_labels.npy\"), kmeans_coords_with_labels)\n",
    "        # Save as .mat\n",
    "        sio.savemat(os.path.join(kmeans_intResultDir, \"kmeans_coords_labels.mat\"), {\"kmeans_coords_labels\": kmeans_coords_with_labels})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## here test the code at each satement one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "volume = np.array([\n",
    "    [[0.5, 1.4],\n",
    "     [1.0, 1.5]],\n",
    "\n",
    "    [[1.3, 1.2],\n",
    "     [1.6, 0]]\n",
    "])\n",
    "\n",
    "volume1 = volume.reshape(4,2)\n",
    "volume2 = volume1[:,1]\n",
    "print(f\"reshape: {volume1},\\n  and \\n {volume1[:,1]} and {volume2.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a small 3D array\n",
    "volume = np.array([\n",
    "    [[0.5, 1.4],\n",
    "     [1.0, 1.5]],\n",
    "\n",
    "    [[1.3, 1.2],\n",
    "     [1.6, 0]]\n",
    "])\n",
    "print(f\"size of vol: {volume.shape} \\n\")\n",
    "coords = np.array(np.nonzero(volume))\n",
    "print(f\"size of nonzeros : {np.nonzero(volume)} \\n\")\n",
    "#  here np.nonzero(volume) -> returns the (z,x,y) coordinates of all nonzero points --> \n",
    "#  output size of nonzeros : (array([0, 0, 0, 0, 1, 1, 1]), array([0, 0, 1, 1, 0, 0, 1]), array([0, 1, 0, 1, 0, 1, 0]))\n",
    "# coord:\n",
    "#  [[0 0 0 0 1 1 1]\n",
    "#  [0 0 1 1 0 0 1]\n",
    "#  [0 1 0 1 0 1 0]]\n",
    "\n",
    "coordst = np.array(np.nonzero(volume)).T\n",
    "print(f\"coord:\\n {coords}\\n  and \\n taranspose: \\n \\n {coordst}\")\n",
    "# taranspose: \n",
    "#  [[0 0 0]\n",
    "#  [0 0 1]\n",
    "#  [0 1 0]\n",
    "#  [0 1 1]\n",
    "#  [1 0 0]\n",
    "#  [1 0 1]\n",
    "#  [1 1 0]]\n",
    "\n",
    "# thres = 1.3\n",
    "# volume[volume <= thres] = 0\n",
    "# print(\"Original Volume:\\n\",volume)\n",
    "# volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create 3D grid coordinates (4,3,3) = 36 points\n",
    "x, y, z = np.meshgrid(np.arange(4), np.arange(3), np.arange(3), indexing='ij')\n",
    "coords = np.stack((x.ravel(), y.ravel(), z.ravel()), axis=1)\n",
    "\n",
    "# Random intensity values between 50 and 200\n",
    "intensity = np.random.uniform(50, 200, size=(coords.shape[0], 1))\n",
    "\n",
    "# Combine coords + intensity\n",
    "X = np.hstack((coords, intensity))\n",
    "\n",
    "# Apply standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Print first 5 for illustration\n",
    "print(f\"Original Data (X):\\n, {X[:5]}  and  {X.shape}\")\n",
    "print(\"\\nScaled Data (X_scaled):\\n\", X_scaled[:5])\n",
    "kmeans_k =3\n",
    "kmeans = KMeans(n_clusters=kmeans_k,init = 'k-means++',random_state=42).fit(X_scaled)\n",
    "# kmeans = KMeans(n_clusters=kmeans_k, init = 'k-means++', random_state=42).fit(X)\n",
    "kmeans_labels = kmeans.labels_   \n",
    "final_labels = -np.ones(len(X), dtype=int)  # Prepares an array to hold your final clustering labels. -1 means unassigned/outlier (just like DBSCAN does).Example: If X has 1000 points → final_labels = [-1, -1, ..., -1] (length 1000)\n",
    "\n",
    "label_offset = 0\n",
    "\n",
    "for cluster_id in np.unique(kmeans_labels):\n",
    "    print(f\"i have compl k-means now in dbsacn, cluster_id: {cluster_id}\")\n",
    "    indices = np.where(kmeans_labels == cluster_id)[0]  #  indices = np.where(kmeans_labels == cluster_id) --> indices returns tuple of array like -> (array([1, 4]),) to extract use [0] first array(np.where(kmeans_labels == cluster_id))[0] and get result like this # array([1, 4])\n",
    "    \n",
    "    X_sub = X_scaled[indices] # here extracting the coordinate and intensity value according to the cluster_id. [X_scaled size is: (N, 4)]\n",
    "    # X_sub = X[indices]\n",
    "\n",
    "    db = DBSCAN(eps= 0.8, min_samples=5).fit(X_sub)  # db scan here in each loop for each cluster further.\n",
    "    db_labels = db.labels_\n",
    "    db_labels[db_labels != -1] += label_offset\n",
    "    final_labels[indices] = db_labels\n",
    "    label_offset += db_labels.max() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create a moon-shaped dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.5, random_state=42)\n",
    "\n",
    "# Normalize\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Plot original data\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c='gray', edgecolor='k')\n",
    "plt.title(\"Input Data (Normalized)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=5)\n",
    "# labels = db.fit_predict(X_scaled)\n",
    "clusters = db.fit(X_scaled)\n",
    "db_labels = clusters.labels_\n",
    "l0 = []\n",
    "l1 = []\n",
    "for l in db_labels:\n",
    "    # print(f\"lables:{l}\")\n",
    "    if l == 0:\n",
    "        l0.append(int(l))\n",
    "    else:\n",
    "        l1.append(int(l))\n",
    "\n",
    "print(f\"l0 index:{l0} and \\n  l1 index:{l1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulab =np.unique(db_labels)\n",
    "print(ulab)\n",
    "label_offset =0 \n",
    "db_labels[db_labels != -1] += label_offset\n",
    "print(db_labels)\n",
    "# final_labels[indices] = db_labels\n",
    "label_offset += db_labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Labels assigned:\", np.unique(labels))\n",
    "print(\"Noise points (label == -1):\", list(labels).count(-1))\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='Set1', edgecolor='k')\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = np.load(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\clustering_output\\voxel_coords.npy\")\n",
    "print(dc.shape)\n",
    "# from scipy.io import loadmat\n",
    "# fd = loadmat(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\hybrid_Kdbcluster\\cluster_labels.mat\")\n",
    "# print(fd['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data1 = np.load(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\raw_npyData\\Tomogramma_BuddingYeastCell.npy\")\n",
    "data2 = data1[:50,:50,:50]\n",
    "size1 = data2.shape\n",
    "print(data2.shape[0],data2.shape[1],data2.shape[2],\"\\n --\" )\n",
    "print(f\"size: {size1[0]}\\n {size1[1]}\\n {size1[2]}\")\n",
    "print(f\"data1 shape: {data1.shape} \\n data2 shape: {data2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "kmeans_labels = np.array([0, 1, 0, 2, 1, 2, 0])\n",
    "cluster_id = 0\n",
    "\n",
    "indices = np.where(kmeans_labels == cluster_id)\n",
    "print(indices)  # (array([1, 4]),)\n",
    "\n",
    "indices = np.where(kmeans_labels == cluster_id)[0]\n",
    "print(indices)  # array([1, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# Adjust parsing to handle np.float64(...) formatting using regular expressions\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Reload the file after execution reset\n",
    "# txt_path = 'results\\featureQuantileThres\\AllFeatures_Stats.txt'\n",
    "# import os\n",
    "\n",
    "txt_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\featureQuantileThres\\AllFeatures_Stats.txt\"\n",
    "txt_path = os.path.normpath(txt_path)\n",
    "print(f\"txt_path: {txt_path}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "with open(txt_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if ':' in line:\n",
    "            name, data_str = line.split(':', 1)\n",
    "            # Replace np.float64(...) with float values using regex\n",
    "            cleaned_data_str = re.sub(r'np\\.float64\\((.*?)\\)', r'\\1', data_str.strip())\n",
    "            data_dict = ast.literal_eval(cleaned_data_str)\n",
    "            data_dict = {k: float(v) for k, v in data_dict.items()}\n",
    "            data_dict['Filename'] = name.strip()\n",
    "            rows.append(data_dict)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Reorder columns to start with Filename\n",
    "cols = ['Filename'] + [col for col in df.columns if col != 'Filename']\n",
    "df = df[cols]\n",
    "\n",
    "# Save as CSV\n",
    "BASE_DIR = Path.cwd().parent\n",
    "csv_path = BASE_DIR/ \"results\"/ \"featureQuantileThres\"\n",
    "\n",
    "csv_file = \"AllFeatures_Stats_Converted.csv\" # Save CSV locally\n",
    "csv_path = os.path.join(csv_path,csv_file)\n",
    "\n",
    "# df.to_csv(\"AllFeatures_Stats_Converted.csv\", index=False)\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"CSV file saved as AllFeatures_Stats_Converted.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def create_mesh_from_mask(data, mask, title=\"Mesh\", transparency=0.3):\n",
    "    if not np.any(mask):\n",
    "        print(f\"⚠️ No points in {title}. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        verts, faces, _, _ = marching_cubes(data * mask, level=0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Mesh creation failed for {title}: {e}\")\n",
    "        return\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.4, 0.6, 1.0])\n",
    "    mesh = mesh.filter_smooth_simple(number_of_iterations=1)\n",
    "\n",
    "    app = o3d.visualization.gui.Application.instance\n",
    "    app.initialize()\n",
    "    window = app.create_window(title, 1024, 768)\n",
    "    scene = o3d.visualization.rendering.Open3DScene(window.renderer)\n",
    "\n",
    "    mat = o3d.visualization.rendering.MaterialRecord()\n",
    "    mat.shader = \"defaultLitTransparency\"\n",
    "    mat.base_color = [0.4, 0.6, 1.0, transparency]\n",
    "    mat.base_roughness = 0.5\n",
    "\n",
    "    scene.add_geometry(\"mesh\", mesh, mat)\n",
    "    bbox = mesh.get_axis_aligned_bounding_box()\n",
    "    \n",
    "    center = bbox.get_center()\n",
    "    eye = center + np.array([0, 0, -1])  # Convert to NumPy array\n",
    "    up = [0, -1, 0]\n",
    "\n",
    "    scene.scene.camera.look_at(center, eye, up)\n",
    "\n",
    "    # scene.scene.camera.look_at(\n",
    "    # bbox.get_center(),           # center\n",
    "    # bbox.get_center() + [0, 0, -1],  # eye position\n",
    "    # [0, -1, 0]                   # up vector\n",
    "    # )\n",
    "\n",
    "    # scene.setup_camera(60, bbox, bbox.get_center())\n",
    "\n",
    "    def on_layout(context):\n",
    "        r = window.content_rect\n",
    "        scene.scene.set_viewport(r)\n",
    "\n",
    "    window.set_on_layout(on_layout)\n",
    "    app.run()\n",
    "\n",
    "def threshold_and_visualize(npy_file_path, threshold_val=1.20148978654012):\n",
    "    print(f\"CAN SEE THE NPYPATH : {npy_file_path}\")\n",
    "    # data = np.load(str(npy_file_path))\n",
    "    data = np.load(npy_file_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D numpy array.\")\n",
    "\n",
    "    print(f\"📁 Loaded: {npy_file_path.name} with shape {data.shape}\")\n",
    "    \n",
    "    mask_lower = data <= threshold_val\n",
    "    mask_upper = data > threshold_val\n",
    "\n",
    "    # create_mesh_from_mask(data, mask_lower, title=\"Lower Threshold Mesh\", transparency=0.1)\n",
    "    create_mesh_from_mask(data, mask_upper, title=\"Upper Threshold Mesh\", transparency=0.2)\n",
    "\n",
    "\n",
    "# ====== Replace this with your actual .npy file path ======\n",
    "# import os\n",
    "\n",
    "# filepath = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\raw_npyData\"\n",
    "# npy_path = os.path.join(filepath,'tomo_Grafene_24h.npy')\n",
    "# # npy_path = Path(\"path/to/your/datafile.npy\")  # 🛠️ Replace this path\n",
    "# threshold_and_visualize(npy_path, threshold_val=1.44148978654012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Safe path definition\n",
    "filepath = Path(r\"E:/Projects/substructure_3d_data/Substructure_Different_DataTypes/data/raw_npyData\")\n",
    "npy_path = filepath / \"tomo_Grafene_24h.npy\"\n",
    "\n",
    "threshold_and_visualize(npy_path, threshold_val=1.201978654012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "thres = 5\n",
    "array = np.random.randint(0,10,size=(5,5,5))\n",
    "print(array)\n",
    "data = array > thres\n",
    "print(f\"\\n data --> {thres} --> \\n {data} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "thres = 0.5\n",
    "array = np.random.rand(2,2,2)\n",
    "print(array)\n",
    "data = array > thres\n",
    "print(f\"\\n data --> {thres} --> \\n {data} \\n\")\n",
    "array = array\n",
    "print(f\"arraysize:{array.shape} \\n and \\n {array} \\n \" )\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from scipy.io import loadmat  # For .mat support\n",
    "# volume = np.load('data')\n",
    "verts,faces,_, _ = marching_cubes(data,level=0)\n",
    "mesh = o3d.geometry.TriangleMesh()\n",
    "mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "mesh.compute_vertex_normals()\n",
    "mesh.paint_uniform_color([0.6, 0.2, 1.0])\n",
    "\n",
    "# Visualize\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "# Simulated 3D data (a sphere)\n",
    "# x, y, z = np.indices((100, 100, 100))\n",
    "# sphere = (x - 50)**2 + (y - 50)**2 + (z - 50)**2\n",
    "# volume = np.exp(-sphere / 500)  # Smooth decay\n",
    "data = np.zeros((2, 3, 3))\n",
    "data[1, 1, 1] = 1.0 \n",
    "# data[1, 1, 2] = 1.5 \n",
    "# data[1, 2, 2] = 1.8\n",
    "\n",
    "print(data)\n",
    "# Threshold\n",
    "# threshold = 0.5\n",
    "# binary = volume > threshold\n",
    "\n",
    "# Mesh\n",
    "# verts, faces, _, _ = marching_cubes(volume * binary, level=0)\n",
    "verts, faces, _, _ = marching_cubes(data, level=0)\n",
    "mesh = o3d.geometry.TriangleMesh()\n",
    "mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "mesh.compute_vertex_normals()\n",
    "mesh.paint_uniform_color([0.6, 0.2, 1.0])\n",
    "\n",
    "# Visualize\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this below code is workin fine even if large data size is there. make a function or class of it , and use it when required.\n",
    "## but in line at this position just increase the size here: \n",
    "- if simplify:\n",
    "   - voxel_size = max(volume.shape) / 64  # can vary value from 64-128-256-512 etc to smooth the visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is workin fine even if large data size is there. make a function or class of it , and use it when required.\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be 3D\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, simplify=True):\n",
    "    # Automatically find threshold using Otsu’s method\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold used: {threshold:.4f}\")\n",
    "\n",
    "    # Marching cubes\n",
    "    print(\"[INFO] Extracting mesh using marching cubes...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Original mesh: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / 64  # Tweakable\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Simplified mesh: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    mesh.paint_uniform_color([0.6, 0.7, 1.0])\n",
    "    return mesh\n",
    "\n",
    "def visualize_mesh(mesh):\n",
    "#     o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "    o3d.visualization.draw_geometries([mesh], mesh_show_back_face=False)\n",
    "\n",
    "# ========== 🧪 Example ========== #\n",
    "# npy_path = \"path_to_your_large_3d_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, simplify=True)\n",
    "# visualize_mesh(mesh)\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "resultData = PROJECT_PATH/\"results\"\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "input_npy = npy_path  # <-- Replace with your actual file path\n",
    "# input_npy = os.listdir(input_npy)\n",
    "for filename  in os.listdir(input_npy):\n",
    "    if filename.endswith('.npy'):\n",
    "\n",
    "        print(f\"filename in the path : {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "    #     input_npy = input_npy/\"tomo_Grafene_24h.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "    #     volume = load_data(input_npy)\n",
    "        volume = load_data(str(npyfilePath))\n",
    "        mesh = create_mesh_from_volume(volume, simplify=True)\n",
    "        visualize_mesh(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(npy_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be a 3D numpy array\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, grid_factor=128, simplify=True, color_mode='gradient'):\n",
    "    \"\"\"\n",
    "        Enhancements:\n",
    "        Control grid resolution using grid_factor (affects voxel_size for simplification).\n",
    "        Color grading based on vertex Z-values (color_mode='gradient') or keep uniform (color_mode='uniform').\n",
    "        Clear toggles for both via function parameters.\n",
    "    \"\"\"\n",
    "    # Compute Otsu threshold from non-zero values\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold: {threshold:.4f}\")\n",
    "\n",
    "    # Generate mesh using marching cubes\n",
    "    print(\"[INFO] Extracting mesh...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Mesh before simplification: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Simplify mesh\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / grid_factor\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Mesh after simplification: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    # Apply color grading\n",
    "    if color_mode == 'gradient':\n",
    "        z_vals = np.asarray(mesh.vertices)[:, 2]\n",
    "        z_min, z_max = z_vals.min(), z_vals.max()\n",
    "        norm_z = (z_vals - z_min) / (z_max - z_min + 1e-8)\n",
    "        colors = np.stack([norm_z, 0.6 * np.ones_like(norm_z), 1.0 - norm_z], axis=1)\n",
    "        mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        mesh.paint_uniform_color([0.6, 0.7, 1.0])  # Default blueish\n",
    "\n",
    "    return mesh\n",
    "\n",
    "def visualize_mesh(mesh):\n",
    "    o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "\n",
    "# ========== 🧪 Example Usage ========== #\n",
    "# npy_path = \"path_to_your_large_3d_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=32, simplify=True, color_mode='gradient')\n",
    "# visualize_mesh(mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "resultData = PROJECT_PATH/\"results\"\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "input_npy = npy_path  # <-- Replace with your actual file path\n",
    "# input_npy = os.listdir(input_npy)\n",
    "for filename  in os.listdir(input_npy):\n",
    "    if filename.endswith('.npy'):\n",
    "\n",
    "        print(f\"filename in the path : {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "    #     input_npy = input_npy/\"tomo_Grafene_24h.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "    #     volume = load_data(input_npy)\n",
    "        volume = load_data(str(npyfilePath))\n",
    "        mesh = create_mesh_from_volume(volume, grid_factor=256, simplify=True, color_mode='gradient')\n",
    "        visualize_mesh(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  improvement using d solution , \n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be a 3D numpy array\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, grid_factor=32, simplify=True, color_mode='gradient'):\n",
    "    # Compute Otsu threshold from non-zero values\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold: {threshold:.4f}\")\n",
    "\n",
    "    # Generate mesh using marching cubes\n",
    "    print(\"[INFO] Extracting mesh...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Mesh before simplification: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Simplify mesh\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / grid_factor\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Mesh after simplification: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    # Apply color grading\n",
    "    if color_mode == 'gradient':\n",
    "        z_vals = np.asarray(mesh.vertices)[:, 2]\n",
    "        z_min, z_max = z_vals.min(), z_vals.max()\n",
    "        norm_z = (z_vals - z_min) / (z_max - z_min + 1e-8)\n",
    "        # Enhanced color gradient (red to green to blue)\n",
    "        colors = np.zeros((len(norm_z), 3))\n",
    "        colors[:, 0] = 1.0 - norm_z  # Red decreases with Z\n",
    "        colors[:, 1] = norm_z        # Green increases with Z\n",
    "        colors[:, 2] = norm_z        # Blue increases with Z\n",
    "        mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        mesh.paint_uniform_color([0.6, 0.7, 1.0])  # Default blueish\n",
    "\n",
    "    return mesh\n",
    "\n",
    "def visualize_mesh(mesh, transparency=0.5):\n",
    "    # Create visualizer with material properties\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    \n",
    "    # Add mesh with material properties\n",
    "    mat = o3d.visualization.rendering.MaterialRecord()\n",
    "    mat.shader = \"defaultLit\"\n",
    "    \n",
    "    # Modern Open3D versions use base_alpha instead of transparency\n",
    "        # Handle version compatibility\n",
    "    if hasattr(mat, 'transparency'):\n",
    "        mat.transparency = transparency\n",
    "    else:\n",
    "        mat.base_alpha = 1.0 - transparency\n",
    "    # mat.base_alpha = 1.0 - transparency  # Alpha is inverse of transparency\n",
    "    \n",
    "    # Additional material properties for better visualization\n",
    "    mat.base_roughness = 0.4\n",
    "    mat.base_metallic = 0.0\n",
    "    mat.base_color = [1.0, 1.0, 1.0, 1.0]  # RGBA\n",
    "    \n",
    "    vis.add_geometry(mesh, material=mat)\n",
    "    \n",
    "    # Configure render options\n",
    "    render_opt = vis.get_render_option()\n",
    "    render_opt.mesh_show_back_face = True\n",
    "    render_opt.mesh_show_wireframe = False\n",
    "    render_opt.light_on = True\n",
    "    render_opt.background_color = np.asarray([1.0, 1.0, 1.0])\n",
    "    render_opt.mesh_show_transparency = True  # Critical for transparency\n",
    "    \n",
    "    # Run visualization\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "# ========== Example Usage ========== #\n",
    "# npy_path = \"path_to_your_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=64, simplify=True, color_mode='gradient')\n",
    "# visualize_mesh(mesh, transparency=0.6)\n",
    "\n",
    "# ========== Example Usage ========== #\n",
    "# npy_path = \"path_to_your_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# Create mesh with 60% transparency\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=64, simplify=True, \n",
    "#                               color_mode='gradient', transparency=0.6)\n",
    "# visualize_mesh(mesh)\n",
    "\n",
    "    \n",
    "from pathlib import Path\n",
    "import os\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "datafile = npy_path/\"tomo_Grafene_24h.npy\"\n",
    "volume = load_data(str(datafile))\n",
    "# Create mesh first (without transparency)\n",
    "mesh = create_mesh_from_volume(volume, grid_factor=64, color_mode='gradient')\n",
    "\n",
    "# Then visualize with transparency\n",
    "transparency=0.6\n",
    "base_alpha = 1.0 - transparency \n",
    "# visualize_mesh(mesh, transparency=0.6)  # 60% transparent\n",
    "visualize_mesh(mesh, base_alpha)  # 60% transparent\n",
    "\n",
    "# mesh, alpha = create_mesh_from_volume(volume, grid_factor=64, simplify=True, color_mode='gradient', transparency=0.2)\n",
    "# visualize_mesh_with_transparency(mesh, transparency=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d;\n",
    "# print(f\"this is version of o3d: {o3d.__version__}\")\n",
    "print(o3d.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be a 3D numpy array\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, grid_factor=128, simplify=True, color_mode='gradient'):\n",
    "    \"\"\"\n",
    "        Enhancements:\n",
    "        Control grid resolution using grid_factor (affects voxel_size for simplification).\n",
    "        Color grading based on vertex Z-values (color_mode='gradient') or keep uniform (color_mode='uniform').\n",
    "        Clear toggles for both via function parameters.\n",
    "    \"\"\"\n",
    "    # Compute Otsu threshold from non-zero values\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold: {threshold:.4f}\")\n",
    "\n",
    "    # Generate mesh using marching cubes\n",
    "    print(\"[INFO] Extracting mesh...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Mesh before simplification: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Simplify mesh\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / grid_factor\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Mesh after simplification: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    # Apply color grading\n",
    "    if color_mode == 'gradient':\n",
    "        z_vals = np.asarray(mesh.vertices)[:, 2]\n",
    "        z_min, z_max = z_vals.min(), z_vals.max()\n",
    "        norm_z = (z_vals - z_min) / (z_max - z_min + 1e-8)\n",
    "        colors = np.stack([norm_z, 0.6 * np.ones_like(norm_z), 1.0 - norm_z], axis=1)\n",
    "        mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        mesh.paint_uniform_color([0.6, 0.7, 1.0])  # Default blueish\n",
    "\n",
    "    return mesh\n",
    "\n",
    "# def visualize_mesh(mesh):\n",
    "#     o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "\n",
    "# ========== 🧪 Example Usage ========== #\n",
    "# npy_path = \"path_to_your_large_3d_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=32, simplify=True, color_mode='gradient')\n",
    "# visualize_mesh(mesh)\n",
    "\n",
    "def visualize_mesh(mesh, transparency=0.5):\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Convert to TriangleMeshModel for transparency\n",
    "    mesh.material = o3d.visualization.rendering.MaterialRecord()\n",
    "    mesh.material.shader = \"defaultLitTransparency\"\n",
    "    mesh.material.base_color = [1.0, 0.6, 1.0, transparency]  # RGBA\n",
    "    vis.get_render_option().background_color = np.array([0, 0, 0])  # black background\n",
    "\n",
    "    vis = o3d.visualization.O3DVisualizer(\"Transparent Mesh Viewer\", 1024, 768)\n",
    "    vis.add_geometry(\"Mesh\", mesh, mesh.material)\n",
    "    vis.reset_camera_to_default()\n",
    "    vis.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy_path = \"path/to/your/3ddata.npy\"\n",
    "from pathlib import Path\n",
    "import os\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "datafile = npy_path/\"tomo_Grafene_24h.npy\"\n",
    "volume = load_data(str(datafile))\n",
    "mesh = create_mesh_from_volume(volume, grid_factor=64, simplify=True, color_mode='gradient')\n",
    "visualize_mesh(mesh, transparency=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# data = range(11)\n",
    "# data = list(data)\n",
    "# print(data)\n",
    "# quartileData= np.quantile(data,0.50)\n",
    "# print(quartileData)\n",
    "from pathlib import Path\n",
    "import os\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "resultData = PROJECT_PATH/\"results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.filters import threshold_otsu\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "def visualize_ostu(extract_data,npy_path):\n",
    "    # Extract coordinates of foreground for plotting\n",
    "    foreground_mask = extract_data\n",
    "    coords = np.argwhere(foreground_mask)\n",
    "    coords = coords[np.random.choice(len(coords), size=min(len(coords), 50000), replace=False)]\n",
    "\n",
    "    # Plot\n",
    "    fig = px.scatter_3d(\n",
    "        x=coords[:, 0], y=coords[:, 1], z=coords[:, 2],\n",
    "        opacity=0.008,\n",
    "        title=f\"Foreground Voxel Visualization ({os.path.basename(npy_path)})\",\n",
    "        labels={'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, resultDataPath, output_dir=\"otsu_results\"):\n",
    "    output_dir = os.path.join(resultDataPath,output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    extractedFilename = os.path.basename(npy_path) # return the filename as string.\n",
    "    # Load the 3D data\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    # Apply Otsu threshold\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    # Create masks\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    # Save masks\n",
    "    np.save(os.path.join(output_dir, f\"fgnd_mask{extractedFilename}.npy\"), foreground_mask)\n",
    "    np.save(os.path.join(output_dir, f\"bgnd_mask{extractedFilename}.npy\"), background_mask)\n",
    "\n",
    "    # # Extract coordinates of foreground for plotting\n",
    "    # coords = np.argwhere(foreground_mask)\n",
    "    # coords = coords[np.random.choice(len(coords), size=min(len(coords), 50000), replace=False)]\n",
    "\n",
    "    # # Plot\n",
    "    # fig = px.scatter_3d(\n",
    "    #     x=coords[:, 0], y=coords[:, 1], z=coords[:, 2],\n",
    "    #     opacity=0.5,\n",
    "    #     title=f\"Foreground Voxel Visualization ({os.path.basename(npy_path)})\",\n",
    "    #     labels={'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
    "    # )\n",
    "    # fig.show()\n",
    "\n",
    "    return threshold, foreground_mask, background_mask\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    # PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "    print(PROJECT_PATH)\n",
    "    resultData = PROJECT_PATH/\"results\"\n",
    "    npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "    # npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "    input_npy = npy_path  # <-- Replace with your actual file path\n",
    "    # input_npy = os.listdir(input_npy)\n",
    "    for filename  in os.listdir(input_npy):\n",
    "        print(f\"filename in the path : {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "\n",
    "        res = apply_otsu_segmentation(npyfilePath,resultData,output_dir=\"otsu_results\")\n",
    "        visualize_ostu(res[1],npyfilePath)\n",
    "        visualize_ostu(res[2],npyfilePath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary packages after kernel reset\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.filters import threshold_otsu\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, resultDataPath, output_dir=\"otsu_results\"):\n",
    "    output_dir = os.path.join(resultDataPath, output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    extractedFilename = os.path.basename(npy_path)\n",
    "    data = np.load(npy_path)\n",
    "\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    np.save(os.path.join(output_dir, f\"{extractedFilename}_fg_mask.npy\"), foreground_mask)\n",
    "    np.save(os.path.join(output_dir, f\"{extractedFilename}_bg_mask.npy\"), background_mask)\n",
    "\n",
    "    return threshold, foreground_mask, background_mask\n",
    "\n",
    "\n",
    "def visualize_foreground_background(fg_mask, bg_mask, title=\"3D Otsu Segmentation\", use_mesh=False, downsample=True, max_points=50000):\n",
    "    fg_coords = np.argwhere(fg_mask)\n",
    "    bg_coords = np.argwhere(bg_mask)\n",
    "\n",
    "    if downsample:\n",
    "        if len(fg_coords) > max_points:\n",
    "            fg_coords = fg_coords[np.random.choice(len(fg_coords), max_points, replace=False)]\n",
    "        if len(bg_coords) > max_points:\n",
    "            bg_coords = bg_coords[np.random.choice(len(bg_coords), max_points, replace=False)]\n",
    "\n",
    "    if not use_mesh:\n",
    "        fig = px.scatter_3d(\n",
    "            x=fg_coords[:, 0], y=fg_coords[:, 1], z=fg_coords[:, 2],\n",
    "            color=fg_coords[:, 2],\n",
    "            opacity=0.08,\n",
    "            color_continuous_scale=\"Blues\",\n",
    "            title=f\"{title} - Foreground\",\n",
    "            labels={'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
    "        )\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=bg_coords[:, 0], y=bg_coords[:, 1], z=bg_coords[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=1, opacity=0.01, color='gray'),\n",
    "            name='Background'\n",
    "        ))\n",
    "    else:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Mesh3d(\n",
    "            x=fg_coords[:, 0], y=fg_coords[:, 1], z=fg_coords[:, 2],\n",
    "            alphahull=5,\n",
    "            opacity=0.15,\n",
    "            color='lightblue',\n",
    "            name='Foreground (Mesh)'\n",
    "        ))\n",
    "        # fig.add_trace(go.Mesh3d(\n",
    "        #     x=bg_coords[:, 0], y=bg_coords[:, 1], z=bg_coords[:, 2],\n",
    "        #     alphahull=10,\n",
    "        #     opacity=0.02,\n",
    "        #     color='gray',\n",
    "        #     name='Background (Mesh)'\n",
    "        # ))\n",
    "        fig.update_layout(title=title, scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(margin=dict(l=0, r=0, t=40, b=0))\n",
    "    fig.show()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     from pathlib import Path\n",
    "#     import os\n",
    "\n",
    "#     PROJECT_PATH = Path.cwd().parent\n",
    "#     # PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "#     print(PROJECT_PATH)\n",
    "#     resultData = PROJECT_PATH/\"results\"\n",
    "#     npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "#     # npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "#     input_npy = npy_path  # <-- Replace with your actual file path\n",
    "#     # input_npy = os.listdir(input_npy)\n",
    "#     for filename  in os.listdir(input_npy):\n",
    "#         print(f\"filename in the path : {filename}\")\n",
    "#         # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "#         npyfilePath = input_npy/filename\n",
    "#         # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "\n",
    "#         res = apply_otsu_segmentation(npyfilePath,resultData,output_dir=\"otsu_results\")\n",
    "#         visualize_ostu(res[1],npyfilePath)\n",
    "#         visualize_ostu(res[2],npyfilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def threshold_and_visualize(npy_file_path, threshold_val=1.44148978654012):\n",
    "    print(f\"CAN SEE THE NPYPATH : {npy_file_path}\")\n",
    "    # data = np.load(str(npy_file_path))\n",
    "    data = np.load(npy_file_path)\n",
    "    print(f\"data shape --> {data.shape}\")\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D numpy array.\")\n",
    "\n",
    "    print(f\"📁 Loaded: {npy_file_path.name} with shape {data.shape}\")\n",
    "    \n",
    "#     mask_lower = data <= threshold_val\n",
    "#     data[data>threshold_val]\n",
    "#     mask_upper = data > threshold_val\n",
    "\n",
    "    \n",
    "    \n",
    "#     (mask_lower.fl), min(mask_lower),max(mask_upper), min(mask_upper)\n",
    "    return mask_lower,mask_upper\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Safe path definition\n",
    "filepath = Path(r\"E:/Projects/substructure_3d_data/Substructure_Different_DataTypes/data/raw_npyData\")\n",
    "npy_path = filepath / \"tomo_Grafene_24h.npy\"\n",
    "\n",
    "# from path_manager import AddPath\n",
    "# AddPath()\n",
    "import sys\n",
    "import os\n",
    "GARBAGE_PATH = Path.cwd()\n",
    "\n",
    "modulePath = GARBAGE_PATH/\"modules\"\n",
    "print(f\"module path  {modulePath}\")\n",
    "\n",
    "sys.path.append(str(modulePath))\n",
    "\n",
    "# for file in os.listdir(modulePath):\n",
    "#     if file.endswith('.py'):\n",
    "#         print(f\"filename : {file}\")\n",
    "    \n",
    "from plot3dint import plot3dinteractive\n",
    "\n",
    "\n",
    "\n",
    "res = threshold_and_visualize(npy_path, threshold_val=1.44148978654012)\n",
    "fg_mask = res[1]  # mask_upper\n",
    "bg_mask = res[0]  # mask_lower\n",
    "for RES in res:\n",
    "    val = np.array(RES)\n",
    "    val = val.flatten()\n",
    "    print(f\"max value: {max(val)} and min val:{min(val)} \\n\")\n",
    "# plot3dinteractive(bg_mask,\"upper\",sample_fraction=0.2)\n",
    "# visualize_foreground_background(fg_mask, bg_mask, title=\"3D Otsu Segmentation\", use_mesh=False, downsample=True, max_points=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "resultData = PROJECT_PATH/\"results\"\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "input_npy = npy_path  # <-- Replace with your actual file path\n",
    "# input_npy = os.listdir(input_npy)\n",
    "count = 0\n",
    "# filelistnName = np.random.choice(os.listdir(input_npy),5)\n",
    "# print(f\"randomly selected 5 files in --> {filelistnName}\")\n",
    "for filename in os.listdir(input_npy):\n",
    "# for filename in filelistnName:\n",
    "    if filename in ['tomo_Grafene_24h.npy','tomo_grafene_48h.npy']:\n",
    "        print(f\"filename in the path : and processing with it ---->  {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "    \n",
    "        # res = apply_otsu_segmentation(npyfilePath,resultData,output_dir=\"otsu_results\")\n",
    "        # visualize_ostu(res[1],npyfilePath)\n",
    "        # visualize_ostu(res[2],npyfilePath)\n",
    "        # apply_otsu_segmentation(npy_path, resultDataPath, output_dir=\"otsu_results\"):\n",
    "        threshold, fg_mask, bg_mask = apply_otsu_segmentation(npyfilePath, resultData,output_dir=\"otsu_results\")\n",
    "        visualize_foreground_background(fg_mask, bg_mask, title=f\"{filename[:-4]}\", use_mesh= True, downsample=True, max_points=60000)\n",
    "        # visualize_foreground_background(fg_mask, bg_mask, use_mesh=True, downsample=True)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        print(f\"couting the file processed  --> {count}\")\n",
    "    \n",
    "        if count == 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(5)\n",
    "# print(list(x))\n",
    "# for val in list(x):\n",
    "#     if val in [3,2]:\n",
    "#         print(f\"val is-->2,3 : {val}\")\n",
    "#     else:\n",
    "#         print(val)\n",
    "    \n",
    "# # if val in \n",
    "# # y = np.random.choice(x,3)\n",
    "# print(f\"{x}, y--> {y} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from skimage.filters import threshold_otsu\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation_with_cupy(npy_path, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    flat_data = cp.asarray(data[data > 0].flatten())\n",
    "    threshold = float(threshold_otsu(cp.asnumpy(flat_data)))\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    base_name = Path(npy_path).stem\n",
    "    np.save(output_dir / f\"{base_name}_fg_mask.npy\", foreground_mask)\n",
    "    np.save(output_dir / f\"{base_name}_bg_mask.npy\", background_mask)\n",
    "\n",
    "    return threshold, foreground_mask, background_mask\n",
    "\n",
    "\n",
    "def visualize_with_open3d(fg_mask, bg_mask, title=\"3D Visualization\", downsample=True, max_points=50000):\n",
    "    fg_coords = np.argwhere(fg_mask)\n",
    "    bg_coords = np.argwhere(bg_mask)\n",
    "\n",
    "    if downsample:\n",
    "        if len(fg_coords) > max_points:\n",
    "            fg_coords = fg_coords[np.random.choice(len(fg_coords), max_points, replace=False)]\n",
    "        if len(bg_coords) > max_points:\n",
    "            bg_coords = bg_coords[np.random.choice(len(bg_coords), max_points, replace=False)]\n",
    "\n",
    "    fg_pcd = o3d.geometry.PointCloud()\n",
    "    fg_pcd.points = o3d.utility.Vector3dVector(fg_coords)\n",
    "    fg_colors = np.tile([0.3, 0.5, 1.0], (fg_coords.shape[0], 1))\n",
    "    fg_pcd.colors = o3d.utility.Vector3dVector(fg_colors)\n",
    "\n",
    "    bg_pcd = o3d.geometry.PointCloud()\n",
    "    bg_pcd.points = o3d.utility.Vector3dVector(bg_coords)\n",
    "    bg_colors = np.tile([0.6, 0.6, 0.6], (bg_coords.shape[0], 1))\n",
    "    bg_pcd.colors = o3d.utility.Vector3dVector(bg_colors)\n",
    "\n",
    "    o3d.visualization.draw_geometries([fg_pcd, bg_pcd], window_name=title)\n",
    "\n",
    "\n",
    "def batch_otsu_segmentation(input_dir, output_dir, use_open3d=True, downsample=True, max_points=50000):\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    npy_files = list(input_dir.glob(\"*.npy\"))\n",
    "\n",
    "    print(f\"🔍 Found {len(npy_files)} .npy files in: {input_dir}\")\n",
    "\n",
    "    for file in npy_files:\n",
    "        print(f\"\\n🚀 Processing: {file.name}\")\n",
    "        try:\n",
    "            threshold, fg_mask, bg_mask = apply_otsu_segmentation_with_cupy(file, output_dir)\n",
    "            if use_open3d:\n",
    "                visualize_with_open3d(fg_mask, bg_mask, title=file.stem, downsample=downsample, max_points=max_points)\n",
    "            print(f\"✅ Finished {file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed {file.name}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_npy_path = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    result_path = PROJECT_PATH / \"results\" / \"otsu_gpu\"\n",
    "\n",
    "    batch_otsu_segmentation(\n",
    "        input_dir=input_npy_path,\n",
    "        output_dir=result_path,\n",
    "        use_open3d=True,\n",
    "        downsample=True,\n",
    "        max_points=50000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"script has been modified to include the following enhancements:\n",
    "\n",
    "User Options:\n",
    "\n",
    "Choose to visualize foreground, background, or both.\n",
    "\n",
    "Enable or disable saving of .obj files.\n",
    "\n",
    "Color Grading:\n",
    "\n",
    "Color intensity is based on voxel values using matplotlib color maps.\n",
    "\n",
    "Interactive Visualization:\n",
    "\n",
    "Foreground and background are visualized using Open3D with transparency and downsampling.\n",
    "\n",
    "Robust Saving:\n",
    "\n",
    "Saves .obj files for foreground/background point clouds if enabled.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from skimage.filters import threshold_otsu\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation_with_cupy(npy_path, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    flat_data = cp.asarray(data[data > 0].flatten())\n",
    "    threshold = float(threshold_otsu(cp.asnumpy(flat_data)))\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    base_name = Path(npy_path).stem\n",
    "    np.save(output_dir / f\"{base_name}_fg_mask.npy\", foreground_mask)\n",
    "    np.save(output_dir / f\"{base_name}_bg_mask.npy\", background_mask)\n",
    "\n",
    "    return threshold, data, foreground_mask, background_mask\n",
    "\n",
    "\n",
    "def visualize_with_open3d(fg_mask, bg_mask, data, title=\"3D Visualization\", show_fg=True, show_bg=False,\n",
    "                           downsample=True, max_points=50000, save_obj=False, obj_output_dir=None):\n",
    "    geometries = []\n",
    "\n",
    "    if show_fg:\n",
    "        fg_coords = np.argwhere(fg_mask)\n",
    "        if downsample and len(fg_coords) > max_points:\n",
    "            fg_coords = fg_coords[np.random.choice(len(fg_coords), max_points, replace=False)]\n",
    "        fg_values = data[tuple(fg_coords.T)]\n",
    "        fg_colors = plt.get_cmap(\"Blues\")((fg_values - fg_values.min()) / (np.ptp(fg_values) + 1e-6))[:, :3]\n",
    "        fg_pcd = o3d.geometry.PointCloud()\n",
    "        fg_pcd.points = o3d.utility.Vector3dVector(fg_coords)\n",
    "        fg_pcd.colors = o3d.utility.Vector3dVector(fg_colors)\n",
    "        geometries.append(fg_pcd)\n",
    "        if save_obj and obj_output_dir:\n",
    "            o3d.io.write_point_cloud(str(Path(obj_output_dir) / f\"{title}_foreground.obj\"), fg_pcd)\n",
    "\n",
    "    if show_bg:\n",
    "        bg_coords = np.argwhere(bg_mask)\n",
    "        if downsample and len(bg_coords) > max_points:\n",
    "            bg_coords = bg_coords[np.random.choice(len(bg_coords), max_points, replace=False)]\n",
    "        bg_values = data[tuple(bg_coords.T)]\n",
    "        bg_colors = plt.get_cmap(\"Greys\")((bg_values - bg_values.min()) / (np.ptp(bg_values) + 1e-6))[:, :3]\n",
    "        bg_pcd = o3d.geometry.PointCloud()\n",
    "        bg_pcd.points = o3d.utility.Vector3dVector(bg_coords)\n",
    "        bg_pcd.colors = o3d.utility.Vector3dVector(bg_colors)\n",
    "        geometries.append(bg_pcd)\n",
    "        if save_obj and obj_output_dir:\n",
    "            o3d.io.write_point_cloud(str(Path(obj_output_dir) / f\"{title}_background.obj\"), bg_pcd)\n",
    "\n",
    "    if geometries:\n",
    "        o3d.visualization.draw_geometries(geometries, window_name=title)\n",
    "\n",
    "\n",
    "def batch_otsu_segmentation(input_dir, output_dir, use_open3d=True, downsample=True, max_points=50000,\n",
    "                             show_fg=True, show_bg=False, save_obj=False):\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    npy_files = list(input_dir.glob(\"*.npy\"))\n",
    "\n",
    "    print(f\"🔍 Found {len(npy_files)} .npy files in: {input_dir}\")\n",
    "\n",
    "    for file in npy_files:\n",
    "        print(f\"\\n🚀 Processing: {file.name}\")\n",
    "        try:\n",
    "            threshold, data, fg_mask, bg_mask = apply_otsu_segmentation_with_cupy(file, output_dir)\n",
    "            if use_open3d:\n",
    "                visualize_with_open3d(\n",
    "                    fg_mask, bg_mask, data,\n",
    "                    title=file.stem,\n",
    "                    show_fg=show_fg,\n",
    "                    show_bg=show_bg,\n",
    "                    downsample=downsample,\n",
    "                    max_points=max_points,\n",
    "                    save_obj=save_obj,\n",
    "                    obj_output_dir=output_dir\n",
    "                )\n",
    "            print(f\"✅ Finished {file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed {file.name}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_npy_path = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    result_path = PROJECT_PATH / \"results\" / \"otsu_gpu\"\n",
    "\n",
    "    batch_otsu_segmentation(\n",
    "        input_dir=input_npy_path,\n",
    "        output_dir=result_path,\n",
    "        use_open3d=True,\n",
    "        downsample=True,\n",
    "        max_points=80000,\n",
    "        show_fg=True,\n",
    "        show_bg=False,\n",
    "        save_obj=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import marching_cubes\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Input must be a 3D array\")\n",
    "\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    fg_mask = data > threshold\n",
    "    bg_mask = ~fg_mask\n",
    "\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_fg_mask.npy\"), fg_mask)\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_bg_mask.npy\"), bg_mask)\n",
    "\n",
    "    return threshold, fg_mask, bg_mask, data\n",
    "\n",
    "\n",
    "def visualize_3d_marching_cubes(fg_mask, data, title, save_obj_path=None):\n",
    "    if not np.any(fg_mask):\n",
    "        print(\"⚠️ Foreground mask is empty. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    verts, faces, _, _ = marching_cubes(data * fg_mask, level=0)\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.2, 0.6, 1.0])\n",
    "\n",
    "    fg_points = np.argwhere(fg_mask)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(fg_points)\n",
    "    pcd.paint_uniform_color([1.0, 0.5, 0.0])\n",
    "\n",
    "    o3d.visualization.draw_geometries([pcd, mesh], window_name=title)\n",
    "\n",
    "    if save_obj_path:\n",
    "        o3d.io.write_triangle_mesh(save_obj_path, mesh)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_dir = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    output_dir = PROJECT_PATH / \"results\" / \"otsu_results\"\n",
    "\n",
    "    print(f\"Found {len(os.listdir(input_dir))} .npy files in: {input_dir}\\n\")\n",
    "\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".npy\"):\n",
    "            print(f\"\\n🚀 Processing: {file}\")\n",
    "            npy_path = input_dir / file\n",
    "            try:\n",
    "                threshold, fg_mask, bg_mask, data = apply_otsu_segmentation(npy_path, output_dir)\n",
    "                print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "                visualize_3d_marching_cubes(fg_mask, data, title=file, save_obj_path=output_dir / f\"{file[:-4]}_mesh.obj\")\n",
    "                print(f\"✅ Finished {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import marching_cubes\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Input must be a 3D array\")\n",
    "\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    fg_mask = data > threshold\n",
    "    bg_mask = ~fg_mask\n",
    "\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_fg_mask.npy\"), fg_mask)\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_bg_mask.npy\"), bg_mask)\n",
    "\n",
    "    return threshold, fg_mask, bg_mask, data\n",
    "\n",
    "\n",
    "def visualize_3d_marching_cubes_gui(fg_mask, data, title, save_obj_path=None, transparency=0.3):\n",
    "    if not np.any(fg_mask):\n",
    "        print(\"⚠️ Foreground mask is empty. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        verts, faces, _, _ = marching_cubes(data * fg_mask, level=0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Mesh creation failed: {e}\")\n",
    "        return\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.2, 0.6, 1.0])\n",
    "    mesh = mesh.filter_smooth_simple(number_of_iterations=1)\n",
    "\n",
    "    app = o3d.visualization.gui.Application.instance\n",
    "    app.initialize()\n",
    "    window = app.create_window(title, 1024, 768)\n",
    "    scene = o3d.visualization.rendering.Open3DScene(window.renderer)\n",
    "    mat = o3d.visualization.rendering.MaterialRecord()\n",
    "    mat.shader = \"defaultLitTransparency\"\n",
    "    mat.base_color = [0.2, 0.6, 1.0, transparency]\n",
    "    mat.base_roughness = 0.5\n",
    "    mat.point_size = 3\n",
    "\n",
    "    scene.add_geometry(\"mesh\", mesh, mat)\n",
    "    # Removed invalid call to set_background to fix compatibility with some Open3D versions\n",
    "    # scene.scene.set_background([1.0, 1.0, 1.0, 1.0])\n",
    "    bbox = mesh.get_axis_aligned_bounding_box()\n",
    "    scene.setup_camera(60, bbox, bbox.get_center())\n",
    "\n",
    "    def on_layout(context):\n",
    "        r = window.content_rect\n",
    "        scene.scene.set_viewport(r)\n",
    "\n",
    "    window.set_on_layout(on_layout)\n",
    "    app.run()\n",
    "\n",
    "    if save_obj_path:\n",
    "        o3d.io.write_triangle_mesh(str(save_obj_path), mesh)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_dir = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    output_dir = PROJECT_PATH / \"results\" / \"otsu_results\"\n",
    "\n",
    "    print(f\"Found {len(os.listdir(input_dir))} .npy files in: {input_dir}\\n\")\n",
    "\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".npy\"):\n",
    "            print(f\"\\n🚀 Processing: {file}\")\n",
    "            npy_path = input_dir / file\n",
    "            try:\n",
    "                threshold, fg_mask, bg_mask, data = apply_otsu_segmentation(npy_path, output_dir)\n",
    "                print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "                visualize_3d_marching_cubes_gui(\n",
    "                    fg_mask,\n",
    "                    data,\n",
    "                    title=file,\n",
    "                    save_obj_path=output_dir / f\"{file[:-4]}_mesh.obj\",\n",
    "                    transparency=0.2\n",
    "                )\n",
    "                print(f\"✅ Finished {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ Failed {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
