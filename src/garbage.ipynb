{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use Case:\n",
    "    1. Use It in Jupyter:\n",
    "                \n",
    "        from path_manager import addpath\n",
    "        paths = addpath()\n",
    "        # Use the returned dictionary if needed\n",
    "        print(\"Base dir:\", paths['BASE_DIR'])\n",
    "\n",
    "    2. Use in Python Script:\n",
    "        from path_manager import addpath\n",
    "        addpath()\n",
    "        # Then import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### below code , To save the plot and see the data distribution values. x_data = x-axis is created from the data values by uniformly distributting the data from min(data) to max(data)\n",
    "### we can have quick look of data arrangement , but there is drawback in plot that y-values if they are same should be look piled up on each other will look apart by a certain x-axis difference. so we will plot it differently : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "# print(f\"garbage path : {os.getcwd()} and \\n {Path.home()}\")\n",
    "\n",
    "# cws = Path(__file__).resolve().parent   # use it everywhere for current working scripts path (cws)\n",
    "cws = Path.cwd()\n",
    "BASE_DIR = cws.parent        # \\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\\n",
    "DATA_PATH = BASE_DIR/\"data\"  #  Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\\n",
    "rawnypyData_Path = DATA_PATH/\"raw_npyData\"\n",
    "data_dir = str(rawnypyData_Path)\n",
    "print(f\"data_dir : {data_dir}\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "save_dir = BASE_DIR/\"results\"/\"rawData_XYPlot\"\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create if missing\n",
    "\n",
    "plot_Type = int(input(\"enter value 0/1: for simple:0 Complex :1\"))\n",
    "t_start = time.time()\n",
    "\n",
    "if plot_Type == 0:\n",
    "   \n",
    "    for file in os.listdir(data_dir):\n",
    "\n",
    "        if file.endswith('.npy'):\n",
    "            filename = os.path.join(data_dir,file)\n",
    "            print(f\"filename:{file}\")\n",
    "            data = np.load(filename)\n",
    "            data = data.flatten().reshape(-1,1)\n",
    "\n",
    "            x_data = np.linspace(data.min(),data.max(),len(data)) # created the x_axis with it's min and max values and uniformly distributed between them.\n",
    "\n",
    "            # plot the data \n",
    "            # print(f\"data size: {data.shape}\")\n",
    "            plt.plot(x_data,data,'.', color = 'blue', markersize=0.4)\n",
    "            plt.title(f\"{file[:-4]} Linearly Spaced Vector \")\n",
    "            plt.xlabel('linearly spaced vector from data itself')\n",
    "            plt.ylabel('original Values')\n",
    "            plt.grid(True)\n",
    "            # plt.show()\n",
    "\n",
    "            save_dir = BASE_DIR/\"results\"/\"rawData_XYPlot\"\n",
    "            save_path = os.path.join(save_dir, f\"{file[:-4]}\")\n",
    "            if not save_path:\n",
    "                plt.savefig(save_path, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "if plot_Type == 1:\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.npy'):\n",
    "            filename = os.path.join(data_dir, file)\n",
    "            print(f\"filename: {file}\")\n",
    "\n",
    "            data = np.load(filename).flatten()\n",
    "\n",
    "            x_data = np.linspace(data.min(), data.max(), len(data))\n",
    "\n",
    "            # === 1. Find horizontal \"flat line\" band ===\n",
    "            # Detect most frequent value range using histogram\n",
    "            hist_vals, bin_edges = np.histogram(data, bins=10000)\n",
    "            dominant_bin_index = np.argmax(hist_vals)\n",
    "            bin_start = bin_edges[dominant_bin_index]\n",
    "            bin_end = bin_edges[dominant_bin_index + 1]\n",
    "\n",
    "            flat_band = data[(data >= bin_start) & (data < bin_end)]\n",
    "\n",
    "            if flat_band.size == 0:\n",
    "                print(\"No flat band detected.\")\n",
    "                continue\n",
    "\n",
    "            flat_min = np.min(flat_band)\n",
    "            flat_max = np.max(flat_band)\n",
    "            flat_mean = np.mean(flat_band)\n",
    "\n",
    "            # === 2. Plot with color-coded data ===\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            scatter = plt.scatter(x_data, data, c=data, cmap='viridis', s=0.5)\n",
    "\n",
    "            # === 3. Draw horizontal lines for flat values ===\n",
    "            flat_y_vals = [(\"Min\", flat_min, 'red'), \n",
    "                        (\"Mean\", flat_mean, 'orange'), \n",
    "                        (\"Max\", flat_max, 'green')]\n",
    "            used_y = []\n",
    "\n",
    "            for label, y_val, color in flat_y_vals:\n",
    "                plt.axhline(y=y_val, color=color, linestyle='--', linewidth=1)\n",
    "\n",
    "                # === 4. Auto offset to avoid text overlap ===\n",
    "                offset = 0\n",
    "                while any(abs((y_val + offset) - y) < 0.002 * (flat_max - flat_min) for y in used_y):\n",
    "                    offset += 0.002 * (flat_max - flat_min)\n",
    "                y_text = y_val + offset\n",
    "                used_y.append(y_text)\n",
    "\n",
    "                # Text with position\n",
    "                plt.text(x_data[0], y_text, f'{label}: {y_val:.8f}', \n",
    "                        color=color, fontsize=8, verticalalignment='bottom')\n",
    "\n",
    "            # === 5. Plot settings ===\n",
    "            plt.title(f\"{file[:-4]} Linearly Spaced Vector\")\n",
    "            plt.xlabel('Linearly spaced vector from data')\n",
    "            plt.ylabel('Original Values')\n",
    "            plt.grid(True)\n",
    "            cbar = plt.colorbar(scatter)\n",
    "\n",
    "            cbar.set_label('Intensity')\n",
    "\n",
    "            # Save the figure\n",
    "            plt.tight_layout()\n",
    "            save_path = os.path.join(save_dir, f\"{file[:-4]}_marked.png\")\n",
    "            if not save_path:\n",
    "                plt.savefig(save_path, dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "t_end = time.time()\n",
    "print(f\"Total Time --------------->: {t_end - t_start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding resolution of data, so not to skip small varaition, \n",
    "# First just plot Raw Data after making flat, if it is equal and more than 2 dimensions -> data = data.flatten()\n",
    "# x_data = np.linspace(min(data),max(data),len(data)), data = data.flatten() ---> y_data  =  sorted_data = np.sort(data)\n",
    "\n",
    "# binsize = diff(choosen smallest values after sorting the data) : data = data.flatten().reshape(-1,1)\n",
    "# 201*201*201 = 8120601; 200*200*200 = 8000000;\n",
    "# :.2e\t1.23e-05\t2 decimal places\n",
    "# :.8e\t1.23456000e-11\t8 decimal places\n",
    "# # Print with scientific notation and 8 decimal places\n",
    "# print(f\"{value:.8e}\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "from listspecificfiles import readlistFiles\n",
    "data_path = r\"data\\raw_npyData\"  # relative path with r\"\"relativepath\"\n",
    "files = readlistFiles(data_path,'.npy')\n",
    "fpath = files.file_with_Path()  # file with path name.\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path.cwd().parent\n",
    "print(f\"BASE_DIR:--------> {BASE_DIR}\")\n",
    "save_pathtTEXT  = BASE_DIR/\"results\"  \n",
    "\n",
    "################################ for simple plot of data ##############################################\n",
    "def simple_plot(data,file,savepathplot):\n",
    "    x_data = np.linspace(min(data),max(data),len(data))\n",
    "    # fig = plt.figure(figsize=(10,6))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_data,data,s=0.1)\n",
    "    plt.title(f\"{os.path.basename(file)[:-4]} sortred Data \")\n",
    "    plt.xlabel('Data: linearly spaced data itself')\n",
    "    plt.ylabel('Data:RI_value')\n",
    "    plt.grid(True)\n",
    "    save_dir = BASE_DIR/\"results\"/\"rawData_XYPlot\"/\"plotSortedData\"\n",
    "    os.makedirs(save_dir,exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f\"{os.path.basename(file)[:-4]}\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "#####################################################################################################\n",
    "\n",
    "textFilesSave = os.path.join(save_pathtTEXT,'details_Datafile.txt')   \n",
    "with open(textFilesSave,'w') as f:\n",
    "    f.close()\n",
    "# count = 0\n",
    "for file in fpath:\n",
    "    # count +=1\n",
    "    fileName = os.path.basename(file)\n",
    "    # print(f\"FullfilePath:{file} \\n fileName:{fileName}\")\n",
    "    data = np.load(file) \n",
    "    Data_shape = data.shape\n",
    "    data = data.flatten()\n",
    "    sorted_data = np.sort(data)\n",
    "    # simple_plot(sorted_data,file,BASE_DIR)\n",
    "\n",
    "    diff_1data = np.diff(sorted_data)\n",
    "    # diff_2data = np.diff(sorted_data)\n",
    "\n",
    "### ------------------------  found zeros in the diff1, Now filter zeros then go for first and second nonzero values --------------\n",
    "    # Filter out exact or near-zero differences to avoid duplicates\n",
    "    prcsn = 1e-9 # set precison  \n",
    "    nonzero_diffs = diff_1data[np.abs(diff_1data) > prcsn]\n",
    "\n",
    "    # Use np.unique to get distinct difference values\n",
    "    unique_diffs = np.unique(nonzero_diffs)\n",
    "\n",
    "    # Extract min and second min, keeping 9 decimal precision\n",
    "    # for unique_val in range(unique_diffs):\n",
    "    min_val = unique_diffs[0]\n",
    "    second_min_val = unique_diffs[1] if len(unique_diffs) > 1 else None\n",
    "\n",
    "    # Print values with high precision\n",
    "    print(f\"Minimum difference {fileName}  : {min_val:.12f} i.e. {min_val:.12e}\")\n",
    "    if second_min_val is not None:\n",
    "        print(f\"Second minimum difference {fileName}: {second_min_val:.12f} i.e. {second_min_val:.12e}\")\n",
    "    else:\n",
    "        print(\"No second unique non-zero difference found.\")\n",
    "\n",
    "    # print(f\"RESOLUTION_DATA: {min(diff_1data):.8f} DATA SIZE: {sorted_data.shape} \\n lower value:{sorted_data[0:3]} and upper value:{sorted_data[-4:]}\")\n",
    "    # text_data = f\"{fileName}: Data_shape:{Data_shape}, DATA SIZE: {sorted_data.shape}, RESOLUTION_DATA: {min(diff_1data)},lower value:{sorted_data[0:3]}, upper value:{sorted_data[-3:]}\\n\"\n",
    "    # with open(textFilesSave,'a') as f:\n",
    "    #     f.write(text_data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:(5,) \n",
      "\n",
      "a : [1 2 4 3 2] and its size: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4]),\n",
       " array([1, 2, 1, 1]),\n",
       " array([0, 1, 3, 2]),\n",
       " array([0, 1, 3, 2, 1]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# np.unique_counts(nonzero_diffs)\n",
    "a = np.array([1, 2, 4, 3, 2])\n",
    "# values,count = np.unique(a, return_counts=True) # return_counts -returns the unique value counts.\n",
    "# print(f\"a:{a.shape} \\n values:{values} \\n count:{count} \\n\")\n",
    "print(f\"a:{a.shape} \\n\")\n",
    "# a = np.array([1.3312498765,1.3312498761,1.3312498712,1.3312498763,1.3312498768,1.3312498765])\n",
    "# a  = np.sort(a)\n",
    "# a = list(a)\n",
    "f,count,inv,count1 = np.unique(np.round(a,decimals= 11), return_counts=True,return_inverse=True,return_index = True)\n",
    "# print(f\"a : {float(a)} and its size: {len(a)}\")\n",
    "print(f\"a : {a} and its size: {len(a)}\")\n",
    "\n",
    "# f,idx1,count = np.unique(a,return_index = True, return_counts=True)\n",
    "f,count1,count,inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1.01e-9\n",
    "print(f\"val:{x:.11f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = '1.000010200000'\n",
    "tt1 = str1.rstrip('0')   # eliminate the trailing zeros.\n",
    "tt1\n",
    "# 'Hello world'.title()  # 'Hello World'\n",
    "# \"they're bill's friends from the UK\".title() #\"They'Re Bill'S Friends From The Uk\"\n",
    "# \"42\".zfill(5)  # padding of total 5 including the \"42\", so three zeros (000) will be added at starting. \n",
    "str2 = 'janakak'\n",
    "str2.replace('k','R')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## - below code find the significant digits in my data and plot the histogram: \n",
    "##ðŸ’¡  it tells you how many digits are non-zero or meaningful in the floatâ€™s scientific representation.\n",
    "\n",
    "##ðŸ’¡ Itâ€™s a heuristic method â€” useful for checking how many digits \"matter\" in terms of numerical precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4]),\n",
       " array([0, 1, 3, 2]),\n",
       " array([0, 1, 3, 2, 1]),\n",
       " array([1, 2, 1, 1]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def UniqueValueCount(data):\n",
    "    \"\"\"\n",
    "    give your data in numpy array format 1d: \n",
    "    uniqValues,idx,inverse_idx,counts reurn values in this order\n",
    "    get your results:, it will return Total_res then extract these: uniqValues,idx,inverse_idx,counts\n",
    "    extract like this : uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    \"\"\"\n",
    "    significant_digit_data = data\n",
    "    # uniqValues,idx,inverse_idx,counts = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    Total_res = np.unique(significant_digit_data,return_counts=True,return_index=True,return_inverse=True)\n",
    "    uniqValues,idx,inverse_idx,counts = Total_res\n",
    "    return Total_res\n",
    "    print(f\"\\n unique values:{uniqValues},\\n  Unique counts: {counts} and \\n indexes:{idx}, \\n reverse_idx : {inverse_idx} and \\n orginal Array :{uniqValues[inverse_idx]}\")\n",
    "\n",
    "data = np.array([1,2,4,3,2])\n",
    "UniqueValueCount(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored file .: 1\n",
      "Ignored file .: 2\n",
      "Ignored file .: 3\n",
      "Ignored file .: 4\n",
      "Ignored file .: 5\n",
      "processing file no.: 6\n",
      "Processing filename: Tomogramma_Cell3.npy\n",
      "Shape of data: 8120601\n",
      "HERE precision  digits in the data values: 16\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored file .: 7\n",
      "Ignored file .: 8\n"
     ]
    }
   ],
   "source": [
    "# def significant_digits(val):\n",
    "#     s = f\"{val:.18e}\".split('e')[0].replace('.', '').lstrip('0')\n",
    "#     return len(s)\n",
    "\n",
    "#   useful for checking how many digits \"matter\" in terms of numerical precision.\n",
    "from  pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "from listspecificfiles import readlistFiles\n",
    "data_path = r\"data\\raw_npyData\"  # relative path with r\"\"relativepath\"\n",
    "files = readlistFiles(data_path,'.npy')\n",
    "fpath = files.file_with_Path()  # file with path name.\n",
    "\n",
    "# Directory to save histograms\n",
    "BASE_DIR = Path.cwd().parent\n",
    "save_dir = BASE_DIR/\"results\"/\"histogram_significantDigits\"\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "\n",
    "# def significant_digits(val):\n",
    "#     \"\"\"Extract first significant digit (ignores sign and zeros).\"\"\"\n",
    "#     val = float(val)\n",
    "#     if val == 0:\n",
    "#         return 0\n",
    "#     my_strVal = str(val)  \n",
    "#     return int(my_strVal.lstrip('-0.')[0])\n",
    "\n",
    "# def decimal_places(val):\n",
    "#         s = f\"{val:.18f}\".rstrip('0')\n",
    "#         if '.' in s:\n",
    "#             return len(s.split('.')[1])\n",
    "#         return 0\n",
    "\n",
    "from decimal import Decimal\n",
    "def actual_significant_digits_after_decimal(val):\n",
    "    \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    d = Decimal(str(val)).normalize()\n",
    "    if '.' not in str(d):\n",
    "        return 0\n",
    "    return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "countfile =0\n",
    "for FileWithPath in fpath:\n",
    "    countfile +=1\n",
    "    if countfile <=5 or countfile >=7 :\n",
    "        print(f\"Ignored file .: {countfile}\")\n",
    "        # break\n",
    "        continue\n",
    "    print(f\"processing file no.: {countfile}\")\n",
    "    # filename = os.path.basename(FileWithPath)\n",
    "    # print(f\"processing filename: {filename}\")\n",
    "    # data = np.load(FileWithPath)\n",
    "    # data = data.flatten().reshape(-1,1)\n",
    "    # # data = data.flatten()\n",
    "    # print(f\" shape of data: {data.shape[0]}\")\n",
    "    # significant_digit_data = []\n",
    "    # countloop = 0\n",
    "    # for val in data:\n",
    "    #     countloop +=1\n",
    "    #     # print(f\"val:{val} and Type: {type(val)} and extract val = {val[0]} and type of val[0] : {type(val[0])}\")\n",
    "    #     # output : val:[1.334] and Type: <class 'numpy.ndarray'> and extract val = 1.334 and type of val[0] : <class 'numpy.float64'>\n",
    "    #     val = val[0]\n",
    "    #     if countloop == 3:\n",
    "    #         break\n",
    "    #     significant_values = significant_digits(val)\n",
    "    #     if significant_values <= 9:\n",
    "    #         print(significant_values)\n",
    "    #     significant_digit_data.append(significant_values)\n",
    "\n",
    "    # plt.hist(significant_digit_data)\n",
    "    # plt.title(filename[:-4].title())\n",
    "    # plt.ylim([0,500])\n",
    "    # # save_path = save_dir/f\"{filename[:-4]}\"\n",
    "    # # plt.savefig(save_path,dpi=300)\n",
    "    # plt.show()        \n",
    "\n",
    "\n",
    "    filename = os.path.basename(FileWithPath)\n",
    "    print(f\"Processing filename: {filename}\")\n",
    "    \n",
    "    # Load and flatten data\n",
    "    data = np.load(FileWithPath)\n",
    "    data = data.flatten()\n",
    "    print(f\"Shape of data: {data.shape[0]}\")\n",
    "    \n",
    "    # Extract first significant digit\n",
    "    # significant_digit_data = [significant_digits(val) for val in data if val != 0]\n",
    "    significant_digit_data = [actual_significant_digits_after_decimal(val) for val in data if val != 0]\n",
    "\n",
    "    # Plot histogram\n",
    "    UniqueValues = UniqueValueCount(significant_digit_data)[0] \n",
    "    print(F\"HERE precision  digits in the data values: {max(UniqueValues)}\")\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    # counts, bins, _ = plt.hist(significant_digit_data, bins=np.arange(1, 11) - 0.5, edgecolor='black', rwidth=0.8)\n",
    "    plt.hist(significant_digit_data)\n",
    "    plt.title(f\"Significant Digit Histogram: {filename}\")\n",
    "    plt.xlabel(f\"Significant Digit (1-{max(UniqueValues)})\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    # plt.ylim([0,8000])\n",
    "    plt.xticks(range(1, max(UniqueValues)+1))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    # Save the figure\n",
    "    # hist_path = os.path.join(save_dir, f\"{filename[:-4]}_histogram.png\")\n",
    "    # plt.savefig(hist_path)\n",
    "    # plt.close()\n",
    "\n",
    "    # Optional: Print most common digit\n",
    "    # most_common_digit = np.argmax(counts) + 1  # bins are 1-indexed\n",
    "    # print(f\"Most frequent significant digit: {most_common_digit}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"min val: {min(significant_digit_data)} and max val :{max(significant_digit_data)},len: {len(significant_digit_data)}\")\n",
    "most_common_digit = np.argmax(significant_digit_data) + 1\n",
    "print(most_common_digit)\n",
    "significant_digit_data[most_common_digit -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334\n",
      "334\n",
      "334\n",
      "334\n",
      "334\n",
      "334\n",
      "334\n",
      "334\n",
      "334\n",
      "334\n",
      "decimal value till 7 place:10\n"
     ]
    }
   ],
   "source": [
    "# tested working fin eto count the number after the decimal(howmany decimal place number exist in a number.) --> give the precision plac eof the given number. \n",
    "#  AS I HAVE Seen in Matlab , it was 16 digits precision in data values.\n",
    "\n",
    "c =0\n",
    "cdeclist =[]\n",
    "for val in data:\n",
    "    c +=1\n",
    "    \n",
    "    strval = str(val)\n",
    "    # rawstrip = strval.split('.')\n",
    "    cdec1 = strval.split('.')[1].rstrip('0')\n",
    "    # cdec0 = strval.split('.')[0].lstrip('0')\n",
    "    # print(f\"cdecl: {cdec1} and cdec0:{cdec0} and original val: {strval}     {rawstrip}\")\n",
    "    print(cdec1)\n",
    "    cdeclist.append(len(cdec1))\n",
    "    # print(f\"{val} and {type(val)}\")\n",
    "    \n",
    "    # if len(cdec) >= 14:\n",
    "    #     print(f\"decimal value till 7 place:{cdec}\")\n",
    "    if c == 10:\n",
    "        print(f\"decimal value till 7 place:{c}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3] and <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"{cdeclist} and {type(cdeclist[1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =\"1.334000000000000\"\n",
    "len(x)\n",
    "for val in data:\n",
    "    print\n",
    "x.split('.')[1].rstrip('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "def actual_significant_digits_after_decimal(val):\n",
    "    \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    d = Decimal(str(val)).normalize()\n",
    "    if '.' not in str(d):\n",
    "        return 0\n",
    "    return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "dec_place = []\n",
    "for val in data:\n",
    "      decimal_placesinEcahVal = actual_significant_digits_after_decimal(val)\n",
    "      dec_place.append(decimal_placesinEcahVal)\n",
    "      print(f\"decimal values:{decimal_placesinEcahVal}\")\n",
    "\n",
    "# data.shape\n",
    "# c =0\n",
    "# for val in data:\n",
    "#     c +=1\n",
    "#     print(f\"{val} and {type(val)}\")\n",
    "    \n",
    "#     if c == 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m float_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.0122502\u001b[39m, \u001b[38;5;241m1.2302355\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.02415\u001b[39m, \u001b[38;5;241m0.00034001\u001b[39m, \u001b[38;5;241m10.000123\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdecimal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Decimal\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mactual_significant_digits_after_decimal\u001b[39m(val):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "float_array = np.array([0.0122502, 1.2302355, -1.02415, 0.00034001, 10.000123])\n",
    "from decimal import Decimal\n",
    "def actual_significant_digits_after_decimal(val):\n",
    "    \"\"\"Returns number of digits after the decimal in original float (ignoring trailing zeros).\"\"\"\n",
    "    if val == 0:\n",
    "        return 0\n",
    "    d = Decimal(str(val)).normalize()\n",
    "    if '.' not in str(d):\n",
    "        return 0\n",
    "    return len(str(d).split('.')[1].rstrip('0'))\n",
    "\n",
    "dec_place = []\n",
    "for val in float_array:\n",
    "      decimal_placesinEcahVal = actual_significant_digits_after_decimal(val)\n",
    "      dec_place.append(decimal_placesinEcahVal)\n",
    "      print(f\"decimal values:{decimal_placesinEcahVal}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dec_place)\n",
    "plt.title('hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 123.782\n",
    "print(f\"val : {val:.9e}\")\n",
    "# res= significant_digits(val)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def analyze_decimal_precision_npy(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    print(folder_path)\n",
    "    result_rows = []\n",
    "\n",
    "    def decimal_places(val):\n",
    "        s = f\"{val:.18f}\".rstrip('0')\n",
    "        if '.' in s:\n",
    "            return len(s.split('.')[1])\n",
    "        return 0\n",
    "\n",
    "    def significant_digits(val):\n",
    "        s = f\"{val:.18e}\".split('e')[0].replace('.', '').lstrip('0')\n",
    "        return len(s)\n",
    "\n",
    "    for file in folder.glob(\"*.npy\"):\n",
    "        data = np.load(file)\n",
    "        flat_data = data.flatten()\n",
    "        nonzero_data = flat_data[flat_data != 0]\n",
    "\n",
    "        if len(nonzero_data) == 0:\n",
    "            continue\n",
    "\n",
    "        dec_places_list = [decimal_places(v) for v in nonzero_data]\n",
    "        sig_digits_list = [significant_digits(v) for v in nonzero_data]\n",
    "\n",
    "        max_dec_places = max(dec_places_list)\n",
    "        max_sig_digits = max(sig_digits_list)\n",
    "\n",
    "        # Get values with the most decimal places\n",
    "        max_dec_values = nonzero_data[np.array(dec_places_list) == max_dec_places]\n",
    "\n",
    "        # Group by decimal place count\n",
    "        unique_dec_counts, dec_counts = np.unique(dec_places_list, return_counts=True)\n",
    "\n",
    "        # Prepare summary row\n",
    "        row = {\n",
    "            'File': file.name,\n",
    "            'Max Decimal Places': max_dec_places,\n",
    "            'Max Significant Digits': max_sig_digits,\n",
    "            'Total Elements': len(flat_data),\n",
    "            'Nonzero Elements': len(nonzero_data),\n",
    "            'Values with Max Decimal Places': \"; \".join([f\"{v:.18f}\" for v in max_dec_values[:3]]) + (\" ...\" if len(max_dec_values) > 3 else \"\")\n",
    "        }\n",
    "\n",
    "        # Add grouped counts\n",
    "        for d, count in zip(unique_dec_counts, dec_counts):\n",
    "            row[f\"Decimals={d} Count\"] = count\n",
    "\n",
    "        result_rows.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(result_rows)\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv = Path(BASE_DIR/\"results\"/\"decimal_precision_report.csv\")\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "    return df, output_csv\n",
    "\n",
    "# Run the function on a sample folder (adjust path accordingly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_folder = data_path\n",
    "df_result, csv_path = analyze_decimal_precision_npy(sample_folder)\n",
    "\n",
    "# import ace_tools as tools \n",
    "# tools.display_dataframe_to_user(name=\"Decimal Precision Analysis\", dataframe=df_result)\n",
    "# csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(5)\n",
    "x = [0,1,2,3,4,5]\n",
    "print(x)\n",
    "ll1 = [1,1,3,4,4,6,5,4,4,0,0,0,55,65]\n",
    "for val in ll1:\n",
    "    x.append(val)\n",
    "un1 = np.unique(x)\n",
    "count = np.unique_counts(x)\n",
    "print(f\"x as original:{x} \\n un1:{un1} and count:{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Compute the non-zero differences in sorted data\n",
    "    Plot a histogram of spacing (difference values)\n",
    "    Optionally zoom in on small-scale structure (e.g., 99th percentile or top N smallest)\n",
    " \"\"\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_diff_distribution(data_array, file_label=\"Data\", show_log=False, zoom_percentile=None):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of spacing (differences) in sorted data.\n",
    "\n",
    "    Parameters:\n",
    "        data_array (np.ndarray): Flattened array of values.\n",
    "        file_label (str): Label for title or saving (default \"Data\").\n",
    "        show_log (bool): If True, plot x-axis in log scale.\n",
    "        zoom_percentile (float): Zoom into differences below this percentile (e.g., 99.0).\n",
    "    \"\"\"\n",
    "    # Flatten and sort\n",
    "    sorted_data = np.sort(data_array.flatten())\n",
    "\n",
    "    # Compute differences\n",
    "    diffs = np.diff(sorted_data)\n",
    "\n",
    "    # Filter out near-zero diffs (due to float precision or repeats)\n",
    "    diffs = diffs[np.abs(diffs) > 1e-12]\n",
    "\n",
    "    # Optional zoom\n",
    "    if zoom_percentile:\n",
    "        cutoff = np.percentile(diffs, zoom_percentile)\n",
    "        diffs = diffs[diffs <= cutoff]\n",
    "        print(f\"Zooming into differences <= {cutoff:.9f} (percentile {zoom_percentile})\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(diffs, bins=100, color='skyblue', edgecolor='gray')\n",
    "    plt.title(f\"Histogram of Value Differences ({file_label})\")\n",
    "    plt.xlabel(\"Spacing Between Consecutive Values\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    if show_log:\n",
    "        plt.xscale('log')\n",
    "        plt.xlabel(\"Log-scaled Spacing\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from listspecificfiles import readlistFiles\n",
    "fpaths = readlistFiles(data_path,'.npy').file_with_Path()\n",
    "for fpath in fpaths:\n",
    "    data_array = np.load(fpath)\n",
    "    plot_diff_distribution(data_array, file_label=\"Data\", show_log=False, zoom_percentile=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.shape\n",
    "from posixpath import basename\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "# pathModule = BASE_DIR/\"src\"/\"modules\"\n",
    "# sys.path.append(str(pathModule))\n",
    "# print(f\"pathofmodules:{pathModule}\")\n",
    "\n",
    "from path_manager import addpath\n",
    "paths = addpath()\n",
    "\n",
    "from listspecificfiles import*\n",
    "Relative_data_path = r\"data\\raw_npyData\"\n",
    "\n",
    "# Relative_data_path = os.path.normpath()\n",
    "# files = readlistFiles(Relative_data_path,'.npy')\n",
    "# print(f\"full output: {readlistFiles(Relative_data_path,'.npy').file_with_Path()}\")\n",
    "fpath = readlistFiles(Relative_data_path,'.npy').file_with_Path() \n",
    "for file in fpath:\n",
    "    data = np.load(file)\n",
    "    print(file)\n",
    "    uniquedata = np.unique_counts(data)\n",
    "    print(f\"data shape : {data.shape} \\n unique Count: {uniquedata.counts} \\n unique_values: {uniquedata.values} and \\n now see the differencs: { data.shape[0] - uniquedata.counts.shape[0]} \")\n",
    "    # datadic = {f\"{basename[:-4]}_shape\":{data.shape}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_valuesInData = data[data == 1.33] ; print(f\"total values in this:{most_valuesInData.shape} and data : {data.shape} \");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hybrid_kmeans_dbscan.py\n",
    "\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import scipy.io as sio\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.cluster import KMeans, DBSCAN\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # THRESHOLD_VALUE = 1.334\n",
    "# def load_volume(filepath,THRESHOLD_VALUE):\n",
    "#     if filepath.endswith('.npy'):\n",
    "#         volume = np.load(filepath)\n",
    "#         volume[volume <= THRESHOLD_VALUE] = 0  # Threshold to remove background\n",
    "#     elif filepath.endswith('.mat'):\n",
    "#         mat = sio.loadmat(filepath)\n",
    "#         # Assuming your volume variable is named 'volume' in .mat\n",
    "#         volume = next(v for v in mat.values() if isinstance(v, np.ndarray) and v.ndim == 3)\n",
    "#         volume[volume <= THRESHOLD_VALUE] = 0  # Threshold to remove background\n",
    "#     else:\n",
    "#         raise ValueError(\"Unsupported file format. Use .mat or .npy\")\n",
    "#     return volume\n",
    "\n",
    "# def extract_features(volume):\n",
    "#     coords = np.array(np.nonzero(volume)).T\n",
    "#     intensities = volume[volume > 0].flatten().reshape(-1, 1)\n",
    "#     return np.hstack((coords, intensities))\n",
    "\n",
    "# def run_kmeans(X_scaled, n_clusters=4):\n",
    "#     kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "#     return kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# def run_dbscan_per_cluster(X_scaled, kmeans_labels, eps=0.6, min_samples=5):\n",
    "#     final_labels = -np.ones(len(X_scaled), dtype=int)\n",
    "#     label_offset = 0\n",
    "#     for cluster_id in np.unique(kmeans_labels):\n",
    "#         indices = np.where(kmeans_labels == cluster_id)[0]\n",
    "#         db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "#         sub_labels = db.fit_predict(X_scaled[indices])\n",
    "#         sub_labels[sub_labels != -1] += label_offset\n",
    "#         final_labels[indices] = sub_labels\n",
    "#         label_offset += sub_labels.max() + 1 if sub_labels.max() != -1 else 0\n",
    "#     return final_labels\n",
    "\n",
    "# def save_results(output_dir, labels, coords):\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     np.save(os.path.join(output_dir, \"cluster_labels.npy\"), labels)\n",
    "#     sio.savemat(os.path.join(output_dir, \"cluster_labels.mat\"), {\"labels\": labels})\n",
    "#     np.save(os.path.join(output_dir, \"voxel_coords.npy\"), coords)\n",
    "\n",
    "\n",
    "# def plot_clusters(coords, labels, title=\"Cluster Visualization\"):\n",
    "#     fig = plt.figure(figsize=(10, 7))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "#     scatter = ax.scatter(coords[:, 0], coords[:, 1], coords[:, 2], c=labels, cmap='tab20', s=2)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar(scatter)\n",
    "#     plt.show()\n",
    "\n",
    "# # # Example use:\n",
    "# volume = load_volume(\"yourfile.mat\")\n",
    "# X = extract_features(volume)\n",
    "# X_scaled = StandardScaler().fit_transform(X)\n",
    "# kmeans_labels = run_kmeans(X_scaled, n_clusters=4)\n",
    "# final_labels = run_dbscan_per_cluster(X_scaled, kmeans_labels)\n",
    "# plot_clusters(X[:, :3], final_labels)\n",
    "# save_results(\"output_dir\", final_labels, X[:, :3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "# from basicstatics import basicstat\n",
    "from scipy.io import savemat\n",
    "\n",
    "from pathlib import Path\n",
    "from path_manager import AddPath\n",
    "import sys\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\")\n",
    "from createmat2npyViceVersa import npy2mat \n",
    "\n",
    "datapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\"\n",
    "outputdatapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\\convertedmatfiles\"\n",
    "\n",
    "npy2mat(datapath=datapath,outputdatapath=outputdatapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I've created a working .py script that:\n",
    "-  #  load either .mat or .npy files,\n",
    "-  # Applies K-Means followed by DBSCAN\n",
    "-  #  Visualizes clusters using matplotlib in 3D\n",
    "-  #  Saves cluster labels and voxel coordinates to both .npy and .mat formats\n",
    "-  # Created an output directory automatically for saved files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hybrid_kmeans_dbscan import *\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "SRCFILES  = Path.cwd().parent\n",
    "DATAFILES  = SRCFILES/\"data\"/\"raw_npyData\"\n",
    "\n",
    "listfiles = os.listdir(str(DATAFILES))\n",
    "Datafile = random.choice(listfiles)\n",
    "\n",
    "print(f\" --------------->  check step by step : SRCFILES: {SRCFILES} --> DATAFILES: {DATAFILES} --> \\n listfiles: {listfiles} \\n --> Datafile random choice: {Datafile}\")\n",
    "# Datafile = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\\Tomogramma_BuddingYeastCell.mat\"\n",
    "FileCompletePath = os.path.join(DATAFILES,Datafile)\n",
    "THRESHOLD_VALUE = 1.334\n",
    "\n",
    "volume = load_volume(FileCompletePath,THRESHOLD_VALUE)  # or .npy\n",
    "\n",
    "X = extract_features(volume)\n",
    "\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "kmeans_labels = run_kmeans(X_scaled, n_clusters=4)\n",
    "final_labels = run_dbscan_per_cluster(X_scaled, kmeans_labels, eps=0.6, min_samples=20)\n",
    "\n",
    "plot_clusters(X[:, :3], final_labels, title=\"Final Clusters\")\n",
    "\n",
    "save_results(\"clustering_output\", final_labels, X[:, :3])\n",
    "\n",
    "# for invoking the open3d plot function ------>\n",
    "# SRCFILES  = Path.cwd()\n",
    "# RESFilesINsrc = SRCFILES/\"clustering_output/\"\n",
    "# print(RESFilesINsrc)\n",
    " \n",
    "# clusteredNPY_Path = os.path.join(str(RESFilesINsrc),'cluster_labels.npy')\n",
    "# clusteredNPY_Coords = os.path.join(str(RESFilesINsrc),'voxel_coords.npy')\n",
    "\n",
    "# VisualizeOpen3d(clusteredNPY_Path,clusteredNPY_Coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordCluster = np.load(r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\\voxel_coords.npy\")\n",
    "clusterLabels = np.load(r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\clustering_output\\cluster_labels.npy\")\n",
    "# clusterLabels.shape\n",
    "print(f\"coord: {coordCluster.shape} and \\n {coordCluster[1:5,:]} \\n  and \\n cluster labels: {clusterLabels.shape}\\n {clusterLabels[1:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SRCFILES  = Path.cwd()\n",
    "# RESFilesINsrc = SRCFILES/\"clustering_output/\"\n",
    "# print(RESFilesINsrc)\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SRCFILES  = Path.cwd()\n",
    "RESFilesINsrc = SRCFILES/\"clustering_output/\"\n",
    "print(RESFilesINsrc)\n",
    " \n",
    "clusteredNPY_Path = os.path.join(str(RESFilesINsrc),'cluster_labels.npy')\n",
    "clusteredNPY_Coords = os.path.join(str(RESFilesINsrc),'voxel_coords.npy')\n",
    "\n",
    "print(f\" here to check the final path : {clusteredNPY_Path},\\n --{clusteredNPY_Coords} <----------------------\\n\" )\n",
    "\n",
    "\n",
    "label_path = clusteredNPY_Path\n",
    "coord_path = clusteredNPY_Coords \n",
    "\n",
    "from meshvisClustCordLabels import *\n",
    "visualize_and_save_clusters(label_path, coord_path, output_dir=\"o3d_clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## here below the code which test only k-means with coordinates and intensity as features save all parameters in .mat format  \n",
    "### in second run : I just consider the intesnsity values for k-means clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Clustering with K-Means + DBSCAN for 3D Volume Data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- USER OPTIONS ---\n",
    "\n",
    "import os \n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "SRCFILES  = Path.cwd().parent\n",
    "# SRCFILES  = Path(__file__).resolve().parent.parent\n",
    "\n",
    "DATAFILES  = SRCFILES/\"data\"/\"raw_npyData\"\n",
    "\n",
    "listfiles = os.listdir(str(DATAFILES))\n",
    "\n",
    "for filename in listfiles:\n",
    "    if filename.endswith('.npy'):\n",
    "        DATAFILE_NAME = filename\n",
    "# Datafile = random.choice(listfiles)\n",
    "# DATAFILE_NAME = \"AML2_cell11.npy\"  # data\\raw_npyData\\tomo_Grafene_24h.npy\n",
    "# DATAFILE_NAME = \"tomo_Grafene_24h.npy\"  #  data\\raw_npyData\\tomo_Grafene_24h.npy\n",
    "        DATAFILES = str(DATAFILES)\n",
    "        input_path = os.path.join(DATAFILES,DATAFILE_NAME)\n",
    "        print(f\"input path : {input_path}\")\n",
    "        \n",
    "\n",
    "# for file in listfiles:\n",
    "#     if file == DATAFILE_NAME:\n",
    "#         DATAFILES = str(DATAFILES)\n",
    "#         input_path = os.path.join(DATAFILES,DATAFILE_NAME)\n",
    "#         print(f\"input path : {input_path}\")\n",
    "        \n",
    "# print(f\" --------------->  check step by step : SRCFILES: {SRCFILES} --> DATAFILES: {DATAFILES} --> \\n listfiles: {listfiles} \\n --> Datafile random choice: {Datafile}\")\n",
    "# Datafile = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\\Tomogramma_BuddingYeastCell.mat\"\n",
    "\n",
    "        # Choose input type\n",
    "        input_type = \"npy\"   #\"mat\"   or \"npy\"\n",
    "        input_path = input_path  # or .npy\n",
    "        volume_key = \"volume\"  # for .mat file: key inside the .mat dict\n",
    "\n",
    "        # output_dir = \"cluster_output\"\n",
    "        output_dir = SRCFILES/\"results\"/\"hybrid_Kdbcluster\"\n",
    "        kmeans_k = 7\n",
    "\n",
    "        # --- LOAD VOLUME DATA ---\n",
    "        if input_type == \"mat\":\n",
    "            mat_data = sio.loadmat(input_path)\n",
    "            volume = mat_data[volume_key]\n",
    "        elif input_type == \"npy\":\n",
    "            volume = np.load(input_path)\n",
    "            # to reduce the size of data volume.\n",
    "            # x_row,y_row,z_row = volume.shape\n",
    "            # volume = volume[:x_row/2,:y_row/2,:z_row/2]\n",
    "            # volume = volume[::5, ::5, ::5]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Invalid input_type. Choose 'mat' or 'npy'.\")\n",
    "\n",
    "        # --- EXTRACT NONZERO VOXELS AS POINT CLOUD ---\n",
    "        coords = np.array(np.nonzero(volume)).T  \n",
    "        # it will returns the coordinate of each nonzero values in volume and formate will be like this [[]\n",
    "        # coords =\n",
    "        # [z1,x1,y1]\n",
    "        # [z2,x2,y2]\n",
    "        # ........\n",
    "        # [zn,xn,yn]]\n",
    "\n",
    "        # intensities = volume[volume > 0].reshape(-1, 1)\n",
    "        intensities = volume[volume != 0].reshape(-1, 1)  # beacuse 48 hour data has some negative values.\n",
    "        # X = np.hstack((coords, intensities))  # shape: (N, 4)\n",
    "        X = np.hstack((coords,intensities))  # shape: (N, 4)\n",
    "        # np.hstack() horizontally stacks arrays (i.e., along columns / axis=1), meaning it concatenates them side by side. a = [[1],[2],[3]] , b = [[10],[20],[30]]\n",
    "        # np.hstack(a,b) --> results will be [[1,10],[2,20],[3,30]]\n",
    "        X1 = X[:,3]\n",
    "        X2 = X1.reshape(-1, 1)\n",
    "        # --- SCALE FEATURES ---\n",
    "        X_scaled = StandardScaler().fit_transform(X2)   # Z-score scaling/normalization -> zero mean, unit variance\n",
    "\n",
    "        # --- APPLY K-MEANS ---\n",
    "        kmeans = KMeans(n_clusters=kmeans_k,init = 'k-means++', random_state=42).fit(X_scaled)\n",
    "        # kmeans = KMeans(n_clusters=kmeans_k, init = 'k-means++', random_state=42).fit(X)\n",
    "        kmeans_labels = kmeans.labels_   # kmeans.labels_ --> kmeans_labels is one row (1xN) of labels (0,1,.., n_clusters -1) as output â†’ array of cluster assignments for each data point, storing the cluster labels for all samples in the kmeans_labels variable, so you can use them later for saving or analyzing clusters / future use.\n",
    "\n",
    "        # <------------ For saving the k-means cluster and corresponding coordinates results -------- >\n",
    "        kmeans_coords_with_labels = np.hstack((coords, kmeans_labels.reshape(-1, 1)))  # [x, y, z, kmeans_label]\n",
    "        # Save as .npy\n",
    "        kmeans_intResultDir = os.path.join(output_dir,f\"kmIntensity{DATAFILE_NAME[:-4]}\")\n",
    "        os.makedirs(kmeans_intResultDir,exist_ok=True)\n",
    "        np.save(os.path.join(kmeans_intResultDir, \"kmeans_coords_labels.npy\"), kmeans_coords_with_labels)\n",
    "        # Save as .mat\n",
    "        sio.savemat(os.path.join(kmeans_intResultDir, \"kmeans_coords_labels.mat\"), {\"kmeans_coords_labels\": kmeans_coords_with_labels})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## here test the code at each satement one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "volume = np.array([\n",
    "    [[0.5, 1.4],\n",
    "     [1.0, 1.5]],\n",
    "\n",
    "    [[1.3, 1.2],\n",
    "     [1.6, 0]]\n",
    "])\n",
    "\n",
    "volume1 = volume.reshape(4,2)\n",
    "volume2 = volume1[:,1]\n",
    "print(f\"reshape: {volume1},\\n  and \\n {volume1[:,1]} and {volume2.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a small 3D array\n",
    "volume = np.array([\n",
    "    [[0.5, 1.4],\n",
    "     [1.0, 1.5]],\n",
    "\n",
    "    [[1.3, 1.2],\n",
    "     [1.6, 0]]\n",
    "])\n",
    "print(f\"size of vol: {volume.shape} \\n\")\n",
    "coords = np.array(np.nonzero(volume))\n",
    "print(f\"size of nonzeros : {np.nonzero(volume)} \\n\")\n",
    "#  here np.nonzero(volume) -> returns the (z,x,y) coordinates of all nonzero points --> \n",
    "#  output size of nonzeros : (array([0, 0, 0, 0, 1, 1, 1]), array([0, 0, 1, 1, 0, 0, 1]), array([0, 1, 0, 1, 0, 1, 0]))\n",
    "# coord:\n",
    "#  [[0 0 0 0 1 1 1]\n",
    "#  [0 0 1 1 0 0 1]\n",
    "#  [0 1 0 1 0 1 0]]\n",
    "\n",
    "coordst = np.array(np.nonzero(volume)).T\n",
    "print(f\"coord:\\n {coords}\\n  and \\n taranspose: \\n \\n {coordst}\")\n",
    "# taranspose: \n",
    "#  [[0 0 0]\n",
    "#  [0 0 1]\n",
    "#  [0 1 0]\n",
    "#  [0 1 1]\n",
    "#  [1 0 0]\n",
    "#  [1 0 1]\n",
    "#  [1 1 0]]\n",
    "\n",
    "# thres = 1.3\n",
    "# volume[volume <= thres] = 0\n",
    "# print(\"Original Volume:\\n\",volume)\n",
    "# volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Create 3D grid coordinates (4,3,3) = 36 points\n",
    "x, y, z = np.meshgrid(np.arange(4), np.arange(3), np.arange(3), indexing='ij')\n",
    "coords = np.stack((x.ravel(), y.ravel(), z.ravel()), axis=1)\n",
    "\n",
    "# Random intensity values between 50 and 200\n",
    "intensity = np.random.uniform(50, 200, size=(coords.shape[0], 1))\n",
    "\n",
    "# Combine coords + intensity\n",
    "X = np.hstack((coords, intensity))\n",
    "\n",
    "# Apply standard scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Print first 5 for illustration\n",
    "print(f\"Original Data (X):\\n, {X[:5]}  and  {X.shape}\")\n",
    "print(\"\\nScaled Data (X_scaled):\\n\", X_scaled[:5])\n",
    "kmeans_k =3\n",
    "kmeans = KMeans(n_clusters=kmeans_k,init = 'k-means++',random_state=42).fit(X_scaled)\n",
    "# kmeans = KMeans(n_clusters=kmeans_k, init = 'k-means++', random_state=42).fit(X)\n",
    "kmeans_labels = kmeans.labels_   \n",
    "final_labels = -np.ones(len(X), dtype=int)  # Prepares an array to hold your final clustering labels. -1 means unassigned/outlier (just like DBSCAN does).Example: If X has 1000 points â†’ final_labels = [-1, -1, ..., -1] (length 1000)\n",
    "\n",
    "label_offset = 0\n",
    "\n",
    "for cluster_id in np.unique(kmeans_labels):\n",
    "    print(f\"i have compl k-means now in dbsacn, cluster_id: {cluster_id}\")\n",
    "    indices = np.where(kmeans_labels == cluster_id)[0]  #  indices = np.where(kmeans_labels == cluster_id) --> indices returns tuple of array like -> (array([1, 4]),) to extract use [0] first array(np.where(kmeans_labels == cluster_id))[0] and get result like this # array([1, 4])\n",
    "    \n",
    "    X_sub = X_scaled[indices] # here extracting the coordinate and intensity value according to the cluster_id. [X_scaled size is: (N, 4)]\n",
    "    # X_sub = X[indices]\n",
    "\n",
    "    db = DBSCAN(eps= 0.8, min_samples=5).fit(X_sub)  # db scan here in each loop for each cluster further.\n",
    "    db_labels = db.labels_\n",
    "    db_labels[db_labels != -1] += label_offset\n",
    "    final_labels[indices] = db_labels\n",
    "    label_offset += db_labels.max() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Create a moon-shaped dataset\n",
    "X, y = make_moons(n_samples=300, noise=0.5, random_state=42)\n",
    "\n",
    "# Normalize\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Plot original data\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c='gray', edgecolor='k')\n",
    "plt.title(\"Input Data (Normalized)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=5)\n",
    "# labels = db.fit_predict(X_scaled)\n",
    "clusters = db.fit(X_scaled)\n",
    "db_labels = clusters.labels_\n",
    "l0 = []\n",
    "l1 = []\n",
    "for l in db_labels:\n",
    "    # print(f\"lables:{l}\")\n",
    "    if l == 0:\n",
    "        l0.append(int(l))\n",
    "    else:\n",
    "        l1.append(int(l))\n",
    "\n",
    "print(f\"l0 index:{l0} and \\n  l1 index:{l1}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulab =np.unique(db_labels)\n",
    "print(ulab)\n",
    "label_offset =0 \n",
    "db_labels[db_labels != -1] += label_offset\n",
    "print(db_labels)\n",
    "# final_labels[indices] = db_labels\n",
    "label_offset += db_labels.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Labels assigned:\", np.unique(labels))\n",
    "print(\"Noise points (label == -1):\", list(labels).count(-1))\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='Set1', edgecolor='k')\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = np.load(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\clustering_output\\voxel_coords.npy\")\n",
    "print(dc.shape)\n",
    "# from scipy.io import loadmat\n",
    "# fd = loadmat(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\hybrid_Kdbcluster\\cluster_labels.mat\")\n",
    "# print(fd['labels'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data1 = np.load(r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\raw_npyData\\Tomogramma_BuddingYeastCell.npy\")\n",
    "data2 = data1[:50,:50,:50]\n",
    "size1 = data2.shape\n",
    "print(data2.shape[0],data2.shape[1],data2.shape[2],\"\\n --\" )\n",
    "print(f\"size: {size1[0]}\\n {size1[1]}\\n {size1[2]}\")\n",
    "print(f\"data1 shape: {data1.shape} \\n data2 shape: {data2.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "kmeans_labels = np.array([0, 1, 0, 2, 1, 2, 0])\n",
    "cluster_id = 0\n",
    "\n",
    "indices = np.where(kmeans_labels == cluster_id)\n",
    "print(indices)  # (array([1, 4]),)\n",
    "\n",
    "indices = np.where(kmeans_labels == cluster_id)[0]\n",
    "print(indices)  # array([1, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# Adjust parsing to handle np.float64(...) formatting using regular expressions\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Reload the file after execution reset\n",
    "# txt_path = 'results\\featureQuantileThres\\AllFeatures_Stats.txt'\n",
    "# import os\n",
    "\n",
    "txt_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\results\\featureQuantileThres\\AllFeatures_Stats.txt\"\n",
    "txt_path = os.path.normpath(txt_path)\n",
    "print(f\"txt_path: {txt_path}\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "with open(txt_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if ':' in line:\n",
    "            name, data_str = line.split(':', 1)\n",
    "            # Replace np.float64(...) with float values using regex\n",
    "            cleaned_data_str = re.sub(r'np\\.float64\\((.*?)\\)', r'\\1', data_str.strip())\n",
    "            data_dict = ast.literal_eval(cleaned_data_str)\n",
    "            data_dict = {k: float(v) for k, v in data_dict.items()}\n",
    "            data_dict['Filename'] = name.strip()\n",
    "            rows.append(data_dict)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Reorder columns to start with Filename\n",
    "cols = ['Filename'] + [col for col in df.columns if col != 'Filename']\n",
    "df = df[cols]\n",
    "\n",
    "# Save as CSV\n",
    "BASE_DIR = Path.cwd().parent\n",
    "csv_path = BASE_DIR/ \"results\"/ \"featureQuantileThres\"\n",
    "\n",
    "csv_file = \"AllFeatures_Stats_Converted.csv\" # Save CSV locally\n",
    "csv_path = os.path.join(csv_path,csv_file)\n",
    "\n",
    "# df.to_csv(\"AllFeatures_Stats_Converted.csv\", index=False)\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"CSV file saved as AllFeatures_Stats_Converted.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def create_mesh_from_mask(data, mask, title=\"Mesh\", transparency=0.3):\n",
    "    if not np.any(mask):\n",
    "        print(f\"âš ï¸ No points in {title}. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        verts, faces, _, _ = marching_cubes(data * mask, level=0)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Mesh creation failed for {title}: {e}\")\n",
    "        return\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.4, 0.6, 1.0])\n",
    "    mesh = mesh.filter_smooth_simple(number_of_iterations=1)\n",
    "\n",
    "    app = o3d.visualization.gui.Application.instance\n",
    "    app.initialize()\n",
    "    window = app.create_window(title, 1024, 768)\n",
    "    scene = o3d.visualization.rendering.Open3DScene(window.renderer)\n",
    "\n",
    "    mat = o3d.visualization.rendering.MaterialRecord()\n",
    "    mat.shader = \"defaultLitTransparency\"\n",
    "    mat.base_color = [0.4, 0.6, 1.0, transparency]\n",
    "    mat.base_roughness = 0.5\n",
    "\n",
    "    scene.add_geometry(\"mesh\", mesh, mat)\n",
    "    bbox = mesh.get_axis_aligned_bounding_box()\n",
    "    \n",
    "    center = bbox.get_center()\n",
    "    eye = center + np.array([0, 0, -1])  # Convert to NumPy array\n",
    "    up = [0, -1, 0]\n",
    "\n",
    "    scene.scene.camera.look_at(center, eye, up)\n",
    "\n",
    "    # scene.scene.camera.look_at(\n",
    "    # bbox.get_center(),           # center\n",
    "    # bbox.get_center() + [0, 0, -1],  # eye position\n",
    "    # [0, -1, 0]                   # up vector\n",
    "    # )\n",
    "\n",
    "    # scene.setup_camera(60, bbox, bbox.get_center())\n",
    "\n",
    "    def on_layout(context):\n",
    "        r = window.content_rect\n",
    "        scene.scene.set_viewport(r)\n",
    "\n",
    "    window.set_on_layout(on_layout)\n",
    "    app.run()\n",
    "\n",
    "def threshold_and_visualize(npy_file_path, threshold_val=1.20148978654012):\n",
    "    print(f\"CAN SEE THE NPYPATH : {npy_file_path}\")\n",
    "    # data = np.load(str(npy_file_path))\n",
    "    data = np.load(npy_file_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D numpy array.\")\n",
    "\n",
    "    print(f\"ðŸ“ Loaded: {npy_file_path.name} with shape {data.shape}\")\n",
    "    \n",
    "    mask_lower = data <= threshold_val\n",
    "    mask_upper = data > threshold_val\n",
    "\n",
    "    # create_mesh_from_mask(data, mask_lower, title=\"Lower Threshold Mesh\", transparency=0.1)\n",
    "    create_mesh_from_mask(data, mask_upper, title=\"Upper Threshold Mesh\", transparency=0.2)\n",
    "\n",
    "\n",
    "# ====== Replace this with your actual .npy file path ======\n",
    "# import os\n",
    "\n",
    "# filepath = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\raw_npyData\"\n",
    "# npy_path = os.path.join(filepath,'tomo_Grafene_24h.npy')\n",
    "# # npy_path = Path(\"path/to/your/datafile.npy\")  # ðŸ› ï¸ Replace this path\n",
    "# threshold_and_visualize(npy_path, threshold_val=1.44148978654012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Safe path definition\n",
    "filepath = Path(r\"E:/Projects/substructure_3d_data/Substructure_Different_DataTypes/data/raw_npyData\")\n",
    "npy_path = filepath / \"tomo_Grafene_24h.npy\"\n",
    "\n",
    "threshold_and_visualize(npy_path, threshold_val=1.201978654012)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "thres = 5\n",
    "array = np.random.randint(0,10,size=(5,5,5))\n",
    "print(array)\n",
    "data = array > thres\n",
    "print(f\"\\n data --> {thres} --> \\n {data} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "thres = 0.5\n",
    "array = np.random.rand(2,2,2)\n",
    "print(array)\n",
    "data = array > thres\n",
    "print(f\"\\n data --> {thres} --> \\n {data} \\n\")\n",
    "array = array\n",
    "print(f\"arraysize:{array.shape} \\n and \\n {array} \\n \" )\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from scipy.io import loadmat  # For .mat support\n",
    "# volume = np.load('data')\n",
    "verts,faces,_, _ = marching_cubes(data,level=0)\n",
    "mesh = o3d.geometry.TriangleMesh()\n",
    "mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "mesh.compute_vertex_normals()\n",
    "mesh.paint_uniform_color([0.6, 0.2, 1.0])\n",
    "\n",
    "# Visualize\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "# Simulated 3D data (a sphere)\n",
    "# x, y, z = np.indices((100, 100, 100))\n",
    "# sphere = (x - 50)**2 + (y - 50)**2 + (z - 50)**2\n",
    "# volume = np.exp(-sphere / 500)  # Smooth decay\n",
    "data = np.zeros((2, 3, 3))\n",
    "data[1, 1, 1] = 1.0 \n",
    "# data[1, 1, 2] = 1.5 \n",
    "# data[1, 2, 2] = 1.8\n",
    "\n",
    "print(data)\n",
    "# Threshold\n",
    "# threshold = 0.5\n",
    "# binary = volume > threshold\n",
    "\n",
    "# Mesh\n",
    "# verts, faces, _, _ = marching_cubes(volume * binary, level=0)\n",
    "verts, faces, _, _ = marching_cubes(data, level=0)\n",
    "mesh = o3d.geometry.TriangleMesh()\n",
    "mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "mesh.compute_vertex_normals()\n",
    "mesh.paint_uniform_color([0.6, 0.2, 1.0])\n",
    "\n",
    "# Visualize\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this below code is workin fine even if large data size is there. make a function or class of it , and use it when required.\n",
    "## but in line at this position just increase the size here: \n",
    "- if simplify:\n",
    "   - voxel_size = max(volume.shape) / 64  # can vary value from 64-128-256-512 etc to smooth the visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is workin fine even if large data size is there. make a function or class of it , and use it when required.\n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be 3D\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, simplify=True):\n",
    "    # Automatically find threshold using Otsuâ€™s method\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold used: {threshold:.4f}\")\n",
    "\n",
    "    # Marching cubes\n",
    "    print(\"[INFO] Extracting mesh using marching cubes...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Original mesh: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / 64  # Tweakable\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Simplified mesh: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    mesh.paint_uniform_color([0.6, 0.7, 1.0])\n",
    "    return mesh\n",
    "\n",
    "def visualize_mesh(mesh):\n",
    "#     o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "    o3d.visualization.draw_geometries([mesh], mesh_show_back_face=False)\n",
    "\n",
    "# ========== ðŸ§ª Example ========== #\n",
    "# npy_path = \"path_to_your_large_3d_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, simplify=True)\n",
    "# visualize_mesh(mesh)\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "resultData = PROJECT_PATH/\"results\"\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "input_npy = npy_path  # <-- Replace with your actual file path\n",
    "# input_npy = os.listdir(input_npy)\n",
    "for filename  in os.listdir(input_npy):\n",
    "    if filename.endswith('.npy'):\n",
    "\n",
    "        print(f\"filename in the path : {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "    #     input_npy = input_npy/\"tomo_Grafene_24h.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "    #     volume = load_data(input_npy)\n",
    "        volume = load_data(str(npyfilePath))\n",
    "        mesh = create_mesh_from_volume(volume, simplify=True)\n",
    "        visualize_mesh(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(npy_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be a 3D numpy array\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, grid_factor=128, simplify=True, color_mode='gradient'):\n",
    "    \"\"\"\n",
    "        Enhancements:\n",
    "        Control grid resolution using grid_factor (affects voxel_size for simplification).\n",
    "        Color grading based on vertex Z-values (color_mode='gradient') or keep uniform (color_mode='uniform').\n",
    "        Clear toggles for both via function parameters.\n",
    "    \"\"\"\n",
    "    # Compute Otsu threshold from non-zero values\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold: {threshold:.4f}\")\n",
    "\n",
    "    # Generate mesh using marching cubes\n",
    "    print(\"[INFO] Extracting mesh...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Mesh before simplification: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Simplify mesh\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / grid_factor\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Mesh after simplification: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    # Apply color grading\n",
    "    if color_mode == 'gradient':\n",
    "        z_vals = np.asarray(mesh.vertices)[:, 2]\n",
    "        z_min, z_max = z_vals.min(), z_vals.max()\n",
    "        norm_z = (z_vals - z_min) / (z_max - z_min + 1e-8)\n",
    "        colors = np.stack([norm_z, 0.6 * np.ones_like(norm_z), 1.0 - norm_z], axis=1)\n",
    "        mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        mesh.paint_uniform_color([0.6, 0.7, 1.0])  # Default blueish\n",
    "\n",
    "    return mesh\n",
    "\n",
    "def visualize_mesh(mesh):\n",
    "    o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "\n",
    "# ========== ðŸ§ª Example Usage ========== #\n",
    "# npy_path = \"path_to_your_large_3d_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=32, simplify=True, color_mode='gradient')\n",
    "# visualize_mesh(mesh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "resultData = PROJECT_PATH/\"results\"\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "input_npy = npy_path  # <-- Replace with your actual file path\n",
    "# input_npy = os.listdir(input_npy)\n",
    "for filename  in os.listdir(input_npy):\n",
    "    if filename.endswith('.npy'):\n",
    "\n",
    "        print(f\"filename in the path : {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "    #     input_npy = input_npy/\"tomo_Grafene_24h.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "    #     volume = load_data(input_npy)\n",
    "        volume = load_data(str(npyfilePath))\n",
    "        mesh = create_mesh_from_volume(volume, grid_factor=256, simplify=True, color_mode='gradient')\n",
    "        visualize_mesh(mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  improvement using d solution , \n",
    "\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be a 3D numpy array\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, grid_factor=32, simplify=True, color_mode='gradient'):\n",
    "    # Compute Otsu threshold from non-zero values\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold: {threshold:.4f}\")\n",
    "\n",
    "    # Generate mesh using marching cubes\n",
    "    print(\"[INFO] Extracting mesh...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Mesh before simplification: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Simplify mesh\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / grid_factor\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Mesh after simplification: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    # Apply color grading\n",
    "    if color_mode == 'gradient':\n",
    "        z_vals = np.asarray(mesh.vertices)[:, 2]\n",
    "        z_min, z_max = z_vals.min(), z_vals.max()\n",
    "        norm_z = (z_vals - z_min) / (z_max - z_min + 1e-8)\n",
    "        # Enhanced color gradient (red to green to blue)\n",
    "        colors = np.zeros((len(norm_z), 3))\n",
    "        colors[:, 0] = 1.0 - norm_z  # Red decreases with Z\n",
    "        colors[:, 1] = norm_z        # Green increases with Z\n",
    "        colors[:, 2] = norm_z        # Blue increases with Z\n",
    "        mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        mesh.paint_uniform_color([0.6, 0.7, 1.0])  # Default blueish\n",
    "\n",
    "    return mesh\n",
    "\n",
    "def visualize_mesh(mesh, transparency=0.5):\n",
    "    # Create visualizer with material properties\n",
    "    vis = o3d.visualization.Visualizer()\n",
    "    vis.create_window()\n",
    "    \n",
    "    # Add mesh with material properties\n",
    "    mat = o3d.visualization.rendering.MaterialRecord()\n",
    "    mat.shader = \"defaultLit\"\n",
    "    \n",
    "    # Modern Open3D versions use base_alpha instead of transparency\n",
    "        # Handle version compatibility\n",
    "    if hasattr(mat, 'transparency'):\n",
    "        mat.transparency = transparency\n",
    "    else:\n",
    "        mat.base_alpha = 1.0 - transparency\n",
    "    # mat.base_alpha = 1.0 - transparency  # Alpha is inverse of transparency\n",
    "    \n",
    "    # Additional material properties for better visualization\n",
    "    mat.base_roughness = 0.4\n",
    "    mat.base_metallic = 0.0\n",
    "    mat.base_color = [1.0, 1.0, 1.0, 1.0]  # RGBA\n",
    "    \n",
    "    vis.add_geometry(mesh, material=mat)\n",
    "    \n",
    "    # Configure render options\n",
    "    render_opt = vis.get_render_option()\n",
    "    render_opt.mesh_show_back_face = True\n",
    "    render_opt.mesh_show_wireframe = False\n",
    "    render_opt.light_on = True\n",
    "    render_opt.background_color = np.asarray([1.0, 1.0, 1.0])\n",
    "    render_opt.mesh_show_transparency = True  # Critical for transparency\n",
    "    \n",
    "    # Run visualization\n",
    "    vis.run()\n",
    "    vis.destroy_window()\n",
    "\n",
    "# ========== Example Usage ========== #\n",
    "# npy_path = \"path_to_your_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=64, simplify=True, color_mode='gradient')\n",
    "# visualize_mesh(mesh, transparency=0.6)\n",
    "\n",
    "# ========== Example Usage ========== #\n",
    "# npy_path = \"path_to_your_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# Create mesh with 60% transparency\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=64, simplify=True, \n",
    "#                               color_mode='gradient', transparency=0.6)\n",
    "# visualize_mesh(mesh)\n",
    "\n",
    "    \n",
    "from pathlib import Path\n",
    "import os\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "datafile = npy_path/\"tomo_Grafene_24h.npy\"\n",
    "volume = load_data(str(datafile))\n",
    "# Create mesh first (without transparency)\n",
    "mesh = create_mesh_from_volume(volume, grid_factor=64, color_mode='gradient')\n",
    "\n",
    "# Then visualize with transparency\n",
    "transparency=0.6\n",
    "base_alpha = 1.0 - transparency \n",
    "# visualize_mesh(mesh, transparency=0.6)  # 60% transparent\n",
    "visualize_mesh(mesh, base_alpha)  # 60% transparent\n",
    "\n",
    "# mesh, alpha = create_mesh_from_volume(volume, grid_factor=64, simplify=True, color_mode='gradient', transparency=0.2)\n",
    "# visualize_mesh_with_transparency(mesh, transparency=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d;\n",
    "# print(f\"this is version of o3d: {o3d.__version__}\")\n",
    "print(o3d.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.measure import marching_cubes\n",
    "from skimage.filters import threshold_otsu\n",
    "\n",
    "def load_data(npy_path):\n",
    "    data = np.load(npy_path)\n",
    "    assert data.ndim == 3, \"Data must be a 3D numpy array\"\n",
    "    return data\n",
    "\n",
    "def create_mesh_from_volume(volume, grid_factor=128, simplify=True, color_mode='gradient'):\n",
    "    \"\"\"\n",
    "        Enhancements:\n",
    "        Control grid resolution using grid_factor (affects voxel_size for simplification).\n",
    "        Color grading based on vertex Z-values (color_mode='gradient') or keep uniform (color_mode='uniform').\n",
    "        Clear toggles for both via function parameters.\n",
    "    \"\"\"\n",
    "    # Compute Otsu threshold from non-zero values\n",
    "    flat = volume[volume > 0].flatten()\n",
    "    threshold = threshold_otsu(flat)\n",
    "    print(f\"[INFO] Otsu Threshold: {threshold:.4f}\")\n",
    "\n",
    "    # Generate mesh using marching cubes\n",
    "    print(\"[INFO] Extracting mesh...\")\n",
    "    verts, faces, _, _ = marching_cubes(volume, level=threshold)\n",
    "    print(f\"[INFO] Mesh before simplification: {len(verts)} vertices, {len(faces)} faces\")\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Simplify mesh\n",
    "    if simplify:\n",
    "        voxel_size = max(volume.shape) / grid_factor\n",
    "        mesh = mesh.simplify_vertex_clustering(voxel_size=voxel_size)\n",
    "        print(f\"[INFO] Mesh after simplification: {len(mesh.vertices)} vertices, {len(mesh.triangles)} faces\")\n",
    "\n",
    "    # Apply color grading\n",
    "    if color_mode == 'gradient':\n",
    "        z_vals = np.asarray(mesh.vertices)[:, 2]\n",
    "        z_min, z_max = z_vals.min(), z_vals.max()\n",
    "        norm_z = (z_vals - z_min) / (z_max - z_min + 1e-8)\n",
    "        colors = np.stack([norm_z, 0.6 * np.ones_like(norm_z), 1.0 - norm_z], axis=1)\n",
    "        mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "    else:\n",
    "        mesh.paint_uniform_color([0.6, 0.7, 1.0])  # Default blueish\n",
    "\n",
    "    return mesh\n",
    "\n",
    "# def visualize_mesh(mesh):\n",
    "#     o3d.visualization.draw_geometries([mesh], mesh_show_back_face=True)\n",
    "\n",
    "# ========== ðŸ§ª Example Usage ========== #\n",
    "# npy_path = \"path_to_your_large_3d_data.npy\"\n",
    "# volume = load_data(npy_path)\n",
    "# mesh = create_mesh_from_volume(volume, grid_factor=32, simplify=True, color_mode='gradient')\n",
    "# visualize_mesh(mesh)\n",
    "\n",
    "def visualize_mesh(mesh, transparency=0.5):\n",
    "    mesh.compute_vertex_normals()\n",
    "\n",
    "    # Convert to TriangleMeshModel for transparency\n",
    "    mesh.material = o3d.visualization.rendering.MaterialRecord()\n",
    "    mesh.material.shader = \"defaultLitTransparency\"\n",
    "    mesh.material.base_color = [1.0, 0.6, 1.0, transparency]  # RGBA\n",
    "    vis.get_render_option().background_color = np.array([0, 0, 0])  # black background\n",
    "\n",
    "    vis = o3d.visualization.O3DVisualizer(\"Transparent Mesh Viewer\", 1024, 768)\n",
    "    vis.add_geometry(\"Mesh\", mesh, mesh.material)\n",
    "    vis.reset_camera_to_default()\n",
    "    vis.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npy_path = \"path/to/your/3ddata.npy\"\n",
    "from pathlib import Path\n",
    "import os\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "datafile = npy_path/\"tomo_Grafene_24h.npy\"\n",
    "volume = load_data(str(datafile))\n",
    "mesh = create_mesh_from_volume(volume, grid_factor=64, simplify=True, color_mode='gradient')\n",
    "visualize_mesh(mesh, transparency=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# data = range(11)\n",
    "# data = list(data)\n",
    "# print(data)\n",
    "# quartileData= np.quantile(data,0.50)\n",
    "# print(quartileData)\n",
    "from pathlib import Path\n",
    "import os\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "resultData = PROJECT_PATH/\"results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.filters import threshold_otsu\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "def visualize_ostu(extract_data,npy_path):\n",
    "    # Extract coordinates of foreground for plotting\n",
    "    foreground_mask = extract_data\n",
    "    coords = np.argwhere(foreground_mask)\n",
    "    coords = coords[np.random.choice(len(coords), size=min(len(coords), 50000), replace=False)]\n",
    "\n",
    "    # Plot\n",
    "    fig = px.scatter_3d(\n",
    "        x=coords[:, 0], y=coords[:, 1], z=coords[:, 2],\n",
    "        opacity=0.008,\n",
    "        title=f\"Foreground Voxel Visualization ({os.path.basename(npy_path)})\",\n",
    "        labels={'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, resultDataPath, output_dir=\"otsu_results\"):\n",
    "    output_dir = os.path.join(resultDataPath,output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    extractedFilename = os.path.basename(npy_path) # return the filename as string.\n",
    "    # Load the 3D data\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    # Apply Otsu threshold\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    # Create masks\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    # Save masks\n",
    "    np.save(os.path.join(output_dir, f\"fgnd_mask{extractedFilename}.npy\"), foreground_mask)\n",
    "    np.save(os.path.join(output_dir, f\"bgnd_mask{extractedFilename}.npy\"), background_mask)\n",
    "\n",
    "    # # Extract coordinates of foreground for plotting\n",
    "    # coords = np.argwhere(foreground_mask)\n",
    "    # coords = coords[np.random.choice(len(coords), size=min(len(coords), 50000), replace=False)]\n",
    "\n",
    "    # # Plot\n",
    "    # fig = px.scatter_3d(\n",
    "    #     x=coords[:, 0], y=coords[:, 1], z=coords[:, 2],\n",
    "    #     opacity=0.5,\n",
    "    #     title=f\"Foreground Voxel Visualization ({os.path.basename(npy_path)})\",\n",
    "    #     labels={'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
    "    # )\n",
    "    # fig.show()\n",
    "\n",
    "    return threshold, foreground_mask, background_mask\n",
    "\n",
    "# === Example usage ===\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from pathlib import Path\n",
    "    import os\n",
    "\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    # PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "    print(PROJECT_PATH)\n",
    "    resultData = PROJECT_PATH/\"results\"\n",
    "    npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "    # npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "    input_npy = npy_path  # <-- Replace with your actual file path\n",
    "    # input_npy = os.listdir(input_npy)\n",
    "    for filename  in os.listdir(input_npy):\n",
    "        print(f\"filename in the path : {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "\n",
    "        res = apply_otsu_segmentation(npyfilePath,resultData,output_dir=\"otsu_results\")\n",
    "        visualize_ostu(res[1],npyfilePath)\n",
    "        visualize_ostu(res[2],npyfilePath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary packages after kernel reset\n",
    "import numpy as np\n",
    "import os\n",
    "from skimage.filters import threshold_otsu\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, resultDataPath, output_dir=\"otsu_results\"):\n",
    "    output_dir = os.path.join(resultDataPath, output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    extractedFilename = os.path.basename(npy_path)\n",
    "    data = np.load(npy_path)\n",
    "\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    np.save(os.path.join(output_dir, f\"{extractedFilename}_fg_mask.npy\"), foreground_mask)\n",
    "    np.save(os.path.join(output_dir, f\"{extractedFilename}_bg_mask.npy\"), background_mask)\n",
    "\n",
    "    return threshold, foreground_mask, background_mask\n",
    "\n",
    "\n",
    "def visualize_foreground_background(fg_mask, bg_mask, title=\"3D Otsu Segmentation\", use_mesh=False, downsample=True, max_points=50000):\n",
    "    fg_coords = np.argwhere(fg_mask)\n",
    "    bg_coords = np.argwhere(bg_mask)\n",
    "\n",
    "    if downsample:\n",
    "        if len(fg_coords) > max_points:\n",
    "            fg_coords = fg_coords[np.random.choice(len(fg_coords), max_points, replace=False)]\n",
    "        if len(bg_coords) > max_points:\n",
    "            bg_coords = bg_coords[np.random.choice(len(bg_coords), max_points, replace=False)]\n",
    "\n",
    "    if not use_mesh:\n",
    "        fig = px.scatter_3d(\n",
    "            x=fg_coords[:, 0], y=fg_coords[:, 1], z=fg_coords[:, 2],\n",
    "            color=fg_coords[:, 2],\n",
    "            opacity=0.08,\n",
    "            color_continuous_scale=\"Blues\",\n",
    "            title=f\"{title} - Foreground\",\n",
    "            labels={'x': 'X', 'y': 'Y', 'z': 'Z'}\n",
    "        )\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=bg_coords[:, 0], y=bg_coords[:, 1], z=bg_coords[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=1, opacity=0.01, color='gray'),\n",
    "            name='Background'\n",
    "        ))\n",
    "    else:\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Mesh3d(\n",
    "            x=fg_coords[:, 0], y=fg_coords[:, 1], z=fg_coords[:, 2],\n",
    "            alphahull=5,\n",
    "            opacity=0.15,\n",
    "            color='lightblue',\n",
    "            name='Foreground (Mesh)'\n",
    "        ))\n",
    "        # fig.add_trace(go.Mesh3d(\n",
    "        #     x=bg_coords[:, 0], y=bg_coords[:, 1], z=bg_coords[:, 2],\n",
    "        #     alphahull=10,\n",
    "        #     opacity=0.02,\n",
    "        #     color='gray',\n",
    "        #     name='Background (Mesh)'\n",
    "        # ))\n",
    "        fig.update_layout(title=title, scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(margin=dict(l=0, r=0, t=40, b=0))\n",
    "    fig.show()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     from pathlib import Path\n",
    "#     import os\n",
    "\n",
    "#     PROJECT_PATH = Path.cwd().parent\n",
    "#     # PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "#     print(PROJECT_PATH)\n",
    "#     resultData = PROJECT_PATH/\"results\"\n",
    "#     npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "#     # npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "#     input_npy = npy_path  # <-- Replace with your actual file path\n",
    "#     # input_npy = os.listdir(input_npy)\n",
    "#     for filename  in os.listdir(input_npy):\n",
    "#         print(f\"filename in the path : {filename}\")\n",
    "#         # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "#         npyfilePath = input_npy/filename\n",
    "#         # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "\n",
    "#         res = apply_otsu_segmentation(npyfilePath,resultData,output_dir=\"otsu_results\")\n",
    "#         visualize_ostu(res[1],npyfilePath)\n",
    "#         visualize_ostu(res[2],npyfilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def threshold_and_visualize(npy_file_path, threshold_val=1.44148978654012):\n",
    "    print(f\"CAN SEE THE NPYPATH : {npy_file_path}\")\n",
    "    # data = np.load(str(npy_file_path))\n",
    "    data = np.load(npy_file_path)\n",
    "    print(f\"data shape --> {data.shape}\")\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D numpy array.\")\n",
    "\n",
    "    print(f\"ðŸ“ Loaded: {npy_file_path.name} with shape {data.shape}\")\n",
    "    \n",
    "#     mask_lower = data <= threshold_val\n",
    "#     data[data>threshold_val]\n",
    "#     mask_upper = data > threshold_val\n",
    "\n",
    "    \n",
    "    \n",
    "#     (mask_lower.fl), min(mask_lower),max(mask_upper), min(mask_upper)\n",
    "    return mask_lower,mask_upper\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Safe path definition\n",
    "filepath = Path(r\"E:/Projects/substructure_3d_data/Substructure_Different_DataTypes/data/raw_npyData\")\n",
    "npy_path = filepath / \"tomo_Grafene_24h.npy\"\n",
    "\n",
    "# from path_manager import AddPath\n",
    "# AddPath()\n",
    "import sys\n",
    "import os\n",
    "GARBAGE_PATH = Path.cwd()\n",
    "\n",
    "modulePath = GARBAGE_PATH/\"modules\"\n",
    "print(f\"module path  {modulePath}\")\n",
    "\n",
    "sys.path.append(str(modulePath))\n",
    "\n",
    "# for file in os.listdir(modulePath):\n",
    "#     if file.endswith('.py'):\n",
    "#         print(f\"filename : {file}\")\n",
    "    \n",
    "from plot3dint import plot3dinteractive\n",
    "\n",
    "\n",
    "\n",
    "res = threshold_and_visualize(npy_path, threshold_val=1.44148978654012)\n",
    "fg_mask = res[1]  # mask_upper\n",
    "bg_mask = res[0]  # mask_lower\n",
    "for RES in res:\n",
    "    val = np.array(RES)\n",
    "    val = val.flatten()\n",
    "    print(f\"max value: {max(val)} and min val:{min(val)} \\n\")\n",
    "# plot3dinteractive(bg_mask,\"upper\",sample_fraction=0.2)\n",
    "# visualize_foreground_background(fg_mask, bg_mask, title=\"3D Otsu Segmentation\", use_mesh=False, downsample=True, max_points=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_PATH = Path.cwd().parent\n",
    "# PROJECT_PATH = Path(__file__).resolve().parent.parent\n",
    "print(PROJECT_PATH)\n",
    "resultData = PROJECT_PATH/\"results\"\n",
    "npy_path = PROJECT_PATH/\"data\"/\"raw_npyData\"\n",
    "# npy_path = PROJECT_PATH/\"data\"/\"normalized_npyData\"\n",
    "input_npy = npy_path  # <-- Replace with your actual file path\n",
    "# input_npy = os.listdir(input_npy)\n",
    "count = 0\n",
    "# filelistnName = np.random.choice(os.listdir(input_npy),5)\n",
    "# print(f\"randomly selected 5 files in --> {filelistnName}\")\n",
    "for filename in os.listdir(input_npy):\n",
    "# for filename in filelistnName:\n",
    "    if filename in ['tomo_Grafene_24h.npy','tomo_grafene_48h.npy']:\n",
    "        print(f\"filename in the path : and processing with it ---->  {filename}\")\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell.npy\"\n",
    "        npyfilePath = input_npy/filename\n",
    "        # input_npy = input_npy/\"Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "    \n",
    "        # res = apply_otsu_segmentation(npyfilePath,resultData,output_dir=\"otsu_results\")\n",
    "        # visualize_ostu(res[1],npyfilePath)\n",
    "        # visualize_ostu(res[2],npyfilePath)\n",
    "        # apply_otsu_segmentation(npy_path, resultDataPath, output_dir=\"otsu_results\"):\n",
    "        threshold, fg_mask, bg_mask = apply_otsu_segmentation(npyfilePath, resultData,output_dir=\"otsu_results\")\n",
    "        visualize_foreground_background(fg_mask, bg_mask, title=f\"{filename[:-4]}\", use_mesh= True, downsample=True, max_points=60000)\n",
    "        # visualize_foreground_background(fg_mask, bg_mask, use_mesh=True, downsample=True)\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        print(f\"couting the file processed  --> {count}\")\n",
    "    \n",
    "        if count == 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = range(5)\n",
    "# print(list(x))\n",
    "# for val in list(x):\n",
    "#     if val in [3,2]:\n",
    "#         print(f\"val is-->2,3 : {val}\")\n",
    "#     else:\n",
    "#         print(val)\n",
    "    \n",
    "# # if val in \n",
    "# # y = np.random.choice(x,3)\n",
    "# print(f\"{x}, y--> {y} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from skimage.filters import threshold_otsu\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation_with_cupy(npy_path, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    flat_data = cp.asarray(data[data > 0].flatten())\n",
    "    threshold = float(threshold_otsu(cp.asnumpy(flat_data)))\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    base_name = Path(npy_path).stem\n",
    "    np.save(output_dir / f\"{base_name}_fg_mask.npy\", foreground_mask)\n",
    "    np.save(output_dir / f\"{base_name}_bg_mask.npy\", background_mask)\n",
    "\n",
    "    return threshold, foreground_mask, background_mask\n",
    "\n",
    "\n",
    "def visualize_with_open3d(fg_mask, bg_mask, title=\"3D Visualization\", downsample=True, max_points=50000):\n",
    "    fg_coords = np.argwhere(fg_mask)\n",
    "    bg_coords = np.argwhere(bg_mask)\n",
    "\n",
    "    if downsample:\n",
    "        if len(fg_coords) > max_points:\n",
    "            fg_coords = fg_coords[np.random.choice(len(fg_coords), max_points, replace=False)]\n",
    "        if len(bg_coords) > max_points:\n",
    "            bg_coords = bg_coords[np.random.choice(len(bg_coords), max_points, replace=False)]\n",
    "\n",
    "    fg_pcd = o3d.geometry.PointCloud()\n",
    "    fg_pcd.points = o3d.utility.Vector3dVector(fg_coords)\n",
    "    fg_colors = np.tile([0.3, 0.5, 1.0], (fg_coords.shape[0], 1))\n",
    "    fg_pcd.colors = o3d.utility.Vector3dVector(fg_colors)\n",
    "\n",
    "    bg_pcd = o3d.geometry.PointCloud()\n",
    "    bg_pcd.points = o3d.utility.Vector3dVector(bg_coords)\n",
    "    bg_colors = np.tile([0.6, 0.6, 0.6], (bg_coords.shape[0], 1))\n",
    "    bg_pcd.colors = o3d.utility.Vector3dVector(bg_colors)\n",
    "\n",
    "    o3d.visualization.draw_geometries([fg_pcd, bg_pcd], window_name=title)\n",
    "\n",
    "\n",
    "def batch_otsu_segmentation(input_dir, output_dir, use_open3d=True, downsample=True, max_points=50000):\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    npy_files = list(input_dir.glob(\"*.npy\"))\n",
    "\n",
    "    print(f\"ðŸ” Found {len(npy_files)} .npy files in: {input_dir}\")\n",
    "\n",
    "    for file in npy_files:\n",
    "        print(f\"\\nðŸš€ Processing: {file.name}\")\n",
    "        try:\n",
    "            threshold, fg_mask, bg_mask = apply_otsu_segmentation_with_cupy(file, output_dir)\n",
    "            if use_open3d:\n",
    "                visualize_with_open3d(fg_mask, bg_mask, title=file.stem, downsample=downsample, max_points=max_points)\n",
    "            print(f\"âœ… Finished {file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed {file.name}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_npy_path = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    result_path = PROJECT_PATH / \"results\" / \"otsu_gpu\"\n",
    "\n",
    "    batch_otsu_segmentation(\n",
    "        input_dir=input_npy_path,\n",
    "        output_dir=result_path,\n",
    "        use_open3d=True,\n",
    "        downsample=True,\n",
    "        max_points=50000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"script has been modified to include the following enhancements:\n",
    "\n",
    "User Options:\n",
    "\n",
    "Choose to visualize foreground, background, or both.\n",
    "\n",
    "Enable or disable saving of .obj files.\n",
    "\n",
    "Color Grading:\n",
    "\n",
    "Color intensity is based on voxel values using matplotlib color maps.\n",
    "\n",
    "Interactive Visualization:\n",
    "\n",
    "Foreground and background are visualized using Open3D with transparency and downsampling.\n",
    "\n",
    "Robust Saving:\n",
    "\n",
    "Saves .obj files for foreground/background point clouds if enabled.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from skimage.filters import threshold_otsu\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation_with_cupy(npy_path, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Expected a 3D array\")\n",
    "\n",
    "    flat_data = cp.asarray(data[data > 0].flatten())\n",
    "    threshold = float(threshold_otsu(cp.asnumpy(flat_data)))\n",
    "    print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "\n",
    "    foreground_mask = data > threshold\n",
    "    background_mask = ~foreground_mask\n",
    "\n",
    "    base_name = Path(npy_path).stem\n",
    "    np.save(output_dir / f\"{base_name}_fg_mask.npy\", foreground_mask)\n",
    "    np.save(output_dir / f\"{base_name}_bg_mask.npy\", background_mask)\n",
    "\n",
    "    return threshold, data, foreground_mask, background_mask\n",
    "\n",
    "\n",
    "def visualize_with_open3d(fg_mask, bg_mask, data, title=\"3D Visualization\", show_fg=True, show_bg=False,\n",
    "                           downsample=True, max_points=50000, save_obj=False, obj_output_dir=None):\n",
    "    geometries = []\n",
    "\n",
    "    if show_fg:\n",
    "        fg_coords = np.argwhere(fg_mask)\n",
    "        if downsample and len(fg_coords) > max_points:\n",
    "            fg_coords = fg_coords[np.random.choice(len(fg_coords), max_points, replace=False)]\n",
    "        fg_values = data[tuple(fg_coords.T)]\n",
    "        fg_colors = plt.get_cmap(\"Blues\")((fg_values - fg_values.min()) / (np.ptp(fg_values) + 1e-6))[:, :3]\n",
    "        fg_pcd = o3d.geometry.PointCloud()\n",
    "        fg_pcd.points = o3d.utility.Vector3dVector(fg_coords)\n",
    "        fg_pcd.colors = o3d.utility.Vector3dVector(fg_colors)\n",
    "        geometries.append(fg_pcd)\n",
    "        if save_obj and obj_output_dir:\n",
    "            o3d.io.write_point_cloud(str(Path(obj_output_dir) / f\"{title}_foreground.obj\"), fg_pcd)\n",
    "\n",
    "    if show_bg:\n",
    "        bg_coords = np.argwhere(bg_mask)\n",
    "        if downsample and len(bg_coords) > max_points:\n",
    "            bg_coords = bg_coords[np.random.choice(len(bg_coords), max_points, replace=False)]\n",
    "        bg_values = data[tuple(bg_coords.T)]\n",
    "        bg_colors = plt.get_cmap(\"Greys\")((bg_values - bg_values.min()) / (np.ptp(bg_values) + 1e-6))[:, :3]\n",
    "        bg_pcd = o3d.geometry.PointCloud()\n",
    "        bg_pcd.points = o3d.utility.Vector3dVector(bg_coords)\n",
    "        bg_pcd.colors = o3d.utility.Vector3dVector(bg_colors)\n",
    "        geometries.append(bg_pcd)\n",
    "        if save_obj and obj_output_dir:\n",
    "            o3d.io.write_point_cloud(str(Path(obj_output_dir) / f\"{title}_background.obj\"), bg_pcd)\n",
    "\n",
    "    if geometries:\n",
    "        o3d.visualization.draw_geometries(geometries, window_name=title)\n",
    "\n",
    "\n",
    "def batch_otsu_segmentation(input_dir, output_dir, use_open3d=True, downsample=True, max_points=50000,\n",
    "                             show_fg=True, show_bg=False, save_obj=False):\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    npy_files = list(input_dir.glob(\"*.npy\"))\n",
    "\n",
    "    print(f\"ðŸ” Found {len(npy_files)} .npy files in: {input_dir}\")\n",
    "\n",
    "    for file in npy_files:\n",
    "        print(f\"\\nðŸš€ Processing: {file.name}\")\n",
    "        try:\n",
    "            threshold, data, fg_mask, bg_mask = apply_otsu_segmentation_with_cupy(file, output_dir)\n",
    "            if use_open3d:\n",
    "                visualize_with_open3d(\n",
    "                    fg_mask, bg_mask, data,\n",
    "                    title=file.stem,\n",
    "                    show_fg=show_fg,\n",
    "                    show_bg=show_bg,\n",
    "                    downsample=downsample,\n",
    "                    max_points=max_points,\n",
    "                    save_obj=save_obj,\n",
    "                    obj_output_dir=output_dir\n",
    "                )\n",
    "            print(f\"âœ… Finished {file.name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed {file.name}: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_npy_path = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    result_path = PROJECT_PATH / \"results\" / \"otsu_gpu\"\n",
    "\n",
    "    batch_otsu_segmentation(\n",
    "        input_dir=input_npy_path,\n",
    "        output_dir=result_path,\n",
    "        use_open3d=True,\n",
    "        downsample=True,\n",
    "        max_points=80000,\n",
    "        show_fg=True,\n",
    "        show_bg=False,\n",
    "        save_obj=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import marching_cubes\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Input must be a 3D array\")\n",
    "\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    fg_mask = data > threshold\n",
    "    bg_mask = ~fg_mask\n",
    "\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_fg_mask.npy\"), fg_mask)\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_bg_mask.npy\"), bg_mask)\n",
    "\n",
    "    return threshold, fg_mask, bg_mask, data\n",
    "\n",
    "\n",
    "def visualize_3d_marching_cubes(fg_mask, data, title, save_obj_path=None):\n",
    "    if not np.any(fg_mask):\n",
    "        print(\"âš ï¸ Foreground mask is empty. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    verts, faces, _, _ = marching_cubes(data * fg_mask, level=0)\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.2, 0.6, 1.0])\n",
    "\n",
    "    fg_points = np.argwhere(fg_mask)\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(fg_points)\n",
    "    pcd.paint_uniform_color([1.0, 0.5, 0.0])\n",
    "\n",
    "    o3d.visualization.draw_geometries([pcd, mesh], window_name=title)\n",
    "\n",
    "    if save_obj_path:\n",
    "        o3d.io.write_triangle_mesh(save_obj_path, mesh)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_dir = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    output_dir = PROJECT_PATH / \"results\" / \"otsu_results\"\n",
    "\n",
    "    print(f\"Found {len(os.listdir(input_dir))} .npy files in: {input_dir}\\n\")\n",
    "\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".npy\"):\n",
    "            print(f\"\\nðŸš€ Processing: {file}\")\n",
    "            npy_path = input_dir / file\n",
    "            try:\n",
    "                threshold, fg_mask, bg_mask, data = apply_otsu_segmentation(npy_path, output_dir)\n",
    "                print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "                visualize_3d_marching_cubes(fg_mask, data, title=file, save_obj_path=output_dir / f\"{file[:-4]}_mesh.obj\")\n",
    "                print(f\"âœ… Finished {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.measure import marching_cubes\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def apply_otsu_segmentation(npy_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = np.load(npy_path)\n",
    "    if data.ndim != 3:\n",
    "        raise ValueError(\"Input must be a 3D array\")\n",
    "\n",
    "    flat_data = data[data > 0].flatten()\n",
    "    threshold = threshold_otsu(flat_data)\n",
    "    fg_mask = data > threshold\n",
    "    bg_mask = ~fg_mask\n",
    "\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_fg_mask.npy\"), fg_mask)\n",
    "    np.save(os.path.join(output_dir, f\"{npy_path.stem}_bg_mask.npy\"), bg_mask)\n",
    "\n",
    "    return threshold, fg_mask, bg_mask, data\n",
    "\n",
    "\n",
    "def visualize_3d_marching_cubes_gui(fg_mask, data, title, save_obj_path=None, transparency=0.3):\n",
    "    if not np.any(fg_mask):\n",
    "        print(\"âš ï¸ Foreground mask is empty. Skipping visualization.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        verts, faces, _, _ = marching_cubes(data * fg_mask, level=0)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Mesh creation failed: {e}\")\n",
    "        return\n",
    "\n",
    "    mesh = o3d.geometry.TriangleMesh()\n",
    "    mesh.vertices = o3d.utility.Vector3dVector(verts)\n",
    "    mesh.triangles = o3d.utility.Vector3iVector(faces)\n",
    "    mesh.compute_vertex_normals()\n",
    "    mesh.paint_uniform_color([0.2, 0.6, 1.0])\n",
    "    mesh = mesh.filter_smooth_simple(number_of_iterations=1)\n",
    "\n",
    "    app = o3d.visualization.gui.Application.instance\n",
    "    app.initialize()\n",
    "    window = app.create_window(title, 1024, 768)\n",
    "    scene = o3d.visualization.rendering.Open3DScene(window.renderer)\n",
    "    mat = o3d.visualization.rendering.MaterialRecord()\n",
    "    mat.shader = \"defaultLitTransparency\"\n",
    "    mat.base_color = [0.2, 0.6, 1.0, transparency]\n",
    "    mat.base_roughness = 0.5\n",
    "    mat.point_size = 3\n",
    "\n",
    "    scene.add_geometry(\"mesh\", mesh, mat)\n",
    "    # Removed invalid call to set_background to fix compatibility with some Open3D versions\n",
    "    # scene.scene.set_background([1.0, 1.0, 1.0, 1.0])\n",
    "    bbox = mesh.get_axis_aligned_bounding_box()\n",
    "    scene.setup_camera(60, bbox, bbox.get_center())\n",
    "\n",
    "    def on_layout(context):\n",
    "        r = window.content_rect\n",
    "        scene.scene.set_viewport(r)\n",
    "\n",
    "    window.set_on_layout(on_layout)\n",
    "    app.run()\n",
    "\n",
    "    if save_obj_path:\n",
    "        o3d.io.write_triangle_mesh(str(save_obj_path), mesh)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    PROJECT_PATH = Path.cwd().parent\n",
    "    input_dir = PROJECT_PATH / \"data\" / \"raw_npyData\"\n",
    "    output_dir = PROJECT_PATH / \"results\" / \"otsu_results\"\n",
    "\n",
    "    print(f\"Found {len(os.listdir(input_dir))} .npy files in: {input_dir}\\n\")\n",
    "\n",
    "    for file in os.listdir(input_dir):\n",
    "        if file.endswith(\".npy\"):\n",
    "            print(f\"\\nðŸš€ Processing: {file}\")\n",
    "            npy_path = input_dir / file\n",
    "            try:\n",
    "                threshold, fg_mask, bg_mask, data = apply_otsu_segmentation(npy_path, output_dir)\n",
    "                print(f\"Otsu Threshold: {threshold:.3f}\")\n",
    "                visualize_3d_marching_cubes_gui(\n",
    "                    fg_mask,\n",
    "                    data,\n",
    "                    title=file,\n",
    "                    save_obj_path=output_dir / f\"{file[:-4]}_mesh.obj\",\n",
    "                    transparency=0.2\n",
    "                )\n",
    "                print(f\"âœ… Finished {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
