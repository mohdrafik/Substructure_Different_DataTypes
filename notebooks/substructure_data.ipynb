{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## convert the .mat data into the .npy data using the main key value from the main (.mat)  data files and saved in \"outputdatapath\".\n",
    "\n",
    "# Data/  --> Description \n",
    "- #### raw_npyData/ is directory  --> where file saved as .npy file after converting from .mat file.\n",
    "- #### normalized_matData/ is directory  --> where matNormalized data files saved as ..normalized.mat file after normalizing .mat file.\n",
    "- #### normalized_npyData/ is directory  --> where npyNormalized data files saved as -..normalized.npy file after normalizing .npy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the auxfunction \n",
    "import sys\n",
    "import os\n",
    "# Define the module path\n",
    "module_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "if not os.path.exists(module_path):\n",
    "    module_path = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "\n",
    "# Add the module path to sys.path if it's not already there\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import createmat2npy as mnpy   #  this module load the .mat file,extract data according to the key and convert them into .npy file.\n",
    "datapath = r'C:\\Users\\mrafik\\OneDrive - C.N.R. STIIMA\\tomogram all data\\all_tomogram_data'\n",
    "outputdatapath = r'E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\intermdata1'\n",
    "if not os.path.exists(datapath and outputdatapath):\n",
    "    datapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\SharedContents\\OneDrive - C.N.R. STIIMA\\tomogram all data\\all_tomogram_data\"\n",
    "    outputdatapath = r'C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\intermdata1'\n",
    "\n",
    "mnpy.mat2npy(datapath,outputdatapath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## A function is defined to load and normalize 3D Numpy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Python Script for Loading & Normalization \n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "\n",
    "def load_and_normalize_npy(file_path):\n",
    "    \"\"\"\n",
    "    Loads a .npy file and applies Min-Max normalization.\n",
    "    \"\"\"\n",
    "    data = np.load(file_path)  # Load the file\n",
    "    min_val, max_val = np.min(data), np.max(data)  # Get min-max values\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)  # Normalize to [0,1]\n",
    "    return normalized_data, min_val, max_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### all different normalized npy array data is stored in the normalized data and Dictionary saved as MATLAB .mat with name as 'all_normalizeddata.mat'\n",
    "\n",
    "- And seperatley saved the each npy file as .mat format also.(3d matrix format data) and also in \"__normalized.npy file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory containing .npy files (update this with your folder path)\n",
    "import scipy.io as sio\n",
    "# # datapath = r'C:\\Users\\mrafik\\OneDrive - C.N.R. STIIMA\\tomogram all data\\all_tomogram_data'\n",
    "# outputdatapath = r'E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\processed'\n",
    "# if not os.path.exists(outputdatapath):\n",
    "# #     datapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\SharedContents\\OneDrive - C.N.R. STIIMA\\tomogram all data\\all_tomogram_data\"\n",
    "#     outputdatapath = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\processed\"\n",
    "    \n",
    "    \n",
    "data_folder = outputdatapath\n",
    "\n",
    "npy_files = glob.glob(os.path.join(data_folder, \"*.npy\"))  # List all .npy files\n",
    "# Dictionary to store the normalized datasets\n",
    "normalized_data = {}\n",
    "data_ranges = {}\n",
    "\n",
    "# Load and normalize each dataset\n",
    "for file in npy_files:\n",
    "    file_name = os.path.basename(file)\n",
    "    base_name = os.path.splitext(file_name)[0]  # Remove .npy extension\n",
    "\n",
    "    data, min_val, max_val = load_and_normalize_npy(file)  # data --> normalized data return by above function.\n",
    "    normalized_data[file_name] = data  # Store in dictionary\n",
    "    data_ranges[file_name] = (min_val, max_val)  # Store original data range\n",
    "    print(f\"Loaded and normalized {file_name} - Min: {min_val}, Max: {max_val}\")\n",
    "\n",
    "     # Save normalized data as .mat (MATLAB format)\n",
    "    mat_save_path = os.path.join(data_folder, f\"{base_name}_normalized.mat\")\n",
    "    sio.savemat(mat_save_path, {base_name: data})\n",
    "    print(f\"ðŸ“‚ Saved: {mat_save_path}\")\n",
    " # Save normalized data as .npy numpy array.\n",
    "    save_path = os.path.join(data_folder,  f\"{base_name}_normalized.npy\")  # Keep same filename\n",
    "    np.save(save_path, data)\n",
    "    print(f\"âœ… Saved as npy file: {save_path} | Min: {min_val:.4f} | Max: {max_val:.4f}\")\n",
    "\n",
    "\n",
    "data_dict_npy = normalized_data  # all different normalized npy array data is stored in the normalized data \n",
    "\n",
    "# Save dictionary as `.mat`\n",
    "mat_path = os.path.join(data_folder, \"all_normalizeddata.mat\")\n",
    "sio.savemat(mat_path, data_dict_npy)\n",
    "print(f\"Dictionary saved as MATLAB .mat: {mat_path}\")\n",
    "\n",
    "# # Load dictionary from `.mat`\n",
    "# loaded_mat = sio.loadmat(mat_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "###  below The code plot the histogram of normalize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "if not os.path.exists(module_path):\n",
    "    print(f\" i am looking for the Gaetano sys path\")\n",
    "    module_path = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "    \n",
    "# Add the module path to sys.path if it's nat already there\n",
    "print(f\"module path: {module_path}\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from histogramplot import plot_normalizedata_hist\n",
    "# from plot3dint import plot3dinteractive\n",
    "\n",
    "for keyval in normalized_data:\n",
    "    datakey = keyval\n",
    "    print(f\"\\n data key :{datakey}\")\n",
    "    dataval = normalized_data[datakey]\n",
    "    voldata=dataval\n",
    "    keyvalue = datakey\n",
    "    # plot3dinteractive(voldata,keyvalue)\n",
    "    plot_normalizedata_hist(dataval,datakey)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Step 2: Feature Extraction & Quantile-Based Thresholding\n",
    "- Now that your 3D datasets are normalized, we will proceed with Feature Extraction & Quantile-Based Thresholding to identify meaningful substructures.\n",
    "###  Why This Step is Important?\n",
    "- Feature Extraction helps in understanding the distribution of voxel intensities.\n",
    "- Quantile-Based Thresholding helps to filter noise and identify significant regions in the dataset.\n",
    "\n",
    "## - What I Will Do?\n",
    "\n",
    "-  Step 1: Extract statistical features (mean, variance, quantiles)\n",
    "-  Step 2: Apply Quantile-Based Thresholding (0.95, 0.99 quantiles)\n",
    "-  Step 3: Visualize the thresholded regions in 3D slices\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import sys\n",
    "import os\n",
    "module_path = r\"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "\n",
    "if not os.path.exists(module_path):\n",
    "    print(f\" i am looking for the Gaetano sys path\")\n",
    "    module_path = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\src\\modules\"\n",
    "    \n",
    "# Add the module path to sys.path if it's nat already there\n",
    "print(f\"module path: {module_path}\")\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from listspecificfiles import readlistFiles\n",
    "keyword = 'normalized.npy'\n",
    "dataset_names = readlistFiles(outputdatapath,keyword=keyword)\n",
    "    \n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "# Load the normalized datasets\n",
    "data_folder =  outputdatapath # Update this to your normalized npy fileslocation\n",
    "# dataset_names = [\"AML2_cell11.npy\", \"AML3_cell16.npy\", \"tomo_Grafene_24h.npy\",\n",
    "#                  \"tomo_Grafene_48h.npy\", \"Tomogramma_BuddingYeastCell.npy\",\n",
    "#                  \"Tomogramma_Cell1.npy\", \"Tomogramma_Cell2.npy\", \"Tomogramma_Cell3.npy\"]\n",
    "\n",
    "# dataset_names = readlistFiles\n",
    "# print(f\"enable to see the all dataset names: {dataset_names}\")\n",
    "\n",
    "# Function to load saved normalized data\n",
    "def load_normalized_data(filename):\n",
    "    return np.load(f\"{data_folder}/{filename}\")\n",
    "\n",
    "# Function to extract statistical features\n",
    "def extract_features(data):\n",
    "    mean_val = np.mean(data)\n",
    "    std_dev = np.std(data)\n",
    "    q95 = np.quantile(data, 0.95)\n",
    "    q99 = np.quantile(data, 0.99)\n",
    "    return mean_val, std_dev, q95, q99\n",
    "\n",
    "# Function to to apply threshold and visualize the data after masking\n",
    "def visualizethres(dataset):\n",
    "        # Select a dataset for visualization\n",
    "    selected_dataset = dataset\n",
    "    data = load_normalized_data(selected_dataset)\n",
    "\n",
    "    # Apply Quantile-Based Thresholding\n",
    "    q95_threshold = features[selected_dataset][\"Q95\"]\n",
    "    q99_threshold = features[selected_dataset][\"Q99\"]\n",
    "\n",
    "    # Create binary masks based on quantile thresholds\n",
    "    mask_q95 = data > q95_threshold  # Region above 95th percentile\n",
    "    mask_q99 = data > q99_threshold  # Region above 99th percentile\n",
    "\n",
    "    # Visualizing Slices with Thresholding\n",
    "    slice_index = data.shape[2] // 2  # Choose middle slice for visualization\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(data[:, :, slice_index], cmap=\"gray\")\n",
    "    plt.title(\"Original Slice\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(mask_q95[:, :, slice_index], cmap=\"hot\")\n",
    "    plt.title(\"Thresholded @ Q95\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask_q99[:, :, slice_index], cmap=\"hot\")\n",
    "    plt.title(\"Thresholded @ Q99\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Dictionary to store extracted features\n",
    "features = {}\n",
    "\n",
    "# Iterate through all datasets\n",
    "for dataset in dataset_names:\n",
    "    data = load_normalized_data(dataset)  # Load dataset\n",
    "    mean_val, std_dev, q95, q99 = extract_features(data)  # Extract features\n",
    "    features[dataset] = {\"Mean\": mean_val, \"Std Dev\": std_dev, \"Q95\": q95, \"Q99\": q99}\n",
    "    print(f\"{dataset}: Mean={mean_val:.4f}, Std Dev={std_dev:.4f}, Q95={q95:.4f}, Q99={q99:.4f}\")\n",
    "    visualizethres(dataset)\n",
    "    \n",
    "    \n",
    "# # Select a dataset for visualization\n",
    "# selected_dataset = dataset_names[0]  # Change this for other datasets\n",
    "# data = load_normalized_data(selected_dataset)\n",
    "\n",
    "# # Apply Quantile-Based Thresholding\n",
    "# q95_threshold = features[selected_dataset][\"Q95\"]\n",
    "# q99_threshold = features[selected_dataset][\"Q99\"]\n",
    "\n",
    "# # Create binary masks based on quantile thresholds\n",
    "# mask_q95 = data > q95_threshold  # Region above 95th percentile\n",
    "# mask_q99 = data > q99_threshold  # Region above 99th percentile\n",
    "\n",
    "# # Visualizing Slices with Thresholding\n",
    "# slice_index = data.shape[2] // 2  # Choose middle slice for visualization\n",
    "\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.imshow(data[:, :, slice_index], cmap=\"gray\")\n",
    "# plt.title(\"Original Slice\")\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.imshow(mask_q95[:, :, slice_index], cmap=\"hot\")\n",
    "# plt.title(\"Thresholded @ Q95\")\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.imshow(mask_q99[:, :, slice_index], cmap=\"hot\")\n",
    "# plt.title(\"Thresholded @ Q99\")\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "This script can be run from the command line or within an interactive environment. It prints out the statistics and processing steps to the console for verification. The final visualization step will open an interactive window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# from pathlib import Path\n",
    "import os\n",
    "\n",
    "# **Get Base Directory of Project**\n",
    "# BASE_DIR = Path.cwd().parent  # Moves one level up from notebook directory\n",
    "BASE_DIR = Path.cwd().parent  # Moves one level up from notebook directory\n",
    "# BASE_DIR = Path(__file__).resolve().parent.parent  # Moves 2 levels up to project root -->  for vs code.\n",
    "\n",
    "print(f\" Base directory :{BASE_DIR}\")\n",
    "# **Define Correct Module Path (inside src/)**\n",
    "MODULES_DIR = BASE_DIR / \"src\" / \"modules\"\n",
    "\n",
    "# **Convert to string & add to sys.path**\n",
    "sys.path.append(str(MODULES_DIR))\n",
    "\n",
    "# **Verify path added correctly**\n",
    "# print(\"Updated Python Path:\", sys.path)\n",
    "\n",
    "# **Now, try importing your module**\n",
    "from listspecificfiles import readlistFiles\n",
    "\n",
    "print(\" Module imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# parent_dir = Path.cwd().parent\n",
    "# child_dir = Path.cwd().parent/ \"src\"  # here we moved to the child directory.\n",
    "# Base_dir = Path.cwd()\n",
    "# print(f\"parent_dir: {parent_dir} \\ncurrent directory: {Base_dir} \\nchild directory:{child_dir}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CURR_DIR = Path.cwd().parent\n",
    "print(f\"current dir:{CURR_DIR}\")\n",
    "BASE_DIR = CURR_DIR \n",
    "data_dir = BASE_DIR/ \"data\" / \"intermdata1\"\n",
    "print(data_dir)\n",
    "import os\n",
    "# os.listdir(data_dir)\n",
    "MODULE_DIR = BASE_DIR/ \"src\" / \"modules\"\n",
    "import sys\n",
    "sys.path.append(str(MODULES_DIR)) # before using any modules.\n",
    "normalizedfileList = []\n",
    "from listspecificfiles import readlistFiles\n",
    "normalizedfileList = readlistFiles(data_dir,keyword = 'normalized.npy')\n",
    "file_paths = []\n",
    "for files in normalizedfileList:\n",
    "    complPath = os.path.join(data_dir,files)\n",
    "    file_paths.append(complPath)\n",
    "\n",
    "print(f\"no of normalized numpy array files: {len(file_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for adding any modules in sys path.\n",
    "# file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the 3D numpy arrays\n",
    "# file_paths = [\n",
    "#     \"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\intermdata10\\tomo_Grafene_24h_normalized.npy\",\n",
    "#     \"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\intermdata1\\tomo_grafene_48h_normalized.npy\",\n",
    "#     \"E:\\Projects\\substructure_3d_data\\Substructure_Different_DataTypes\\data\\intermdata1\\Tomogramma_BuddingYeastCell_normalized.npy\"\n",
    "# ]\n",
    "\n",
    "datasets = [np.load(fp) for fp in file_paths]\n",
    "\n",
    "# Function to compute the smallest value differences\n",
    "def compute_min_variation(data):\n",
    "    diffs = []\n",
    "    \n",
    "    # Compute absolute differences between adjacent elements along all three axes\n",
    "    for axis in range(3):\n",
    "        diff = np.abs(np.diff(data, axis=axis))  # Compute along axis\n",
    "        diffs.append(diff[diff > 0])  # Remove zeros (exactly same values)\n",
    "    \n",
    "    diffs = np.concatenate(diffs)  # Flatten to a single list\n",
    "    return diffs\n",
    "\n",
    "# Store all variations from all datasets\n",
    "all_diffs = np.concatenate([compute_min_variation(data) for data in datasets])\n",
    "\n",
    "# Histogram of variations\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(all_diffs, bins=100, log=True, color=\"blue\", alpha=0.7)\n",
    "plt.xlabel(\"Difference Magnitude\")\n",
    "plt.ylabel(\"Frequency (Log Scale)\")\n",
    "plt.title(\"Histogram of Adjacent Voxel Differences\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Determine the smallest significant difference\n",
    "smallest_variation = np.percentile(all_diffs, 5)  # 5th percentile as lower bound\n",
    "\n",
    "# Decide decimal precision based on smallest variation\n",
    "decimal_places = -int(np.floor(np.log10(smallest_variation)))\n",
    "decimal_places = min(4, decimal_places)  # Restrict to 4 decimal places for practical use\n",
    "\n",
    "print(f\"Suggested decimal precision: {decimal_places} decimal places\")\n",
    "\n",
    "# Round datasets to the determined decimal precision\n",
    "rounded_datasets = [np.round(data, decimal_places) for data in datasets]\n",
    "\n",
    "# Save rounded datasets for further analysis\n",
    "for i, fp in enumerate(file_paths):\n",
    "    save_path = fp.replace(\".npy\", f\"_rounded_{decimal_places}dp.npy\")\n",
    "    np.save(save_path, rounded_datasets[i])\n",
    "    print(f\"Saved rounded dataset: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# import gc  # Garbage collector to free memory\n",
    "\n",
    "# def plot3dinteractive(voldata, keyvalue, output_dir, sample_fraction=0.005):\n",
    "#     \"\"\"Plots large 3D NumPy arrays interactively and saves as HTML.\n",
    "    \n",
    "#     - `voldata`: Input 3D NumPy array.a\n",
    "#     - `keyvalue`: Filename for saving.\n",
    "#     - `output_dir`: Directory to save HTML plots.\n",
    "#     - `sample_fraction`: Fraction of points to randomly plot.\n",
    "#     \"\"\"\n",
    "#     array_3d = voldata\n",
    "#     x1, y1, z1 = array_3d.shape\n",
    "#     print(f\"Shape of {keyvalue}: {x1, y1, z1}\")\n",
    "\n",
    "#     # Create a 3D meshgrid\n",
    "#     x, y, z = np.meshgrid(np.arange(x1), np.arange(y1), np.arange(z1))\n",
    "\n",
    "#     # Mask non-zero values\n",
    "#     mask = array_3d > 0\n",
    "#     x_vals = x[mask].flatten()\n",
    "#     y_vals = y[mask].flatten()\n",
    "#     z_vals = z[mask].flatten()\n",
    "#     values = array_3d[mask].flatten()\n",
    "\n",
    "#     # **Randomly sample points** to reduce memory usage\n",
    "#     num_points = len(values)\n",
    "#     sample_size = int(num_points * sample_fraction)\n",
    "\n",
    "#     if sample_size > 0:\n",
    "#         indices = np.random.choice(num_points, sample_size, replace=False)\n",
    "#         x_vals = x_vals[indices]\n",
    "#         y_vals = y_vals[indices]\n",
    "#         z_vals = z_vals[indices]\n",
    "#         values = values[indices]\n",
    "#     else:\n",
    "#         print(f\"âš  Warning: Not enough non-zero points for {keyvalue}. Skipping...\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Plotting {sample_size} points out of {num_points} ({sample_fraction * 100}% sampled)\")\n",
    "\n",
    "#     # Create a 3D scatter plot\n",
    "#     fig = go.Figure(data=go.Scatter3d(\n",
    "#         x=x_vals,\n",
    "#         y=y_vals,\n",
    "#         z=z_vals,\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=1,\n",
    "#             color=values,\n",
    "#             colorscale='Viridis',\n",
    "#             opacity=0.5\n",
    "#         )\n",
    "#     ))\n",
    "\n",
    "#     # Set axis labels\n",
    "#     fig.update_layout(scene=dict(\n",
    "#         xaxis_title='X',\n",
    "#         yaxis_title='Y',\n",
    "#         zaxis_title='Z'\n",
    "#     ))\n",
    "\n",
    "#     # **Save plot as an interactive HTML file**\n",
    "#     save_path = os.path.join(output_dir, f\"{keyvalue}.html\")\n",
    "#     fig.write_html(save_path)\n",
    "#     print(f\"Saved: {save_path}\")\n",
    "\n",
    "#     # **Clear memory**\n",
    "#     del fig, x_vals, y_vals, z_vals, values\n",
    "#     gc.collect()  # Garbage collection to free memory\n",
    "\n",
    "# # **Directory paths**\n",
    "# data_dir = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\intermdata1\"\n",
    "# output_dir = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\"\n",
    "\n",
    "# # **Ensure output directory exists**\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # **Process all .npy files one by one**\n",
    "# npy_files = [f for f in os.listdir(data_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "# for filename in npy_files:\n",
    "#     file_path = os.path.join(data_dir, filename)\n",
    "#     voldata = np.load(file_path)  # Load .npy file\n",
    "#     plot3dinteractive(voldata, filename, output_dir, sample_fraction=0.04)  # Save & clear memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# import gc  # Garbage collector to free memory\n",
    "\n",
    "# def plot3dinteractive(voldata, keyvalue, output_dir, sample_fraction=0.005):\n",
    "#     \"\"\"Plots large 3D NumPy arrays interactively and saves as HTML.\n",
    "    \n",
    "#     - `voldata`: Input 3D NumPy array.\n",
    "#     - `keyvalue`: Filename for saving.\n",
    "#     - `output_dir`: Directory to save HTML plots.\n",
    "#     - `sample_fraction`: Fraction of points to randomly plot.\n",
    "#     \"\"\"\n",
    "#     array_3d = voldata\n",
    "#     x1, y1, z1 = array_3d.shape\n",
    "#     print(f\"Shape of {keyvalue}: {x1, y1, z1}\")\n",
    "\n",
    "\n",
    "#     # Create a 3D meshgrid\n",
    "#     x, y, z = np.meshgrid(np.arange(x1), np.arange(y1), np.arange(z1))\n",
    "\n",
    "#     # Mask non-zero values\n",
    "#     mask = array_3d > 0\n",
    "#     x_vals = x[mask].flatten()\n",
    "#     y_vals = y[mask].flatten()\n",
    "#     z_vals = z[mask].flatten()\n",
    "#     values = array_3d[mask].flatten()\n",
    "\n",
    "#     # **Randomly sample points** to reduce memory usage\n",
    "#     num_points = len(values)\n",
    "#     sample_size = int(num_points * sample_fraction)\n",
    "\n",
    "#     if sample_size > 0:\n",
    "#         indices = np.random.choice(num_points, sample_size, replace=False)\n",
    "#         x_vals = x_vals[indices]\n",
    "#         y_vals = y_vals[indices]\n",
    "#         z_vals = z_vals[indices]\n",
    "#         values = values[indices]\n",
    "#     else:\n",
    "#         print(f\"âš  Warning: Not enough non-zero points for {keyvalue}. Skipping...\")\n",
    "#         return\n",
    "\n",
    "#     print(f\"Plotting {sample_size} points out of {num_points} ({sample_fraction * 100}% sampled)\")\n",
    "\n",
    "#     # **Enhanced Color Grading**\n",
    "#     colorscale = [\n",
    "#         [0.0, \"white\"],    # Outer structure (light color)\n",
    "#         [0.2, \"lightblue\"],\n",
    "#         [0.4, \"deepskyblue\"],\n",
    "#         [0.6, \"dodgerblue\"],\n",
    "#         [0.8, \"blue\"],      # Middle layers\n",
    "#         [1.0, \"darkblue\"]   # Deep inner structure (dark color)\n",
    "#     ]\n",
    "\n",
    "#     # Create a 3D scatter plot\n",
    "#     fig = go.Figure(data=go.Scatter3d(\n",
    "#         x=x_vals,\n",
    "#         y=y_vals,\n",
    "#         z=z_vals,\n",
    "#         mode='markers',\n",
    "#         marker=dict(\n",
    "#             size=2,\n",
    "#             color=values,\n",
    "#             colorscale=colorscale,\n",
    "#             opacity=0.5\n",
    "#         )\n",
    "#     ))\n",
    "\n",
    "#     # Set axis labels and layout\n",
    "#     fig.update_layout(\n",
    "#         title=f\"3D Structure: {keyvalue}\",\n",
    "#         scene=dict(\n",
    "#             xaxis_title='X',\n",
    "#             yaxis_title='Y',\n",
    "#             zaxis_title='Z',\n",
    "#             bgcolor=\"black\"  # Dark background for better contrast\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     # **Save plot as an interactive HTML file**\n",
    "#     save_path = os.path.join(output_dir, f\"{keyvalue}.html\")\n",
    "#     fig.write_html(save_path)\n",
    "#     print(f\" Saved: {save_path}\")\n",
    "\n",
    "#     # **Clear memory**\n",
    "#     del fig, x_vals, y_vals, z_vals, values\n",
    "#     gc.collect()  # Garbage collection to free memory\n",
    "\n",
    "# # **Directory paths**\n",
    "# data_dir = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\intermdata1\"\n",
    "# output_dir = r\"C:\\Users\\Gaetano\\Desktop\\create_with_codeRafi\\MyProjects\\Substructure_Different_DataTypes\\data\\raw\"\n",
    "\n",
    "# # **Ensure output directory exists**\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # **Process all .npy files one by one**\n",
    "# npy_files = [f for f in os.listdir(data_dir) if f.endswith(\".npy\")]\n",
    "\n",
    "# for filename in npy_files:\n",
    "#     file_path = os.path.join(data_dir, filename)\n",
    "#     voldata = np.load(file_path)  # Load .npy file\n",
    "#     plot3dinteractive(voldata, filename, output_dir, sample_fraction=0.05)  # Save & clear memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
